{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase de **Entrenamiento y validación** | Verne Academy **Kaggle Competition**\n",
    "### Por **Mario Jurado Galán**\n",
    "Este notebook incluye:  \n",
    "+ Carga de datos\n",
    "+ EDA\n",
    "+ Transformaciones\n",
    "+ Entrenamiento del modelo y predicción\n",
    "+ Evaluación y visualización de resultados\n",
    "+ Interpretabilidad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div align='center'><img src='data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABkAAAAWCAYAAAA1vze2AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAdxJREFUeNq0Vt1Rg0AQJjcpgBJiBWIFkgoMFYhPPAIVECogPuYpdJBYgXQQrMCUkA50V7+d2ZwXuXPGm9khHLu3f9+3l1nkWNvtNqfHLgpfQ1EUS3tz5nAQ0+NIsiAZSc6eDlI8M3J00B/mDuUKDk6kfOebAgW3pkdD0pFcODGW4gKKvOrAUm04MA4QDt1OEIXU9hDigfS5rC1eS5T90gltck1Xrizo257kgySZcNRzgCSxCvgiE9nckPJo2b/B2AcEkk2OwL8bD8gmOKR1GPbaCUqxEgTq0tLvgb6zfo7+DgYGkkWL2tqLDV4RSITfbHPPfJKIrWz4nJQTMPAWA7IbD6imcNaDeDfgk+4No+wZr40BL3g9eQJJCFqRQ54KiSt72lsLpE3o3MCBSxDuq4yOckU2hKXRuwBH3OyMR4g1UpyTYw6mlmBqNdUXRM1NfyF5EPI6JkcpIDBIX8jX6DR/6ckAZJ0wEAdLR8DEk6OfC1Pp8BKo6TQIwPJbvJ6toK5lmuvJoRtfK6Ym1iRYIarRo2UyYHvRN5qpakR3yoizWrouoyuXXQqI185LCw07op5ZyCRGL99h24InP0e9xdQukEKVmhzrqZuRIfwISB//cP3Wk3f8f/yR+BRgAHu00HjLcEQBAAAAAElFTkSuQmCC' /></div><script charset='utf-8'>/*! For license information please see bundle.js.LICENSE.txt */\n",
       "(()=>{var e={486:function(e,t,n){var r;e=n.nmd(e),function(){var a,i=\"Expected a function\",o=\"__lodash_hash_undefined__\",u=\"__lodash_placeholder__\",l=32,s=128,c=1/0,f=9007199254740991,p=NaN,d=4294967295,h=[[\"ary\",s],[\"bind\",1],[\"bindKey\",2],[\"curry\",8],[\"curryRight\",16],[\"flip\",512],[\"partial\",l],[\"partialRight\",64],[\"rearg\",256]],v=\"[object Arguments]\",g=\"[object Array]\",y=\"[object Boolean]\",m=\"[object Date]\",b=\"[object Error]\",_=\"[object Function]\",w=\"[object GeneratorFunction]\",x=\"[object Map]\",k=\"[object Number]\",S=\"[object Object]\",E=\"[object Promise]\",C=\"[object RegExp]\",T=\"[object Set]\",M=\"[object String]\",N=\"[object Symbol]\",P=\"[object WeakMap]\",z=\"[object ArrayBuffer]\",L=\"[object DataView]\",O=\"[object Float32Array]\",A=\"[object Float64Array]\",F=\"[object Int8Array]\",D=\"[object Int16Array]\",R=\"[object Int32Array]\",j=\"[object Uint8Array]\",U=\"[object Uint8ClampedArray]\",I=\"[object Uint16Array]\",$=\"[object Uint32Array]\",B=/\\b__p \\+= '';/g,W=/\\b(__p \\+=) '' \\+/g,V=/(__e\\(.*?\\)|\\b__t\\)) \\+\\n'';/g,H=/&(?:amp|lt|gt|quot|#39);/g,q=/[&<>\"']/g,Q=RegExp(H.source),Y=RegExp(q.source),G=/<%-([\\s\\S]+?)%>/g,K=/<%([\\s\\S]+?)%>/g,Z=/<%=([\\s\\S]+?)%>/g,X=/\\.|\\[(?:[^[\\]]*|([\"'])(?:(?!\\1)[^\\\\]|\\\\.)*?\\1)\\]/,J=/^\\w*$/,ee=/[^.[\\]]+|\\[(?:(-?\\d+(?:\\.\\d+)?)|([\"'])((?:(?!\\2)[^\\\\]|\\\\.)*?)\\2)\\]|(?=(?:\\.|\\[\\])(?:\\.|\\[\\]|$))/g,te=/[\\\\^$.*+?()[\\]{}|]/g,ne=RegExp(te.source),re=/^\\s+/,ae=/\\s/,ie=/\\{(?:\\n\\/\\* \\[wrapped with .+\\] \\*\\/)?\\n?/,oe=/\\{\\n\\/\\* \\[wrapped with (.+)\\] \\*/,ue=/,? & /,le=/[^\\x00-\\x2f\\x3a-\\x40\\x5b-\\x60\\x7b-\\x7f]+/g,se=/[()=,{}\\[\\]\\/\\s]/,ce=/\\\\(\\\\)?/g,fe=/\\$\\{([^\\\\}]*(?:\\\\.[^\\\\}]*)*)\\}/g,pe=/\\w*$/,de=/^[-+]0x[0-9a-f]+$/i,he=/^0b[01]+$/i,ve=/^\\[object .+?Constructor\\]$/,ge=/^0o[0-7]+$/i,ye=/^(?:0|[1-9]\\d*)$/,me=/[\\xc0-\\xd6\\xd8-\\xf6\\xf8-\\xff\\u0100-\\u017f]/g,be=/($^)/,_e=/['\\n\\r\\u2028\\u2029\\\\]/g,we=\"\\\\ud800-\\\\udfff\",xe=\"\\\\u0300-\\\\u036f\\\\ufe20-\\\\ufe2f\\\\u20d0-\\\\u20ff\",ke=\"\\\\u2700-\\\\u27bf\",Se=\"a-z\\\\xdf-\\\\xf6\\\\xf8-\\\\xff\",Ee=\"A-Z\\\\xc0-\\\\xd6\\\\xd8-\\\\xde\",Ce=\"\\\\ufe0e\\\\ufe0f\",Te=\"\\\\xac\\\\xb1\\\\xd7\\\\xf7\\\\x00-\\\\x2f\\\\x3a-\\\\x40\\\\x5b-\\\\x60\\\\x7b-\\\\xbf\\\\u2000-\\\\u206f \\\\t\\\\x0b\\\\f\\\\xa0\\\\ufeff\\\\n\\\\r\\\\u2028\\\\u2029\\\\u1680\\\\u180e\\\\u2000\\\\u2001\\\\u2002\\\\u2003\\\\u2004\\\\u2005\\\\u2006\\\\u2007\\\\u2008\\\\u2009\\\\u200a\\\\u202f\\\\u205f\\\\u3000\",Me=\"[\"+we+\"]\",Ne=\"[\"+Te+\"]\",Pe=\"[\"+xe+\"]\",ze=\"\\\\d+\",Le=\"[\"+ke+\"]\",Oe=\"[\"+Se+\"]\",Ae=\"[^\"+we+Te+ze+ke+Se+Ee+\"]\",Fe=\"\\\\ud83c[\\\\udffb-\\\\udfff]\",De=\"[^\"+we+\"]\",Re=\"(?:\\\\ud83c[\\\\udde6-\\\\uddff]){2}\",je=\"[\\\\ud800-\\\\udbff][\\\\udc00-\\\\udfff]\",Ue=\"[\"+Ee+\"]\",Ie=\"\\\\u200d\",$e=\"(?:\"+Oe+\"|\"+Ae+\")\",Be=\"(?:\"+Ue+\"|\"+Ae+\")\",We=\"(?:['’](?:d|ll|m|re|s|t|ve))?\",Ve=\"(?:['’](?:D|LL|M|RE|S|T|VE))?\",He=\"(?:\"+Pe+\"|\"+Fe+\")?\",qe=\"[\"+Ce+\"]?\",Qe=qe+He+\"(?:\"+Ie+\"(?:\"+[De,Re,je].join(\"|\")+\")\"+qe+He+\")*\",Ye=\"(?:\"+[Le,Re,je].join(\"|\")+\")\"+Qe,Ge=\"(?:\"+[De+Pe+\"?\",Pe,Re,je,Me].join(\"|\")+\")\",Ke=RegExp(\"['’]\",\"g\"),Ze=RegExp(Pe,\"g\"),Xe=RegExp(Fe+\"(?=\"+Fe+\")|\"+Ge+Qe,\"g\"),Je=RegExp([Ue+\"?\"+Oe+\"+\"+We+\"(?=\"+[Ne,Ue,\"$\"].join(\"|\")+\")\",Be+\"+\"+Ve+\"(?=\"+[Ne,Ue+$e,\"$\"].join(\"|\")+\")\",Ue+\"?\"+$e+\"+\"+We,Ue+\"+\"+Ve,\"\\\\d*(?:1ST|2ND|3RD|(?![123])\\\\dTH)(?=\\\\b|[a-z_])\",\"\\\\d*(?:1st|2nd|3rd|(?![123])\\\\dth)(?=\\\\b|[A-Z_])\",ze,Ye].join(\"|\"),\"g\"),et=RegExp(\"[\"+Ie+we+xe+Ce+\"]\"),tt=/[a-z][A-Z]|[A-Z]{2}[a-z]|[0-9][a-zA-Z]|[a-zA-Z][0-9]|[^a-zA-Z0-9 ]/,nt=[\"Array\",\"Buffer\",\"DataView\",\"Date\",\"Error\",\"Float32Array\",\"Float64Array\",\"Function\",\"Int8Array\",\"Int16Array\",\"Int32Array\",\"Map\",\"Math\",\"Object\",\"Promise\",\"RegExp\",\"Set\",\"String\",\"Symbol\",\"TypeError\",\"Uint8Array\",\"Uint8ClampedArray\",\"Uint16Array\",\"Uint32Array\",\"WeakMap\",\"_\",\"clearTimeout\",\"isFinite\",\"parseInt\",\"setTimeout\"],rt=-1,at={};at[O]=at[A]=at[F]=at[D]=at[R]=at[j]=at[U]=at[I]=at[$]=!0,at[v]=at[g]=at[z]=at[y]=at[L]=at[m]=at[b]=at[_]=at[x]=at[k]=at[S]=at[C]=at[T]=at[M]=at[P]=!1;var it={};it[v]=it[g]=it[z]=it[L]=it[y]=it[m]=it[O]=it[A]=it[F]=it[D]=it[R]=it[x]=it[k]=it[S]=it[C]=it[T]=it[M]=it[N]=it[j]=it[U]=it[I]=it[$]=!0,it[b]=it[_]=it[P]=!1;var ot={\"\\\\\":\"\\\\\",\"'\":\"'\",\"\\n\":\"n\",\"\\r\":\"r\",\"\\u2028\":\"u2028\",\"\\u2029\":\"u2029\"},ut=parseFloat,lt=parseInt,st=\"object\"==typeof n.g&&n.g&&n.g.Object===Object&&n.g,ct=\"object\"==typeof self&&self&&self.Object===Object&&self,ft=st||ct||Function(\"return this\")(),pt=t&&!t.nodeType&&t,dt=pt&&e&&!e.nodeType&&e,ht=dt&&dt.exports===pt,vt=ht&&st.process,gt=function(){try{return dt&&dt.require&&dt.require(\"util\").types||vt&&vt.binding&&vt.binding(\"util\")}catch(e){}}(),yt=gt&&gt.isArrayBuffer,mt=gt&&gt.isDate,bt=gt&&gt.isMap,_t=gt&&gt.isRegExp,wt=gt&&gt.isSet,xt=gt&&gt.isTypedArray;function kt(e,t,n){switch(n.length){case 0:return e.call(t);case 1:return e.call(t,n[0]);case 2:return e.call(t,n[0],n[1]);case 3:return e.call(t,n[0],n[1],n[2])}return e.apply(t,n)}function St(e,t,n,r){for(var a=-1,i=null==e?0:e.length;++a<i;){var o=e[a];t(r,o,n(o),e)}return r}function Et(e,t){for(var n=-1,r=null==e?0:e.length;++n<r&&!1!==t(e[n],n,e););return e}function Ct(e,t){for(var n=null==e?0:e.length;n--&&!1!==t(e[n],n,e););return e}function Tt(e,t){for(var n=-1,r=null==e?0:e.length;++n<r;)if(!t(e[n],n,e))return!1;return!0}function Mt(e,t){for(var n=-1,r=null==e?0:e.length,a=0,i=[];++n<r;){var o=e[n];t(o,n,e)&&(i[a++]=o)}return i}function Nt(e,t){return!(null==e||!e.length)&&Ut(e,t,0)>-1}function Pt(e,t,n){for(var r=-1,a=null==e?0:e.length;++r<a;)if(n(t,e[r]))return!0;return!1}function zt(e,t){for(var n=-1,r=null==e?0:e.length,a=Array(r);++n<r;)a[n]=t(e[n],n,e);return a}function Lt(e,t){for(var n=-1,r=t.length,a=e.length;++n<r;)e[a+n]=t[n];return e}function Ot(e,t,n,r){var a=-1,i=null==e?0:e.length;for(r&&i&&(n=e[++a]);++a<i;)n=t(n,e[a],a,e);return n}function At(e,t,n,r){var a=null==e?0:e.length;for(r&&a&&(n=e[--a]);a--;)n=t(n,e[a],a,e);return n}function Ft(e,t){for(var n=-1,r=null==e?0:e.length;++n<r;)if(t(e[n],n,e))return!0;return!1}var Dt=Wt(\"length\");function Rt(e,t,n){var r;return n(e,(function(e,n,a){if(t(e,n,a))return r=n,!1})),r}function jt(e,t,n,r){for(var a=e.length,i=n+(r?1:-1);r?i--:++i<a;)if(t(e[i],i,e))return i;return-1}function Ut(e,t,n){return t==t?function(e,t,n){for(var r=n-1,a=e.length;++r<a;)if(e[r]===t)return r;return-1}(e,t,n):jt(e,$t,n)}function It(e,t,n,r){for(var a=n-1,i=e.length;++a<i;)if(r(e[a],t))return a;return-1}function $t(e){return e!=e}function Bt(e,t){var n=null==e?0:e.length;return n?qt(e,t)/n:p}function Wt(e){return function(t){return null==t?a:t[e]}}function Vt(e){return function(t){return null==e?a:e[t]}}function Ht(e,t,n,r,a){return a(e,(function(e,a,i){n=r?(r=!1,e):t(n,e,a,i)})),n}function qt(e,t){for(var n,r=-1,i=e.length;++r<i;){var o=t(e[r]);o!==a&&(n=n===a?o:n+o)}return n}function Qt(e,t){for(var n=-1,r=Array(e);++n<e;)r[n]=t(n);return r}function Yt(e){return e?e.slice(0,pn(e)+1).replace(re,\"\"):e}function Gt(e){return function(t){return e(t)}}function Kt(e,t){return zt(t,(function(t){return e[t]}))}function Zt(e,t){return e.has(t)}function Xt(e,t){for(var n=-1,r=e.length;++n<r&&Ut(t,e[n],0)>-1;);return n}function Jt(e,t){for(var n=e.length;n--&&Ut(t,e[n],0)>-1;);return n}var en=Vt({À:\"A\",Á:\"A\",Â:\"A\",Ã:\"A\",Ä:\"A\",Å:\"A\",à:\"a\",á:\"a\",â:\"a\",ã:\"a\",ä:\"a\",å:\"a\",Ç:\"C\",ç:\"c\",Ð:\"D\",ð:\"d\",È:\"E\",É:\"E\",Ê:\"E\",Ë:\"E\",è:\"e\",é:\"e\",ê:\"e\",ë:\"e\",Ì:\"I\",Í:\"I\",Î:\"I\",Ï:\"I\",ì:\"i\",í:\"i\",î:\"i\",ï:\"i\",Ñ:\"N\",ñ:\"n\",Ò:\"O\",Ó:\"O\",Ô:\"O\",Õ:\"O\",Ö:\"O\",Ø:\"O\",ò:\"o\",ó:\"o\",ô:\"o\",õ:\"o\",ö:\"o\",ø:\"o\",Ù:\"U\",Ú:\"U\",Û:\"U\",Ü:\"U\",ù:\"u\",ú:\"u\",û:\"u\",ü:\"u\",Ý:\"Y\",ý:\"y\",ÿ:\"y\",Æ:\"Ae\",æ:\"ae\",Þ:\"Th\",þ:\"th\",ß:\"ss\",Ā:\"A\",Ă:\"A\",Ą:\"A\",ā:\"a\",ă:\"a\",ą:\"a\",Ć:\"C\",Ĉ:\"C\",Ċ:\"C\",Č:\"C\",ć:\"c\",ĉ:\"c\",ċ:\"c\",č:\"c\",Ď:\"D\",Đ:\"D\",ď:\"d\",đ:\"d\",Ē:\"E\",Ĕ:\"E\",Ė:\"E\",Ę:\"E\",Ě:\"E\",ē:\"e\",ĕ:\"e\",ė:\"e\",ę:\"e\",ě:\"e\",Ĝ:\"G\",Ğ:\"G\",Ġ:\"G\",Ģ:\"G\",ĝ:\"g\",ğ:\"g\",ġ:\"g\",ģ:\"g\",Ĥ:\"H\",Ħ:\"H\",ĥ:\"h\",ħ:\"h\",Ĩ:\"I\",Ī:\"I\",Ĭ:\"I\",Į:\"I\",İ:\"I\",ĩ:\"i\",ī:\"i\",ĭ:\"i\",į:\"i\",ı:\"i\",Ĵ:\"J\",ĵ:\"j\",Ķ:\"K\",ķ:\"k\",ĸ:\"k\",Ĺ:\"L\",Ļ:\"L\",Ľ:\"L\",Ŀ:\"L\",Ł:\"L\",ĺ:\"l\",ļ:\"l\",ľ:\"l\",ŀ:\"l\",ł:\"l\",Ń:\"N\",Ņ:\"N\",Ň:\"N\",Ŋ:\"N\",ń:\"n\",ņ:\"n\",ň:\"n\",ŋ:\"n\",Ō:\"O\",Ŏ:\"O\",Ő:\"O\",ō:\"o\",ŏ:\"o\",ő:\"o\",Ŕ:\"R\",Ŗ:\"R\",Ř:\"R\",ŕ:\"r\",ŗ:\"r\",ř:\"r\",Ś:\"S\",Ŝ:\"S\",Ş:\"S\",Š:\"S\",ś:\"s\",ŝ:\"s\",ş:\"s\",š:\"s\",Ţ:\"T\",Ť:\"T\",Ŧ:\"T\",ţ:\"t\",ť:\"t\",ŧ:\"t\",Ũ:\"U\",Ū:\"U\",Ŭ:\"U\",Ů:\"U\",Ű:\"U\",Ų:\"U\",ũ:\"u\",ū:\"u\",ŭ:\"u\",ů:\"u\",ű:\"u\",ų:\"u\",Ŵ:\"W\",ŵ:\"w\",Ŷ:\"Y\",ŷ:\"y\",Ÿ:\"Y\",Ź:\"Z\",Ż:\"Z\",Ž:\"Z\",ź:\"z\",ż:\"z\",ž:\"z\",Ĳ:\"IJ\",ĳ:\"ij\",Œ:\"Oe\",œ:\"oe\",ŉ:\"'n\",ſ:\"s\"}),tn=Vt({\"&\":\"&amp;\",\"<\":\"&lt;\",\">\":\"&gt;\",'\"':\"&quot;\",\"'\":\"&#39;\"});function nn(e){return\"\\\\\"+ot[e]}function rn(e){return et.test(e)}function an(e){var t=-1,n=Array(e.size);return e.forEach((function(e,r){n[++t]=[r,e]})),n}function on(e,t){return function(n){return e(t(n))}}function un(e,t){for(var n=-1,r=e.length,a=0,i=[];++n<r;){var o=e[n];o!==t&&o!==u||(e[n]=u,i[a++]=n)}return i}function ln(e){var t=-1,n=Array(e.size);return e.forEach((function(e){n[++t]=e})),n}function sn(e){var t=-1,n=Array(e.size);return e.forEach((function(e){n[++t]=[e,e]})),n}function cn(e){return rn(e)?function(e){for(var t=Xe.lastIndex=0;Xe.test(e);)++t;return t}(e):Dt(e)}function fn(e){return rn(e)?function(e){return e.match(Xe)||[]}(e):function(e){return e.split(\"\")}(e)}function pn(e){for(var t=e.length;t--&&ae.test(e.charAt(t)););return t}var dn=Vt({\"&amp;\":\"&\",\"&lt;\":\"<\",\"&gt;\":\">\",\"&quot;\":'\"',\"&#39;\":\"'\"}),hn=function e(t){var n,r=(t=null==t?ft:hn.defaults(ft.Object(),t,hn.pick(ft,nt))).Array,ae=t.Date,we=t.Error,xe=t.Function,ke=t.Math,Se=t.Object,Ee=t.RegExp,Ce=t.String,Te=t.TypeError,Me=r.prototype,Ne=xe.prototype,Pe=Se.prototype,ze=t[\"__core-js_shared__\"],Le=Ne.toString,Oe=Pe.hasOwnProperty,Ae=0,Fe=(n=/[^.]+$/.exec(ze&&ze.keys&&ze.keys.IE_PROTO||\"\"))?\"Symbol(src)_1.\"+n:\"\",De=Pe.toString,Re=Le.call(Se),je=ft._,Ue=Ee(\"^\"+Le.call(Oe).replace(te,\"\\\\$&\").replace(/hasOwnProperty|(function).*?(?=\\\\\\()| for .+?(?=\\\\\\])/g,\"$1.*?\")+\"$\"),Ie=ht?t.Buffer:a,$e=t.Symbol,Be=t.Uint8Array,We=Ie?Ie.allocUnsafe:a,Ve=on(Se.getPrototypeOf,Se),He=Se.create,qe=Pe.propertyIsEnumerable,Qe=Me.splice,Ye=$e?$e.isConcatSpreadable:a,Ge=$e?$e.iterator:a,Xe=$e?$e.toStringTag:a,et=function(){try{var e=li(Se,\"defineProperty\");return e({},\"\",{}),e}catch(e){}}(),ot=t.clearTimeout!==ft.clearTimeout&&t.clearTimeout,st=ae&&ae.now!==ft.Date.now&&ae.now,ct=t.setTimeout!==ft.setTimeout&&t.setTimeout,pt=ke.ceil,dt=ke.floor,vt=Se.getOwnPropertySymbols,gt=Ie?Ie.isBuffer:a,Dt=t.isFinite,Vt=Me.join,vn=on(Se.keys,Se),gn=ke.max,yn=ke.min,mn=ae.now,bn=t.parseInt,_n=ke.random,wn=Me.reverse,xn=li(t,\"DataView\"),kn=li(t,\"Map\"),Sn=li(t,\"Promise\"),En=li(t,\"Set\"),Cn=li(t,\"WeakMap\"),Tn=li(Se,\"create\"),Mn=Cn&&new Cn,Nn={},Pn=Di(xn),zn=Di(kn),Ln=Di(Sn),On=Di(En),An=Di(Cn),Fn=$e?$e.prototype:a,Dn=Fn?Fn.valueOf:a,Rn=Fn?Fn.toString:a;function jn(e){if(eu(e)&&!Wo(e)&&!(e instanceof Bn)){if(e instanceof $n)return e;if(Oe.call(e,\"__wrapped__\"))return Ri(e)}return new $n(e)}var Un=function(){function e(){}return function(t){if(!Jo(t))return{};if(He)return He(t);e.prototype=t;var n=new e;return e.prototype=a,n}}();function In(){}function $n(e,t){this.__wrapped__=e,this.__actions__=[],this.__chain__=!!t,this.__index__=0,this.__values__=a}function Bn(e){this.__wrapped__=e,this.__actions__=[],this.__dir__=1,this.__filtered__=!1,this.__iteratees__=[],this.__takeCount__=d,this.__views__=[]}function Wn(e){var t=-1,n=null==e?0:e.length;for(this.clear();++t<n;){var r=e[t];this.set(r[0],r[1])}}function Vn(e){var t=-1,n=null==e?0:e.length;for(this.clear();++t<n;){var r=e[t];this.set(r[0],r[1])}}function Hn(e){var t=-1,n=null==e?0:e.length;for(this.clear();++t<n;){var r=e[t];this.set(r[0],r[1])}}function qn(e){var t=-1,n=null==e?0:e.length;for(this.__data__=new Hn;++t<n;)this.add(e[t])}function Qn(e){var t=this.__data__=new Vn(e);this.size=t.size}function Yn(e,t){var n=Wo(e),r=!n&&Bo(e),a=!n&&!r&&Qo(e),i=!n&&!r&&!a&&lu(e),o=n||r||a||i,u=o?Qt(e.length,Ce):[],l=u.length;for(var s in e)!t&&!Oe.call(e,s)||o&&(\"length\"==s||a&&(\"offset\"==s||\"parent\"==s)||i&&(\"buffer\"==s||\"byteLength\"==s||\"byteOffset\"==s)||vi(s,l))||u.push(s);return u}function Gn(e){var t=e.length;return t?e[Hr(0,t-1)]:a}function Kn(e,t){return zi(Ca(e),ir(t,0,e.length))}function Zn(e){return zi(Ca(e))}function Xn(e,t,n){(n!==a&&!Uo(e[t],n)||n===a&&!(t in e))&&rr(e,t,n)}function Jn(e,t,n){var r=e[t];Oe.call(e,t)&&Uo(r,n)&&(n!==a||t in e)||rr(e,t,n)}function er(e,t){for(var n=e.length;n--;)if(Uo(e[n][0],t))return n;return-1}function tr(e,t,n,r){return cr(e,(function(e,a,i){t(r,e,n(e),i)})),r}function nr(e,t){return e&&Ta(t,Pu(t),e)}function rr(e,t,n){\"__proto__\"==t&&et?et(e,t,{configurable:!0,enumerable:!0,value:n,writable:!0}):e[t]=n}function ar(e,t){for(var n=-1,i=t.length,o=r(i),u=null==e;++n<i;)o[n]=u?a:Eu(e,t[n]);return o}function ir(e,t,n){return e==e&&(n!==a&&(e=e<=n?e:n),t!==a&&(e=e>=t?e:t)),e}function or(e,t,n,r,i,o){var u,l=1&t,s=2&t,c=4&t;if(n&&(u=i?n(e,r,i,o):n(e)),u!==a)return u;if(!Jo(e))return e;var f=Wo(e);if(f){if(u=function(e){var t=e.length,n=new e.constructor(t);return t&&\"string\"==typeof e[0]&&Oe.call(e,\"index\")&&(n.index=e.index,n.input=e.input),n}(e),!l)return Ca(e,u)}else{var p=fi(e),d=p==_||p==w;if(Qo(e))return _a(e,l);if(p==S||p==v||d&&!i){if(u=s||d?{}:di(e),!l)return s?function(e,t){return Ta(e,ci(e),t)}(e,function(e,t){return e&&Ta(t,zu(t),e)}(u,e)):function(e,t){return Ta(e,si(e),t)}(e,nr(u,e))}else{if(!it[p])return i?e:{};u=function(e,t,n){var r,a=e.constructor;switch(t){case z:return wa(e);case y:case m:return new a(+e);case L:return function(e,t){var n=t?wa(e.buffer):e.buffer;return new e.constructor(n,e.byteOffset,e.byteLength)}(e,n);case O:case A:case F:case D:case R:case j:case U:case I:case $:return xa(e,n);case x:return new a;case k:case M:return new a(e);case C:return function(e){var t=new e.constructor(e.source,pe.exec(e));return t.lastIndex=e.lastIndex,t}(e);case T:return new a;case N:return r=e,Dn?Se(Dn.call(r)):{}}}(e,p,l)}}o||(o=new Qn);var h=o.get(e);if(h)return h;o.set(e,u),iu(e)?e.forEach((function(r){u.add(or(r,t,n,r,e,o))})):tu(e)&&e.forEach((function(r,a){u.set(a,or(r,t,n,a,e,o))}));var g=f?a:(c?s?ti:ei:s?zu:Pu)(e);return Et(g||e,(function(r,a){g&&(r=e[a=r]),Jn(u,a,or(r,t,n,a,e,o))})),u}function ur(e,t,n){var r=n.length;if(null==e)return!r;for(e=Se(e);r--;){var i=n[r],o=t[i],u=e[i];if(u===a&&!(i in e)||!o(u))return!1}return!0}function lr(e,t,n){if(\"function\"!=typeof e)throw new Te(i);return Ti((function(){e.apply(a,n)}),t)}function sr(e,t,n,r){var a=-1,i=Nt,o=!0,u=e.length,l=[],s=t.length;if(!u)return l;n&&(t=zt(t,Gt(n))),r?(i=Pt,o=!1):t.length>=200&&(i=Zt,o=!1,t=new qn(t));e:for(;++a<u;){var c=e[a],f=null==n?c:n(c);if(c=r||0!==c?c:0,o&&f==f){for(var p=s;p--;)if(t[p]===f)continue e;l.push(c)}else i(t,f,r)||l.push(c)}return l}jn.templateSettings={escape:G,evaluate:K,interpolate:Z,variable:\"\",imports:{_:jn}},jn.prototype=In.prototype,jn.prototype.constructor=jn,$n.prototype=Un(In.prototype),$n.prototype.constructor=$n,Bn.prototype=Un(In.prototype),Bn.prototype.constructor=Bn,Wn.prototype.clear=function(){this.__data__=Tn?Tn(null):{},this.size=0},Wn.prototype.delete=function(e){var t=this.has(e)&&delete this.__data__[e];return this.size-=t?1:0,t},Wn.prototype.get=function(e){var t=this.__data__;if(Tn){var n=t[e];return n===o?a:n}return Oe.call(t,e)?t[e]:a},Wn.prototype.has=function(e){var t=this.__data__;return Tn?t[e]!==a:Oe.call(t,e)},Wn.prototype.set=function(e,t){var n=this.__data__;return this.size+=this.has(e)?0:1,n[e]=Tn&&t===a?o:t,this},Vn.prototype.clear=function(){this.__data__=[],this.size=0},Vn.prototype.delete=function(e){var t=this.__data__,n=er(t,e);return!(n<0||(n==t.length-1?t.pop():Qe.call(t,n,1),--this.size,0))},Vn.prototype.get=function(e){var t=this.__data__,n=er(t,e);return n<0?a:t[n][1]},Vn.prototype.has=function(e){return er(this.__data__,e)>-1},Vn.prototype.set=function(e,t){var n=this.__data__,r=er(n,e);return r<0?(++this.size,n.push([e,t])):n[r][1]=t,this},Hn.prototype.clear=function(){this.size=0,this.__data__={hash:new Wn,map:new(kn||Vn),string:new Wn}},Hn.prototype.delete=function(e){var t=oi(this,e).delete(e);return this.size-=t?1:0,t},Hn.prototype.get=function(e){return oi(this,e).get(e)},Hn.prototype.has=function(e){return oi(this,e).has(e)},Hn.prototype.set=function(e,t){var n=oi(this,e),r=n.size;return n.set(e,t),this.size+=n.size==r?0:1,this},qn.prototype.add=qn.prototype.push=function(e){return this.__data__.set(e,o),this},qn.prototype.has=function(e){return this.__data__.has(e)},Qn.prototype.clear=function(){this.__data__=new Vn,this.size=0},Qn.prototype.delete=function(e){var t=this.__data__,n=t.delete(e);return this.size=t.size,n},Qn.prototype.get=function(e){return this.__data__.get(e)},Qn.prototype.has=function(e){return this.__data__.has(e)},Qn.prototype.set=function(e,t){var n=this.__data__;if(n instanceof Vn){var r=n.__data__;if(!kn||r.length<199)return r.push([e,t]),this.size=++n.size,this;n=this.__data__=new Hn(r)}return n.set(e,t),this.size=n.size,this};var cr=Pa(mr),fr=Pa(br,!0);function pr(e,t){var n=!0;return cr(e,(function(e,r,a){return n=!!t(e,r,a)})),n}function dr(e,t,n){for(var r=-1,i=e.length;++r<i;){var o=e[r],u=t(o);if(null!=u&&(l===a?u==u&&!uu(u):n(u,l)))var l=u,s=o}return s}function hr(e,t){var n=[];return cr(e,(function(e,r,a){t(e,r,a)&&n.push(e)})),n}function vr(e,t,n,r,a){var i=-1,o=e.length;for(n||(n=hi),a||(a=[]);++i<o;){var u=e[i];t>0&&n(u)?t>1?vr(u,t-1,n,r,a):Lt(a,u):r||(a[a.length]=u)}return a}var gr=za(),yr=za(!0);function mr(e,t){return e&&gr(e,t,Pu)}function br(e,t){return e&&yr(e,t,Pu)}function _r(e,t){return Mt(t,(function(t){return Ko(e[t])}))}function wr(e,t){for(var n=0,r=(t=ga(t,e)).length;null!=e&&n<r;)e=e[Fi(t[n++])];return n&&n==r?e:a}function xr(e,t,n){var r=t(e);return Wo(e)?r:Lt(r,n(e))}function kr(e){return null==e?e===a?\"[object Undefined]\":\"[object Null]\":Xe&&Xe in Se(e)?function(e){var t=Oe.call(e,Xe),n=e[Xe];try{e[Xe]=a;var r=!0}catch(e){}var i=De.call(e);return r&&(t?e[Xe]=n:delete e[Xe]),i}(e):function(e){return De.call(e)}(e)}function Sr(e,t){return e>t}function Er(e,t){return null!=e&&Oe.call(e,t)}function Cr(e,t){return null!=e&&t in Se(e)}function Tr(e,t,n){for(var i=n?Pt:Nt,o=e[0].length,u=e.length,l=u,s=r(u),c=1/0,f=[];l--;){var p=e[l];l&&t&&(p=zt(p,Gt(t))),c=yn(p.length,c),s[l]=!n&&(t||o>=120&&p.length>=120)?new qn(l&&p):a}p=e[0];var d=-1,h=s[0];e:for(;++d<o&&f.length<c;){var v=p[d],g=t?t(v):v;if(v=n||0!==v?v:0,!(h?Zt(h,g):i(f,g,n))){for(l=u;--l;){var y=s[l];if(!(y?Zt(y,g):i(e[l],g,n)))continue e}h&&h.push(g),f.push(v)}}return f}function Mr(e,t,n){var r=null==(e=Si(e,t=ga(t,e)))?e:e[Fi(Yi(t))];return null==r?a:kt(r,e,n)}function Nr(e){return eu(e)&&kr(e)==v}function Pr(e,t,n,r,i){return e===t||(null==e||null==t||!eu(e)&&!eu(t)?e!=e&&t!=t:function(e,t,n,r,i,o){var u=Wo(e),l=Wo(t),s=u?g:fi(e),c=l?g:fi(t),f=(s=s==v?S:s)==S,p=(c=c==v?S:c)==S,d=s==c;if(d&&Qo(e)){if(!Qo(t))return!1;u=!0,f=!1}if(d&&!f)return o||(o=new Qn),u||lu(e)?Xa(e,t,n,r,i,o):function(e,t,n,r,a,i,o){switch(n){case L:if(e.byteLength!=t.byteLength||e.byteOffset!=t.byteOffset)return!1;e=e.buffer,t=t.buffer;case z:return!(e.byteLength!=t.byteLength||!i(new Be(e),new Be(t)));case y:case m:case k:return Uo(+e,+t);case b:return e.name==t.name&&e.message==t.message;case C:case M:return e==t+\"\";case x:var u=an;case T:var l=1&r;if(u||(u=ln),e.size!=t.size&&!l)return!1;var s=o.get(e);if(s)return s==t;r|=2,o.set(e,t);var c=Xa(u(e),u(t),r,a,i,o);return o.delete(e),c;case N:if(Dn)return Dn.call(e)==Dn.call(t)}return!1}(e,t,s,n,r,i,o);if(!(1&n)){var h=f&&Oe.call(e,\"__wrapped__\"),_=p&&Oe.call(t,\"__wrapped__\");if(h||_){var w=h?e.value():e,E=_?t.value():t;return o||(o=new Qn),i(w,E,n,r,o)}}return!!d&&(o||(o=new Qn),function(e,t,n,r,i,o){var u=1&n,l=ei(e),s=l.length;if(s!=ei(t).length&&!u)return!1;for(var c=s;c--;){var f=l[c];if(!(u?f in t:Oe.call(t,f)))return!1}var p=o.get(e),d=o.get(t);if(p&&d)return p==t&&d==e;var h=!0;o.set(e,t),o.set(t,e);for(var v=u;++c<s;){var g=e[f=l[c]],y=t[f];if(r)var m=u?r(y,g,f,t,e,o):r(g,y,f,e,t,o);if(!(m===a?g===y||i(g,y,n,r,o):m)){h=!1;break}v||(v=\"constructor\"==f)}if(h&&!v){var b=e.constructor,_=t.constructor;b==_||!(\"constructor\"in e)||!(\"constructor\"in t)||\"function\"==typeof b&&b instanceof b&&\"function\"==typeof _&&_ instanceof _||(h=!1)}return o.delete(e),o.delete(t),h}(e,t,n,r,i,o))}(e,t,n,r,Pr,i))}function zr(e,t,n,r){var i=n.length,o=i,u=!r;if(null==e)return!o;for(e=Se(e);i--;){var l=n[i];if(u&&l[2]?l[1]!==e[l[0]]:!(l[0]in e))return!1}for(;++i<o;){var s=(l=n[i])[0],c=e[s],f=l[1];if(u&&l[2]){if(c===a&&!(s in e))return!1}else{var p=new Qn;if(r)var d=r(c,f,s,e,t,p);if(!(d===a?Pr(f,c,3,r,p):d))return!1}}return!0}function Lr(e){return!(!Jo(e)||(t=e,Fe&&Fe in t))&&(Ko(e)?Ue:ve).test(Di(e));var t}function Or(e){return\"function\"==typeof e?e:null==e?nl:\"object\"==typeof e?Wo(e)?jr(e[0],e[1]):Rr(e):fl(e)}function Ar(e){if(!_i(e))return vn(e);var t=[];for(var n in Se(e))Oe.call(e,n)&&\"constructor\"!=n&&t.push(n);return t}function Fr(e,t){return e<t}function Dr(e,t){var n=-1,a=Ho(e)?r(e.length):[];return cr(e,(function(e,r,i){a[++n]=t(e,r,i)})),a}function Rr(e){var t=ui(e);return 1==t.length&&t[0][2]?xi(t[0][0],t[0][1]):function(n){return n===e||zr(n,e,t)}}function jr(e,t){return yi(e)&&wi(t)?xi(Fi(e),t):function(n){var r=Eu(n,e);return r===a&&r===t?Cu(n,e):Pr(t,r,3)}}function Ur(e,t,n,r,i){e!==t&&gr(t,(function(o,u){if(i||(i=new Qn),Jo(o))!function(e,t,n,r,i,o,u){var l=Ei(e,n),s=Ei(t,n),c=u.get(s);if(c)Xn(e,n,c);else{var f=o?o(l,s,n+\"\",e,t,u):a,p=f===a;if(p){var d=Wo(s),h=!d&&Qo(s),v=!d&&!h&&lu(s);f=s,d||h||v?Wo(l)?f=l:qo(l)?f=Ca(l):h?(p=!1,f=_a(s,!0)):v?(p=!1,f=xa(s,!0)):f=[]:ru(s)||Bo(s)?(f=l,Bo(l)?f=gu(l):Jo(l)&&!Ko(l)||(f=di(s))):p=!1}p&&(u.set(s,f),i(f,s,r,o,u),u.delete(s)),Xn(e,n,f)}}(e,t,u,n,Ur,r,i);else{var l=r?r(Ei(e,u),o,u+\"\",e,t,i):a;l===a&&(l=o),Xn(e,u,l)}}),zu)}function Ir(e,t){var n=e.length;if(n)return vi(t+=t<0?n:0,n)?e[t]:a}function $r(e,t,n){t=t.length?zt(t,(function(e){return Wo(e)?function(t){return wr(t,1===e.length?e[0]:e)}:e})):[nl];var r=-1;t=zt(t,Gt(ii()));var a=Dr(e,(function(e,n,a){var i=zt(t,(function(t){return t(e)}));return{criteria:i,index:++r,value:e}}));return function(e,t){var r=e.length;for(e.sort((function(e,t){return function(e,t,n){for(var r=-1,a=e.criteria,i=t.criteria,o=a.length,u=n.length;++r<o;){var l=ka(a[r],i[r]);if(l)return r>=u?l:l*(\"desc\"==n[r]?-1:1)}return e.index-t.index}(e,t,n)}));r--;)e[r]=e[r].value;return e}(a)}function Br(e,t,n){for(var r=-1,a=t.length,i={};++r<a;){var o=t[r],u=wr(e,o);n(u,o)&&Kr(i,ga(o,e),u)}return i}function Wr(e,t,n,r){var a=r?It:Ut,i=-1,o=t.length,u=e;for(e===t&&(t=Ca(t)),n&&(u=zt(e,Gt(n)));++i<o;)for(var l=0,s=t[i],c=n?n(s):s;(l=a(u,c,l,r))>-1;)u!==e&&Qe.call(u,l,1),Qe.call(e,l,1);return e}function Vr(e,t){for(var n=e?t.length:0,r=n-1;n--;){var a=t[n];if(n==r||a!==i){var i=a;vi(a)?Qe.call(e,a,1):la(e,a)}}return e}function Hr(e,t){return e+dt(_n()*(t-e+1))}function qr(e,t){var n=\"\";if(!e||t<1||t>f)return n;do{t%2&&(n+=e),(t=dt(t/2))&&(e+=e)}while(t);return n}function Qr(e,t){return Mi(ki(e,t,nl),e+\"\")}function Yr(e){return Gn(Uu(e))}function Gr(e,t){var n=Uu(e);return zi(n,ir(t,0,n.length))}function Kr(e,t,n,r){if(!Jo(e))return e;for(var i=-1,o=(t=ga(t,e)).length,u=o-1,l=e;null!=l&&++i<o;){var s=Fi(t[i]),c=n;if(\"__proto__\"===s||\"constructor\"===s||\"prototype\"===s)return e;if(i!=u){var f=l[s];(c=r?r(f,s,l):a)===a&&(c=Jo(f)?f:vi(t[i+1])?[]:{})}Jn(l,s,c),l=l[s]}return e}var Zr=Mn?function(e,t){return Mn.set(e,t),e}:nl,Xr=et?function(e,t){return et(e,\"toString\",{configurable:!0,enumerable:!1,value:Ju(t),writable:!0})}:nl;function Jr(e){return zi(Uu(e))}function ea(e,t,n){var a=-1,i=e.length;t<0&&(t=-t>i?0:i+t),(n=n>i?i:n)<0&&(n+=i),i=t>n?0:n-t>>>0,t>>>=0;for(var o=r(i);++a<i;)o[a]=e[a+t];return o}function ta(e,t){var n;return cr(e,(function(e,r,a){return!(n=t(e,r,a))})),!!n}function na(e,t,n){var r=0,a=null==e?r:e.length;if(\"number\"==typeof t&&t==t&&a<=2147483647){for(;r<a;){var i=r+a>>>1,o=e[i];null!==o&&!uu(o)&&(n?o<=t:o<t)?r=i+1:a=i}return a}return ra(e,t,nl,n)}function ra(e,t,n,r){var i=0,o=null==e?0:e.length;if(0===o)return 0;for(var u=(t=n(t))!=t,l=null===t,s=uu(t),c=t===a;i<o;){var f=dt((i+o)/2),p=n(e[f]),d=p!==a,h=null===p,v=p==p,g=uu(p);if(u)var y=r||v;else y=c?v&&(r||d):l?v&&d&&(r||!h):s?v&&d&&!h&&(r||!g):!h&&!g&&(r?p<=t:p<t);y?i=f+1:o=f}return yn(o,4294967294)}function aa(e,t){for(var n=-1,r=e.length,a=0,i=[];++n<r;){var o=e[n],u=t?t(o):o;if(!n||!Uo(u,l)){var l=u;i[a++]=0===o?0:o}}return i}function ia(e){return\"number\"==typeof e?e:uu(e)?p:+e}function oa(e){if(\"string\"==typeof e)return e;if(Wo(e))return zt(e,oa)+\"\";if(uu(e))return Rn?Rn.call(e):\"\";var t=e+\"\";return\"0\"==t&&1/e==-1/0?\"-0\":t}function ua(e,t,n){var r=-1,a=Nt,i=e.length,o=!0,u=[],l=u;if(n)o=!1,a=Pt;else if(i>=200){var s=t?null:qa(e);if(s)return ln(s);o=!1,a=Zt,l=new qn}else l=t?[]:u;e:for(;++r<i;){var c=e[r],f=t?t(c):c;if(c=n||0!==c?c:0,o&&f==f){for(var p=l.length;p--;)if(l[p]===f)continue e;t&&l.push(f),u.push(c)}else a(l,f,n)||(l!==u&&l.push(f),u.push(c))}return u}function la(e,t){return null==(e=Si(e,t=ga(t,e)))||delete e[Fi(Yi(t))]}function sa(e,t,n,r){return Kr(e,t,n(wr(e,t)),r)}function ca(e,t,n,r){for(var a=e.length,i=r?a:-1;(r?i--:++i<a)&&t(e[i],i,e););return n?ea(e,r?0:i,r?i+1:a):ea(e,r?i+1:0,r?a:i)}function fa(e,t){var n=e;return n instanceof Bn&&(n=n.value()),Ot(t,(function(e,t){return t.func.apply(t.thisArg,Lt([e],t.args))}),n)}function pa(e,t,n){var a=e.length;if(a<2)return a?ua(e[0]):[];for(var i=-1,o=r(a);++i<a;)for(var u=e[i],l=-1;++l<a;)l!=i&&(o[i]=sr(o[i]||u,e[l],t,n));return ua(vr(o,1),t,n)}function da(e,t,n){for(var r=-1,i=e.length,o=t.length,u={};++r<i;){var l=r<o?t[r]:a;n(u,e[r],l)}return u}function ha(e){return qo(e)?e:[]}function va(e){return\"function\"==typeof e?e:nl}function ga(e,t){return Wo(e)?e:yi(e,t)?[e]:Ai(yu(e))}var ya=Qr;function ma(e,t,n){var r=e.length;return n=n===a?r:n,!t&&n>=r?e:ea(e,t,n)}var ba=ot||function(e){return ft.clearTimeout(e)};function _a(e,t){if(t)return e.slice();var n=e.length,r=We?We(n):new e.constructor(n);return e.copy(r),r}function wa(e){var t=new e.constructor(e.byteLength);return new Be(t).set(new Be(e)),t}function xa(e,t){var n=t?wa(e.buffer):e.buffer;return new e.constructor(n,e.byteOffset,e.length)}function ka(e,t){if(e!==t){var n=e!==a,r=null===e,i=e==e,o=uu(e),u=t!==a,l=null===t,s=t==t,c=uu(t);if(!l&&!c&&!o&&e>t||o&&u&&s&&!l&&!c||r&&u&&s||!n&&s||!i)return 1;if(!r&&!o&&!c&&e<t||c&&n&&i&&!r&&!o||l&&n&&i||!u&&i||!s)return-1}return 0}function Sa(e,t,n,a){for(var i=-1,o=e.length,u=n.length,l=-1,s=t.length,c=gn(o-u,0),f=r(s+c),p=!a;++l<s;)f[l]=t[l];for(;++i<u;)(p||i<o)&&(f[n[i]]=e[i]);for(;c--;)f[l++]=e[i++];return f}function Ea(e,t,n,a){for(var i=-1,o=e.length,u=-1,l=n.length,s=-1,c=t.length,f=gn(o-l,0),p=r(f+c),d=!a;++i<f;)p[i]=e[i];for(var h=i;++s<c;)p[h+s]=t[s];for(;++u<l;)(d||i<o)&&(p[h+n[u]]=e[i++]);return p}function Ca(e,t){var n=-1,a=e.length;for(t||(t=r(a));++n<a;)t[n]=e[n];return t}function Ta(e,t,n,r){var i=!n;n||(n={});for(var o=-1,u=t.length;++o<u;){var l=t[o],s=r?r(n[l],e[l],l,n,e):a;s===a&&(s=e[l]),i?rr(n,l,s):Jn(n,l,s)}return n}function Ma(e,t){return function(n,r){var a=Wo(n)?St:tr,i=t?t():{};return a(n,e,ii(r,2),i)}}function Na(e){return Qr((function(t,n){var r=-1,i=n.length,o=i>1?n[i-1]:a,u=i>2?n[2]:a;for(o=e.length>3&&\"function\"==typeof o?(i--,o):a,u&&gi(n[0],n[1],u)&&(o=i<3?a:o,i=1),t=Se(t);++r<i;){var l=n[r];l&&e(t,l,r,o)}return t}))}function Pa(e,t){return function(n,r){if(null==n)return n;if(!Ho(n))return e(n,r);for(var a=n.length,i=t?a:-1,o=Se(n);(t?i--:++i<a)&&!1!==r(o[i],i,o););return n}}function za(e){return function(t,n,r){for(var a=-1,i=Se(t),o=r(t),u=o.length;u--;){var l=o[e?u:++a];if(!1===n(i[l],l,i))break}return t}}function La(e){return function(t){var n=rn(t=yu(t))?fn(t):a,r=n?n[0]:t.charAt(0),i=n?ma(n,1).join(\"\"):t.slice(1);return r[e]()+i}}function Oa(e){return function(t){return Ot(Ku(Bu(t).replace(Ke,\"\")),e,\"\")}}function Aa(e){return function(){var t=arguments;switch(t.length){case 0:return new e;case 1:return new e(t[0]);case 2:return new e(t[0],t[1]);case 3:return new e(t[0],t[1],t[2]);case 4:return new e(t[0],t[1],t[2],t[3]);case 5:return new e(t[0],t[1],t[2],t[3],t[4]);case 6:return new e(t[0],t[1],t[2],t[3],t[4],t[5]);case 7:return new e(t[0],t[1],t[2],t[3],t[4],t[5],t[6])}var n=Un(e.prototype),r=e.apply(n,t);return Jo(r)?r:n}}function Fa(e){return function(t,n,r){var i=Se(t);if(!Ho(t)){var o=ii(n,3);t=Pu(t),n=function(e){return o(i[e],e,i)}}var u=e(t,n,r);return u>-1?i[o?t[u]:u]:a}}function Da(e){return Ja((function(t){var n=t.length,r=n,o=$n.prototype.thru;for(e&&t.reverse();r--;){var u=t[r];if(\"function\"!=typeof u)throw new Te(i);if(o&&!l&&\"wrapper\"==ri(u))var l=new $n([],!0)}for(r=l?r:n;++r<n;){var s=ri(u=t[r]),c=\"wrapper\"==s?ni(u):a;l=c&&mi(c[0])&&424==c[1]&&!c[4].length&&1==c[9]?l[ri(c[0])].apply(l,c[3]):1==u.length&&mi(u)?l[s]():l.thru(u)}return function(){var e=arguments,r=e[0];if(l&&1==e.length&&Wo(r))return l.plant(r).value();for(var a=0,i=n?t[a].apply(this,e):r;++a<n;)i=t[a].call(this,i);return i}}))}function Ra(e,t,n,i,o,u,l,c,f,p){var d=t&s,h=1&t,v=2&t,g=24&t,y=512&t,m=v?a:Aa(e);return function s(){for(var b=arguments.length,_=r(b),w=b;w--;)_[w]=arguments[w];if(g)var x=ai(s),k=function(e,t){for(var n=e.length,r=0;n--;)e[n]===t&&++r;return r}(_,x);if(i&&(_=Sa(_,i,o,g)),u&&(_=Ea(_,u,l,g)),b-=k,g&&b<p){var S=un(_,x);return Va(e,t,Ra,s.placeholder,n,_,S,c,f,p-b)}var E=h?n:this,C=v?E[e]:e;return b=_.length,c?_=function(e,t){for(var n=e.length,r=yn(t.length,n),i=Ca(e);r--;){var o=t[r];e[r]=vi(o,n)?i[o]:a}return e}(_,c):y&&b>1&&_.reverse(),d&&f<b&&(_.length=f),this&&this!==ft&&this instanceof s&&(C=m||Aa(C)),C.apply(E,_)}}function ja(e,t){return function(n,r){return function(e,t,n,r){return mr(e,(function(e,a,i){t(r,n(e),a,i)})),r}(n,e,t(r),{})}}function Ua(e,t){return function(n,r){var i;if(n===a&&r===a)return t;if(n!==a&&(i=n),r!==a){if(i===a)return r;\"string\"==typeof n||\"string\"==typeof r?(n=oa(n),r=oa(r)):(n=ia(n),r=ia(r)),i=e(n,r)}return i}}function Ia(e){return Ja((function(t){return t=zt(t,Gt(ii())),Qr((function(n){var r=this;return e(t,(function(e){return kt(e,r,n)}))}))}))}function $a(e,t){var n=(t=t===a?\" \":oa(t)).length;if(n<2)return n?qr(t,e):t;var r=qr(t,pt(e/cn(t)));return rn(t)?ma(fn(r),0,e).join(\"\"):r.slice(0,e)}function Ba(e){return function(t,n,i){return i&&\"number\"!=typeof i&&gi(t,n,i)&&(n=i=a),t=pu(t),n===a?(n=t,t=0):n=pu(n),function(e,t,n,a){for(var i=-1,o=gn(pt((t-e)/(n||1)),0),u=r(o);o--;)u[a?o:++i]=e,e+=n;return u}(t,n,i=i===a?t<n?1:-1:pu(i),e)}}function Wa(e){return function(t,n){return\"string\"==typeof t&&\"string\"==typeof n||(t=vu(t),n=vu(n)),e(t,n)}}function Va(e,t,n,r,i,o,u,s,c,f){var p=8&t;t|=p?l:64,4&(t&=~(p?64:l))||(t&=-4);var d=[e,t,i,p?o:a,p?u:a,p?a:o,p?a:u,s,c,f],h=n.apply(a,d);return mi(e)&&Ci(h,d),h.placeholder=r,Ni(h,e,t)}function Ha(e){var t=ke[e];return function(e,n){if(e=vu(e),(n=null==n?0:yn(du(n),292))&&Dt(e)){var r=(yu(e)+\"e\").split(\"e\");return+((r=(yu(t(r[0]+\"e\"+(+r[1]+n)))+\"e\").split(\"e\"))[0]+\"e\"+(+r[1]-n))}return t(e)}}var qa=En&&1/ln(new En([,-0]))[1]==c?function(e){return new En(e)}:ul;function Qa(e){return function(t){var n=fi(t);return n==x?an(t):n==T?sn(t):function(e,t){return zt(t,(function(t){return[t,e[t]]}))}(t,e(t))}}function Ya(e,t,n,o,c,f,p,d){var h=2&t;if(!h&&\"function\"!=typeof e)throw new Te(i);var v=o?o.length:0;if(v||(t&=-97,o=c=a),p=p===a?p:gn(du(p),0),d=d===a?d:du(d),v-=c?c.length:0,64&t){var g=o,y=c;o=c=a}var m=h?a:ni(e),b=[e,t,n,o,c,g,y,f,p,d];if(m&&function(e,t){var n=e[1],r=t[1],a=n|r,i=a<131,o=r==s&&8==n||r==s&&256==n&&e[7].length<=t[8]||384==r&&t[7].length<=t[8]&&8==n;if(!i&&!o)return e;1&r&&(e[2]=t[2],a|=1&n?0:4);var l=t[3];if(l){var c=e[3];e[3]=c?Sa(c,l,t[4]):l,e[4]=c?un(e[3],u):t[4]}(l=t[5])&&(c=e[5],e[5]=c?Ea(c,l,t[6]):l,e[6]=c?un(e[5],u):t[6]),(l=t[7])&&(e[7]=l),r&s&&(e[8]=null==e[8]?t[8]:yn(e[8],t[8])),null==e[9]&&(e[9]=t[9]),e[0]=t[0],e[1]=a}(b,m),e=b[0],t=b[1],n=b[2],o=b[3],c=b[4],!(d=b[9]=b[9]===a?h?0:e.length:gn(b[9]-v,0))&&24&t&&(t&=-25),t&&1!=t)_=8==t||16==t?function(e,t,n){var i=Aa(e);return function o(){for(var u=arguments.length,l=r(u),s=u,c=ai(o);s--;)l[s]=arguments[s];var f=u<3&&l[0]!==c&&l[u-1]!==c?[]:un(l,c);return(u-=f.length)<n?Va(e,t,Ra,o.placeholder,a,l,f,a,a,n-u):kt(this&&this!==ft&&this instanceof o?i:e,this,l)}}(e,t,d):t!=l&&33!=t||c.length?Ra.apply(a,b):function(e,t,n,a){var i=1&t,o=Aa(e);return function t(){for(var u=-1,l=arguments.length,s=-1,c=a.length,f=r(c+l),p=this&&this!==ft&&this instanceof t?o:e;++s<c;)f[s]=a[s];for(;l--;)f[s++]=arguments[++u];return kt(p,i?n:this,f)}}(e,t,n,o);else var _=function(e,t,n){var r=1&t,a=Aa(e);return function t(){return(this&&this!==ft&&this instanceof t?a:e).apply(r?n:this,arguments)}}(e,t,n);return Ni((m?Zr:Ci)(_,b),e,t)}function Ga(e,t,n,r){return e===a||Uo(e,Pe[n])&&!Oe.call(r,n)?t:e}function Ka(e,t,n,r,i,o){return Jo(e)&&Jo(t)&&(o.set(t,e),Ur(e,t,a,Ka,o),o.delete(t)),e}function Za(e){return ru(e)?a:e}function Xa(e,t,n,r,i,o){var u=1&n,l=e.length,s=t.length;if(l!=s&&!(u&&s>l))return!1;var c=o.get(e),f=o.get(t);if(c&&f)return c==t&&f==e;var p=-1,d=!0,h=2&n?new qn:a;for(o.set(e,t),o.set(t,e);++p<l;){var v=e[p],g=t[p];if(r)var y=u?r(g,v,p,t,e,o):r(v,g,p,e,t,o);if(y!==a){if(y)continue;d=!1;break}if(h){if(!Ft(t,(function(e,t){if(!Zt(h,t)&&(v===e||i(v,e,n,r,o)))return h.push(t)}))){d=!1;break}}else if(v!==g&&!i(v,g,n,r,o)){d=!1;break}}return o.delete(e),o.delete(t),d}function Ja(e){return Mi(ki(e,a,Wi),e+\"\")}function ei(e){return xr(e,Pu,si)}function ti(e){return xr(e,zu,ci)}var ni=Mn?function(e){return Mn.get(e)}:ul;function ri(e){for(var t=e.name+\"\",n=Nn[t],r=Oe.call(Nn,t)?n.length:0;r--;){var a=n[r],i=a.func;if(null==i||i==e)return a.name}return t}function ai(e){return(Oe.call(jn,\"placeholder\")?jn:e).placeholder}function ii(){var e=jn.iteratee||rl;return e=e===rl?Or:e,arguments.length?e(arguments[0],arguments[1]):e}function oi(e,t){var n,r,a=e.__data__;return(\"string\"==(r=typeof(n=t))||\"number\"==r||\"symbol\"==r||\"boolean\"==r?\"__proto__\"!==n:null===n)?a[\"string\"==typeof t?\"string\":\"hash\"]:a.map}function ui(e){for(var t=Pu(e),n=t.length;n--;){var r=t[n],a=e[r];t[n]=[r,a,wi(a)]}return t}function li(e,t){var n=function(e,t){return null==e?a:e[t]}(e,t);return Lr(n)?n:a}var si=vt?function(e){return null==e?[]:(e=Se(e),Mt(vt(e),(function(t){return qe.call(e,t)})))}:hl,ci=vt?function(e){for(var t=[];e;)Lt(t,si(e)),e=Ve(e);return t}:hl,fi=kr;function pi(e,t,n){for(var r=-1,a=(t=ga(t,e)).length,i=!1;++r<a;){var o=Fi(t[r]);if(!(i=null!=e&&n(e,o)))break;e=e[o]}return i||++r!=a?i:!!(a=null==e?0:e.length)&&Xo(a)&&vi(o,a)&&(Wo(e)||Bo(e))}function di(e){return\"function\"!=typeof e.constructor||_i(e)?{}:Un(Ve(e))}function hi(e){return Wo(e)||Bo(e)||!!(Ye&&e&&e[Ye])}function vi(e,t){var n=typeof e;return!!(t=null==t?f:t)&&(\"number\"==n||\"symbol\"!=n&&ye.test(e))&&e>-1&&e%1==0&&e<t}function gi(e,t,n){if(!Jo(n))return!1;var r=typeof t;return!!(\"number\"==r?Ho(n)&&vi(t,n.length):\"string\"==r&&t in n)&&Uo(n[t],e)}function yi(e,t){if(Wo(e))return!1;var n=typeof e;return!(\"number\"!=n&&\"symbol\"!=n&&\"boolean\"!=n&&null!=e&&!uu(e))||J.test(e)||!X.test(e)||null!=t&&e in Se(t)}function mi(e){var t=ri(e),n=jn[t];if(\"function\"!=typeof n||!(t in Bn.prototype))return!1;if(e===n)return!0;var r=ni(n);return!!r&&e===r[0]}(xn&&fi(new xn(new ArrayBuffer(1)))!=L||kn&&fi(new kn)!=x||Sn&&fi(Sn.resolve())!=E||En&&fi(new En)!=T||Cn&&fi(new Cn)!=P)&&(fi=function(e){var t=kr(e),n=t==S?e.constructor:a,r=n?Di(n):\"\";if(r)switch(r){case Pn:return L;case zn:return x;case Ln:return E;case On:return T;case An:return P}return t});var bi=ze?Ko:vl;function _i(e){var t=e&&e.constructor;return e===(\"function\"==typeof t&&t.prototype||Pe)}function wi(e){return e==e&&!Jo(e)}function xi(e,t){return function(n){return null!=n&&n[e]===t&&(t!==a||e in Se(n))}}function ki(e,t,n){return t=gn(t===a?e.length-1:t,0),function(){for(var a=arguments,i=-1,o=gn(a.length-t,0),u=r(o);++i<o;)u[i]=a[t+i];i=-1;for(var l=r(t+1);++i<t;)l[i]=a[i];return l[t]=n(u),kt(e,this,l)}}function Si(e,t){return t.length<2?e:wr(e,ea(t,0,-1))}function Ei(e,t){if((\"constructor\"!==t||\"function\"!=typeof e[t])&&\"__proto__\"!=t)return e[t]}var Ci=Pi(Zr),Ti=ct||function(e,t){return ft.setTimeout(e,t)},Mi=Pi(Xr);function Ni(e,t,n){var r=t+\"\";return Mi(e,function(e,t){var n=t.length;if(!n)return e;var r=n-1;return t[r]=(n>1?\"& \":\"\")+t[r],t=t.join(n>2?\", \":\" \"),e.replace(ie,\"{\\n/* [wrapped with \"+t+\"] */\\n\")}(r,function(e,t){return Et(h,(function(n){var r=\"_.\"+n[0];t&n[1]&&!Nt(e,r)&&e.push(r)})),e.sort()}(function(e){var t=e.match(oe);return t?t[1].split(ue):[]}(r),n)))}function Pi(e){var t=0,n=0;return function(){var r=mn(),i=16-(r-n);if(n=r,i>0){if(++t>=800)return arguments[0]}else t=0;return e.apply(a,arguments)}}function zi(e,t){var n=-1,r=e.length,i=r-1;for(t=t===a?r:t;++n<t;){var o=Hr(n,i),u=e[o];e[o]=e[n],e[n]=u}return e.length=t,e}var Li,Oi,Ai=(Li=Oo((function(e){var t=[];return 46===e.charCodeAt(0)&&t.push(\"\"),e.replace(ee,(function(e,n,r,a){t.push(r?a.replace(ce,\"$1\"):n||e)})),t}),(function(e){return 500===Oi.size&&Oi.clear(),e})),Oi=Li.cache,Li);function Fi(e){if(\"string\"==typeof e||uu(e))return e;var t=e+\"\";return\"0\"==t&&1/e==-1/0?\"-0\":t}function Di(e){if(null!=e){try{return Le.call(e)}catch(e){}try{return e+\"\"}catch(e){}}return\"\"}function Ri(e){if(e instanceof Bn)return e.clone();var t=new $n(e.__wrapped__,e.__chain__);return t.__actions__=Ca(e.__actions__),t.__index__=e.__index__,t.__values__=e.__values__,t}var ji=Qr((function(e,t){return qo(e)?sr(e,vr(t,1,qo,!0)):[]})),Ui=Qr((function(e,t){var n=Yi(t);return qo(n)&&(n=a),qo(e)?sr(e,vr(t,1,qo,!0),ii(n,2)):[]})),Ii=Qr((function(e,t){var n=Yi(t);return qo(n)&&(n=a),qo(e)?sr(e,vr(t,1,qo,!0),a,n):[]}));function $i(e,t,n){var r=null==e?0:e.length;if(!r)return-1;var a=null==n?0:du(n);return a<0&&(a=gn(r+a,0)),jt(e,ii(t,3),a)}function Bi(e,t,n){var r=null==e?0:e.length;if(!r)return-1;var i=r-1;return n!==a&&(i=du(n),i=n<0?gn(r+i,0):yn(i,r-1)),jt(e,ii(t,3),i,!0)}function Wi(e){return null!=e&&e.length?vr(e,1):[]}function Vi(e){return e&&e.length?e[0]:a}var Hi=Qr((function(e){var t=zt(e,ha);return t.length&&t[0]===e[0]?Tr(t):[]})),qi=Qr((function(e){var t=Yi(e),n=zt(e,ha);return t===Yi(n)?t=a:n.pop(),n.length&&n[0]===e[0]?Tr(n,ii(t,2)):[]})),Qi=Qr((function(e){var t=Yi(e),n=zt(e,ha);return(t=\"function\"==typeof t?t:a)&&n.pop(),n.length&&n[0]===e[0]?Tr(n,a,t):[]}));function Yi(e){var t=null==e?0:e.length;return t?e[t-1]:a}var Gi=Qr(Ki);function Ki(e,t){return e&&e.length&&t&&t.length?Wr(e,t):e}var Zi=Ja((function(e,t){var n=null==e?0:e.length,r=ar(e,t);return Vr(e,zt(t,(function(e){return vi(e,n)?+e:e})).sort(ka)),r}));function Xi(e){return null==e?e:wn.call(e)}var Ji=Qr((function(e){return ua(vr(e,1,qo,!0))})),eo=Qr((function(e){var t=Yi(e);return qo(t)&&(t=a),ua(vr(e,1,qo,!0),ii(t,2))})),to=Qr((function(e){var t=Yi(e);return t=\"function\"==typeof t?t:a,ua(vr(e,1,qo,!0),a,t)}));function no(e){if(!e||!e.length)return[];var t=0;return e=Mt(e,(function(e){if(qo(e))return t=gn(e.length,t),!0})),Qt(t,(function(t){return zt(e,Wt(t))}))}function ro(e,t){if(!e||!e.length)return[];var n=no(e);return null==t?n:zt(n,(function(e){return kt(t,a,e)}))}var ao=Qr((function(e,t){return qo(e)?sr(e,t):[]})),io=Qr((function(e){return pa(Mt(e,qo))})),oo=Qr((function(e){var t=Yi(e);return qo(t)&&(t=a),pa(Mt(e,qo),ii(t,2))})),uo=Qr((function(e){var t=Yi(e);return t=\"function\"==typeof t?t:a,pa(Mt(e,qo),a,t)})),lo=Qr(no),so=Qr((function(e){var t=e.length,n=t>1?e[t-1]:a;return n=\"function\"==typeof n?(e.pop(),n):a,ro(e,n)}));function co(e){var t=jn(e);return t.__chain__=!0,t}function fo(e,t){return t(e)}var po=Ja((function(e){var t=e.length,n=t?e[0]:0,r=this.__wrapped__,i=function(t){return ar(t,e)};return!(t>1||this.__actions__.length)&&r instanceof Bn&&vi(n)?((r=r.slice(n,+n+(t?1:0))).__actions__.push({func:fo,args:[i],thisArg:a}),new $n(r,this.__chain__).thru((function(e){return t&&!e.length&&e.push(a),e}))):this.thru(i)})),ho=Ma((function(e,t,n){Oe.call(e,n)?++e[n]:rr(e,n,1)})),vo=Fa($i),go=Fa(Bi);function yo(e,t){return(Wo(e)?Et:cr)(e,ii(t,3))}function mo(e,t){return(Wo(e)?Ct:fr)(e,ii(t,3))}var bo=Ma((function(e,t,n){Oe.call(e,n)?e[n].push(t):rr(e,n,[t])})),_o=Qr((function(e,t,n){var a=-1,i=\"function\"==typeof t,o=Ho(e)?r(e.length):[];return cr(e,(function(e){o[++a]=i?kt(t,e,n):Mr(e,t,n)})),o})),wo=Ma((function(e,t,n){rr(e,n,t)}));function xo(e,t){return(Wo(e)?zt:Dr)(e,ii(t,3))}var ko=Ma((function(e,t,n){e[n?0:1].push(t)}),(function(){return[[],[]]})),So=Qr((function(e,t){if(null==e)return[];var n=t.length;return n>1&&gi(e,t[0],t[1])?t=[]:n>2&&gi(t[0],t[1],t[2])&&(t=[t[0]]),$r(e,vr(t,1),[])})),Eo=st||function(){return ft.Date.now()};function Co(e,t,n){return t=n?a:t,t=e&&null==t?e.length:t,Ya(e,s,a,a,a,a,t)}function To(e,t){var n;if(\"function\"!=typeof t)throw new Te(i);return e=du(e),function(){return--e>0&&(n=t.apply(this,arguments)),e<=1&&(t=a),n}}var Mo=Qr((function(e,t,n){var r=1;if(n.length){var a=un(n,ai(Mo));r|=l}return Ya(e,r,t,n,a)})),No=Qr((function(e,t,n){var r=3;if(n.length){var a=un(n,ai(No));r|=l}return Ya(t,r,e,n,a)}));function Po(e,t,n){var r,o,u,l,s,c,f=0,p=!1,d=!1,h=!0;if(\"function\"!=typeof e)throw new Te(i);function v(t){var n=r,i=o;return r=o=a,f=t,l=e.apply(i,n)}function g(e){var n=e-c;return c===a||n>=t||n<0||d&&e-f>=u}function y(){var e=Eo();if(g(e))return m(e);s=Ti(y,function(e){var n=t-(e-c);return d?yn(n,u-(e-f)):n}(e))}function m(e){return s=a,h&&r?v(e):(r=o=a,l)}function b(){var e=Eo(),n=g(e);if(r=arguments,o=this,c=e,n){if(s===a)return function(e){return f=e,s=Ti(y,t),p?v(e):l}(c);if(d)return ba(s),s=Ti(y,t),v(c)}return s===a&&(s=Ti(y,t)),l}return t=vu(t)||0,Jo(n)&&(p=!!n.leading,u=(d=\"maxWait\"in n)?gn(vu(n.maxWait)||0,t):u,h=\"trailing\"in n?!!n.trailing:h),b.cancel=function(){s!==a&&ba(s),f=0,r=c=o=s=a},b.flush=function(){return s===a?l:m(Eo())},b}var zo=Qr((function(e,t){return lr(e,1,t)})),Lo=Qr((function(e,t,n){return lr(e,vu(t)||0,n)}));function Oo(e,t){if(\"function\"!=typeof e||null!=t&&\"function\"!=typeof t)throw new Te(i);var n=function(){var r=arguments,a=t?t.apply(this,r):r[0],i=n.cache;if(i.has(a))return i.get(a);var o=e.apply(this,r);return n.cache=i.set(a,o)||i,o};return n.cache=new(Oo.Cache||Hn),n}function Ao(e){if(\"function\"!=typeof e)throw new Te(i);return function(){var t=arguments;switch(t.length){case 0:return!e.call(this);case 1:return!e.call(this,t[0]);case 2:return!e.call(this,t[0],t[1]);case 3:return!e.call(this,t[0],t[1],t[2])}return!e.apply(this,t)}}Oo.Cache=Hn;var Fo=ya((function(e,t){var n=(t=1==t.length&&Wo(t[0])?zt(t[0],Gt(ii())):zt(vr(t,1),Gt(ii()))).length;return Qr((function(r){for(var a=-1,i=yn(r.length,n);++a<i;)r[a]=t[a].call(this,r[a]);return kt(e,this,r)}))})),Do=Qr((function(e,t){var n=un(t,ai(Do));return Ya(e,l,a,t,n)})),Ro=Qr((function(e,t){var n=un(t,ai(Ro));return Ya(e,64,a,t,n)})),jo=Ja((function(e,t){return Ya(e,256,a,a,a,t)}));function Uo(e,t){return e===t||e!=e&&t!=t}var Io=Wa(Sr),$o=Wa((function(e,t){return e>=t})),Bo=Nr(function(){return arguments}())?Nr:function(e){return eu(e)&&Oe.call(e,\"callee\")&&!qe.call(e,\"callee\")},Wo=r.isArray,Vo=yt?Gt(yt):function(e){return eu(e)&&kr(e)==z};function Ho(e){return null!=e&&Xo(e.length)&&!Ko(e)}function qo(e){return eu(e)&&Ho(e)}var Qo=gt||vl,Yo=mt?Gt(mt):function(e){return eu(e)&&kr(e)==m};function Go(e){if(!eu(e))return!1;var t=kr(e);return t==b||\"[object DOMException]\"==t||\"string\"==typeof e.message&&\"string\"==typeof e.name&&!ru(e)}function Ko(e){if(!Jo(e))return!1;var t=kr(e);return t==_||t==w||\"[object AsyncFunction]\"==t||\"[object Proxy]\"==t}function Zo(e){return\"number\"==typeof e&&e==du(e)}function Xo(e){return\"number\"==typeof e&&e>-1&&e%1==0&&e<=f}function Jo(e){var t=typeof e;return null!=e&&(\"object\"==t||\"function\"==t)}function eu(e){return null!=e&&\"object\"==typeof e}var tu=bt?Gt(bt):function(e){return eu(e)&&fi(e)==x};function nu(e){return\"number\"==typeof e||eu(e)&&kr(e)==k}function ru(e){if(!eu(e)||kr(e)!=S)return!1;var t=Ve(e);if(null===t)return!0;var n=Oe.call(t,\"constructor\")&&t.constructor;return\"function\"==typeof n&&n instanceof n&&Le.call(n)==Re}var au=_t?Gt(_t):function(e){return eu(e)&&kr(e)==C},iu=wt?Gt(wt):function(e){return eu(e)&&fi(e)==T};function ou(e){return\"string\"==typeof e||!Wo(e)&&eu(e)&&kr(e)==M}function uu(e){return\"symbol\"==typeof e||eu(e)&&kr(e)==N}var lu=xt?Gt(xt):function(e){return eu(e)&&Xo(e.length)&&!!at[kr(e)]},su=Wa(Fr),cu=Wa((function(e,t){return e<=t}));function fu(e){if(!e)return[];if(Ho(e))return ou(e)?fn(e):Ca(e);if(Ge&&e[Ge])return function(e){for(var t,n=[];!(t=e.next()).done;)n.push(t.value);return n}(e[Ge]());var t=fi(e);return(t==x?an:t==T?ln:Uu)(e)}function pu(e){return e?(e=vu(e))===c||e===-1/0?17976931348623157e292*(e<0?-1:1):e==e?e:0:0===e?e:0}function du(e){var t=pu(e),n=t%1;return t==t?n?t-n:t:0}function hu(e){return e?ir(du(e),0,d):0}function vu(e){if(\"number\"==typeof e)return e;if(uu(e))return p;if(Jo(e)){var t=\"function\"==typeof e.valueOf?e.valueOf():e;e=Jo(t)?t+\"\":t}if(\"string\"!=typeof e)return 0===e?e:+e;e=Yt(e);var n=he.test(e);return n||ge.test(e)?lt(e.slice(2),n?2:8):de.test(e)?p:+e}function gu(e){return Ta(e,zu(e))}function yu(e){return null==e?\"\":oa(e)}var mu=Na((function(e,t){if(_i(t)||Ho(t))Ta(t,Pu(t),e);else for(var n in t)Oe.call(t,n)&&Jn(e,n,t[n])})),bu=Na((function(e,t){Ta(t,zu(t),e)})),_u=Na((function(e,t,n,r){Ta(t,zu(t),e,r)})),wu=Na((function(e,t,n,r){Ta(t,Pu(t),e,r)})),xu=Ja(ar),ku=Qr((function(e,t){e=Se(e);var n=-1,r=t.length,i=r>2?t[2]:a;for(i&&gi(t[0],t[1],i)&&(r=1);++n<r;)for(var o=t[n],u=zu(o),l=-1,s=u.length;++l<s;){var c=u[l],f=e[c];(f===a||Uo(f,Pe[c])&&!Oe.call(e,c))&&(e[c]=o[c])}return e})),Su=Qr((function(e){return e.push(a,Ka),kt(Ou,a,e)}));function Eu(e,t,n){var r=null==e?a:wr(e,t);return r===a?n:r}function Cu(e,t){return null!=e&&pi(e,t,Cr)}var Tu=ja((function(e,t,n){null!=t&&\"function\"!=typeof t.toString&&(t=De.call(t)),e[t]=n}),Ju(nl)),Mu=ja((function(e,t,n){null!=t&&\"function\"!=typeof t.toString&&(t=De.call(t)),Oe.call(e,t)?e[t].push(n):e[t]=[n]}),ii),Nu=Qr(Mr);function Pu(e){return Ho(e)?Yn(e):Ar(e)}function zu(e){return Ho(e)?Yn(e,!0):function(e){if(!Jo(e))return function(e){var t=[];if(null!=e)for(var n in Se(e))t.push(n);return t}(e);var t=_i(e),n=[];for(var r in e)(\"constructor\"!=r||!t&&Oe.call(e,r))&&n.push(r);return n}(e)}var Lu=Na((function(e,t,n){Ur(e,t,n)})),Ou=Na((function(e,t,n,r){Ur(e,t,n,r)})),Au=Ja((function(e,t){var n={};if(null==e)return n;var r=!1;t=zt(t,(function(t){return t=ga(t,e),r||(r=t.length>1),t})),Ta(e,ti(e),n),r&&(n=or(n,7,Za));for(var a=t.length;a--;)la(n,t[a]);return n})),Fu=Ja((function(e,t){return null==e?{}:function(e,t){return Br(e,t,(function(t,n){return Cu(e,n)}))}(e,t)}));function Du(e,t){if(null==e)return{};var n=zt(ti(e),(function(e){return[e]}));return t=ii(t),Br(e,n,(function(e,n){return t(e,n[0])}))}var Ru=Qa(Pu),ju=Qa(zu);function Uu(e){return null==e?[]:Kt(e,Pu(e))}var Iu=Oa((function(e,t,n){return t=t.toLowerCase(),e+(n?$u(t):t)}));function $u(e){return Gu(yu(e).toLowerCase())}function Bu(e){return(e=yu(e))&&e.replace(me,en).replace(Ze,\"\")}var Wu=Oa((function(e,t,n){return e+(n?\"-\":\"\")+t.toLowerCase()})),Vu=Oa((function(e,t,n){return e+(n?\" \":\"\")+t.toLowerCase()})),Hu=La(\"toLowerCase\"),qu=Oa((function(e,t,n){return e+(n?\"_\":\"\")+t.toLowerCase()})),Qu=Oa((function(e,t,n){return e+(n?\" \":\"\")+Gu(t)})),Yu=Oa((function(e,t,n){return e+(n?\" \":\"\")+t.toUpperCase()})),Gu=La(\"toUpperCase\");function Ku(e,t,n){return e=yu(e),(t=n?a:t)===a?function(e){return tt.test(e)}(e)?function(e){return e.match(Je)||[]}(e):function(e){return e.match(le)||[]}(e):e.match(t)||[]}var Zu=Qr((function(e,t){try{return kt(e,a,t)}catch(e){return Go(e)?e:new we(e)}})),Xu=Ja((function(e,t){return Et(t,(function(t){t=Fi(t),rr(e,t,Mo(e[t],e))})),e}));function Ju(e){return function(){return e}}var el=Da(),tl=Da(!0);function nl(e){return e}function rl(e){return Or(\"function\"==typeof e?e:or(e,1))}var al=Qr((function(e,t){return function(n){return Mr(n,e,t)}})),il=Qr((function(e,t){return function(n){return Mr(e,n,t)}}));function ol(e,t,n){var r=Pu(t),a=_r(t,r);null!=n||Jo(t)&&(a.length||!r.length)||(n=t,t=e,e=this,a=_r(t,Pu(t)));var i=!(Jo(n)&&\"chain\"in n&&!n.chain),o=Ko(e);return Et(a,(function(n){var r=t[n];e[n]=r,o&&(e.prototype[n]=function(){var t=this.__chain__;if(i||t){var n=e(this.__wrapped__);return(n.__actions__=Ca(this.__actions__)).push({func:r,args:arguments,thisArg:e}),n.__chain__=t,n}return r.apply(e,Lt([this.value()],arguments))})})),e}function ul(){}var ll=Ia(zt),sl=Ia(Tt),cl=Ia(Ft);function fl(e){return yi(e)?Wt(Fi(e)):function(e){return function(t){return wr(t,e)}}(e)}var pl=Ba(),dl=Ba(!0);function hl(){return[]}function vl(){return!1}var gl,yl=Ua((function(e,t){return e+t}),0),ml=Ha(\"ceil\"),bl=Ua((function(e,t){return e/t}),1),_l=Ha(\"floor\"),wl=Ua((function(e,t){return e*t}),1),xl=Ha(\"round\"),kl=Ua((function(e,t){return e-t}),0);return jn.after=function(e,t){if(\"function\"!=typeof t)throw new Te(i);return e=du(e),function(){if(--e<1)return t.apply(this,arguments)}},jn.ary=Co,jn.assign=mu,jn.assignIn=bu,jn.assignInWith=_u,jn.assignWith=wu,jn.at=xu,jn.before=To,jn.bind=Mo,jn.bindAll=Xu,jn.bindKey=No,jn.castArray=function(){if(!arguments.length)return[];var e=arguments[0];return Wo(e)?e:[e]},jn.chain=co,jn.chunk=function(e,t,n){t=(n?gi(e,t,n):t===a)?1:gn(du(t),0);var i=null==e?0:e.length;if(!i||t<1)return[];for(var o=0,u=0,l=r(pt(i/t));o<i;)l[u++]=ea(e,o,o+=t);return l},jn.compact=function(e){for(var t=-1,n=null==e?0:e.length,r=0,a=[];++t<n;){var i=e[t];i&&(a[r++]=i)}return a},jn.concat=function(){var e=arguments.length;if(!e)return[];for(var t=r(e-1),n=arguments[0],a=e;a--;)t[a-1]=arguments[a];return Lt(Wo(n)?Ca(n):[n],vr(t,1))},jn.cond=function(e){var t=null==e?0:e.length,n=ii();return e=t?zt(e,(function(e){if(\"function\"!=typeof e[1])throw new Te(i);return[n(e[0]),e[1]]})):[],Qr((function(n){for(var r=-1;++r<t;){var a=e[r];if(kt(a[0],this,n))return kt(a[1],this,n)}}))},jn.conforms=function(e){return function(e){var t=Pu(e);return function(n){return ur(n,e,t)}}(or(e,1))},jn.constant=Ju,jn.countBy=ho,jn.create=function(e,t){var n=Un(e);return null==t?n:nr(n,t)},jn.curry=function e(t,n,r){var i=Ya(t,8,a,a,a,a,a,n=r?a:n);return i.placeholder=e.placeholder,i},jn.curryRight=function e(t,n,r){var i=Ya(t,16,a,a,a,a,a,n=r?a:n);return i.placeholder=e.placeholder,i},jn.debounce=Po,jn.defaults=ku,jn.defaultsDeep=Su,jn.defer=zo,jn.delay=Lo,jn.difference=ji,jn.differenceBy=Ui,jn.differenceWith=Ii,jn.drop=function(e,t,n){var r=null==e?0:e.length;return r?ea(e,(t=n||t===a?1:du(t))<0?0:t,r):[]},jn.dropRight=function(e,t,n){var r=null==e?0:e.length;return r?ea(e,0,(t=r-(t=n||t===a?1:du(t)))<0?0:t):[]},jn.dropRightWhile=function(e,t){return e&&e.length?ca(e,ii(t,3),!0,!0):[]},jn.dropWhile=function(e,t){return e&&e.length?ca(e,ii(t,3),!0):[]},jn.fill=function(e,t,n,r){var i=null==e?0:e.length;return i?(n&&\"number\"!=typeof n&&gi(e,t,n)&&(n=0,r=i),function(e,t,n,r){var i=e.length;for((n=du(n))<0&&(n=-n>i?0:i+n),(r=r===a||r>i?i:du(r))<0&&(r+=i),r=n>r?0:hu(r);n<r;)e[n++]=t;return e}(e,t,n,r)):[]},jn.filter=function(e,t){return(Wo(e)?Mt:hr)(e,ii(t,3))},jn.flatMap=function(e,t){return vr(xo(e,t),1)},jn.flatMapDeep=function(e,t){return vr(xo(e,t),c)},jn.flatMapDepth=function(e,t,n){return n=n===a?1:du(n),vr(xo(e,t),n)},jn.flatten=Wi,jn.flattenDeep=function(e){return null!=e&&e.length?vr(e,c):[]},jn.flattenDepth=function(e,t){return null!=e&&e.length?vr(e,t=t===a?1:du(t)):[]},jn.flip=function(e){return Ya(e,512)},jn.flow=el,jn.flowRight=tl,jn.fromPairs=function(e){for(var t=-1,n=null==e?0:e.length,r={};++t<n;){var a=e[t];r[a[0]]=a[1]}return r},jn.functions=function(e){return null==e?[]:_r(e,Pu(e))},jn.functionsIn=function(e){return null==e?[]:_r(e,zu(e))},jn.groupBy=bo,jn.initial=function(e){return null!=e&&e.length?ea(e,0,-1):[]},jn.intersection=Hi,jn.intersectionBy=qi,jn.intersectionWith=Qi,jn.invert=Tu,jn.invertBy=Mu,jn.invokeMap=_o,jn.iteratee=rl,jn.keyBy=wo,jn.keys=Pu,jn.keysIn=zu,jn.map=xo,jn.mapKeys=function(e,t){var n={};return t=ii(t,3),mr(e,(function(e,r,a){rr(n,t(e,r,a),e)})),n},jn.mapValues=function(e,t){var n={};return t=ii(t,3),mr(e,(function(e,r,a){rr(n,r,t(e,r,a))})),n},jn.matches=function(e){return Rr(or(e,1))},jn.matchesProperty=function(e,t){return jr(e,or(t,1))},jn.memoize=Oo,jn.merge=Lu,jn.mergeWith=Ou,jn.method=al,jn.methodOf=il,jn.mixin=ol,jn.negate=Ao,jn.nthArg=function(e){return e=du(e),Qr((function(t){return Ir(t,e)}))},jn.omit=Au,jn.omitBy=function(e,t){return Du(e,Ao(ii(t)))},jn.once=function(e){return To(2,e)},jn.orderBy=function(e,t,n,r){return null==e?[]:(Wo(t)||(t=null==t?[]:[t]),Wo(n=r?a:n)||(n=null==n?[]:[n]),$r(e,t,n))},jn.over=ll,jn.overArgs=Fo,jn.overEvery=sl,jn.overSome=cl,jn.partial=Do,jn.partialRight=Ro,jn.partition=ko,jn.pick=Fu,jn.pickBy=Du,jn.property=fl,jn.propertyOf=function(e){return function(t){return null==e?a:wr(e,t)}},jn.pull=Gi,jn.pullAll=Ki,jn.pullAllBy=function(e,t,n){return e&&e.length&&t&&t.length?Wr(e,t,ii(n,2)):e},jn.pullAllWith=function(e,t,n){return e&&e.length&&t&&t.length?Wr(e,t,a,n):e},jn.pullAt=Zi,jn.range=pl,jn.rangeRight=dl,jn.rearg=jo,jn.reject=function(e,t){return(Wo(e)?Mt:hr)(e,Ao(ii(t,3)))},jn.remove=function(e,t){var n=[];if(!e||!e.length)return n;var r=-1,a=[],i=e.length;for(t=ii(t,3);++r<i;){var o=e[r];t(o,r,e)&&(n.push(o),a.push(r))}return Vr(e,a),n},jn.rest=function(e,t){if(\"function\"!=typeof e)throw new Te(i);return Qr(e,t=t===a?t:du(t))},jn.reverse=Xi,jn.sampleSize=function(e,t,n){return t=(n?gi(e,t,n):t===a)?1:du(t),(Wo(e)?Kn:Gr)(e,t)},jn.set=function(e,t,n){return null==e?e:Kr(e,t,n)},jn.setWith=function(e,t,n,r){return r=\"function\"==typeof r?r:a,null==e?e:Kr(e,t,n,r)},jn.shuffle=function(e){return(Wo(e)?Zn:Jr)(e)},jn.slice=function(e,t,n){var r=null==e?0:e.length;return r?(n&&\"number\"!=typeof n&&gi(e,t,n)?(t=0,n=r):(t=null==t?0:du(t),n=n===a?r:du(n)),ea(e,t,n)):[]},jn.sortBy=So,jn.sortedUniq=function(e){return e&&e.length?aa(e):[]},jn.sortedUniqBy=function(e,t){return e&&e.length?aa(e,ii(t,2)):[]},jn.split=function(e,t,n){return n&&\"number\"!=typeof n&&gi(e,t,n)&&(t=n=a),(n=n===a?d:n>>>0)?(e=yu(e))&&(\"string\"==typeof t||null!=t&&!au(t))&&!(t=oa(t))&&rn(e)?ma(fn(e),0,n):e.split(t,n):[]},jn.spread=function(e,t){if(\"function\"!=typeof e)throw new Te(i);return t=null==t?0:gn(du(t),0),Qr((function(n){var r=n[t],a=ma(n,0,t);return r&&Lt(a,r),kt(e,this,a)}))},jn.tail=function(e){var t=null==e?0:e.length;return t?ea(e,1,t):[]},jn.take=function(e,t,n){return e&&e.length?ea(e,0,(t=n||t===a?1:du(t))<0?0:t):[]},jn.takeRight=function(e,t,n){var r=null==e?0:e.length;return r?ea(e,(t=r-(t=n||t===a?1:du(t)))<0?0:t,r):[]},jn.takeRightWhile=function(e,t){return e&&e.length?ca(e,ii(t,3),!1,!0):[]},jn.takeWhile=function(e,t){return e&&e.length?ca(e,ii(t,3)):[]},jn.tap=function(e,t){return t(e),e},jn.throttle=function(e,t,n){var r=!0,a=!0;if(\"function\"!=typeof e)throw new Te(i);return Jo(n)&&(r=\"leading\"in n?!!n.leading:r,a=\"trailing\"in n?!!n.trailing:a),Po(e,t,{leading:r,maxWait:t,trailing:a})},jn.thru=fo,jn.toArray=fu,jn.toPairs=Ru,jn.toPairsIn=ju,jn.toPath=function(e){return Wo(e)?zt(e,Fi):uu(e)?[e]:Ca(Ai(yu(e)))},jn.toPlainObject=gu,jn.transform=function(e,t,n){var r=Wo(e),a=r||Qo(e)||lu(e);if(t=ii(t,4),null==n){var i=e&&e.constructor;n=a?r?new i:[]:Jo(e)&&Ko(i)?Un(Ve(e)):{}}return(a?Et:mr)(e,(function(e,r,a){return t(n,e,r,a)})),n},jn.unary=function(e){return Co(e,1)},jn.union=Ji,jn.unionBy=eo,jn.unionWith=to,jn.uniq=function(e){return e&&e.length?ua(e):[]},jn.uniqBy=function(e,t){return e&&e.length?ua(e,ii(t,2)):[]},jn.uniqWith=function(e,t){return t=\"function\"==typeof t?t:a,e&&e.length?ua(e,a,t):[]},jn.unset=function(e,t){return null==e||la(e,t)},jn.unzip=no,jn.unzipWith=ro,jn.update=function(e,t,n){return null==e?e:sa(e,t,va(n))},jn.updateWith=function(e,t,n,r){return r=\"function\"==typeof r?r:a,null==e?e:sa(e,t,va(n),r)},jn.values=Uu,jn.valuesIn=function(e){return null==e?[]:Kt(e,zu(e))},jn.without=ao,jn.words=Ku,jn.wrap=function(e,t){return Do(va(t),e)},jn.xor=io,jn.xorBy=oo,jn.xorWith=uo,jn.zip=lo,jn.zipObject=function(e,t){return da(e||[],t||[],Jn)},jn.zipObjectDeep=function(e,t){return da(e||[],t||[],Kr)},jn.zipWith=so,jn.entries=Ru,jn.entriesIn=ju,jn.extend=bu,jn.extendWith=_u,ol(jn,jn),jn.add=yl,jn.attempt=Zu,jn.camelCase=Iu,jn.capitalize=$u,jn.ceil=ml,jn.clamp=function(e,t,n){return n===a&&(n=t,t=a),n!==a&&(n=(n=vu(n))==n?n:0),t!==a&&(t=(t=vu(t))==t?t:0),ir(vu(e),t,n)},jn.clone=function(e){return or(e,4)},jn.cloneDeep=function(e){return or(e,5)},jn.cloneDeepWith=function(e,t){return or(e,5,t=\"function\"==typeof t?t:a)},jn.cloneWith=function(e,t){return or(e,4,t=\"function\"==typeof t?t:a)},jn.conformsTo=function(e,t){return null==t||ur(e,t,Pu(t))},jn.deburr=Bu,jn.defaultTo=function(e,t){return null==e||e!=e?t:e},jn.divide=bl,jn.endsWith=function(e,t,n){e=yu(e),t=oa(t);var r=e.length,i=n=n===a?r:ir(du(n),0,r);return(n-=t.length)>=0&&e.slice(n,i)==t},jn.eq=Uo,jn.escape=function(e){return(e=yu(e))&&Y.test(e)?e.replace(q,tn):e},jn.escapeRegExp=function(e){return(e=yu(e))&&ne.test(e)?e.replace(te,\"\\\\$&\"):e},jn.every=function(e,t,n){var r=Wo(e)?Tt:pr;return n&&gi(e,t,n)&&(t=a),r(e,ii(t,3))},jn.find=vo,jn.findIndex=$i,jn.findKey=function(e,t){return Rt(e,ii(t,3),mr)},jn.findLast=go,jn.findLastIndex=Bi,jn.findLastKey=function(e,t){return Rt(e,ii(t,3),br)},jn.floor=_l,jn.forEach=yo,jn.forEachRight=mo,jn.forIn=function(e,t){return null==e?e:gr(e,ii(t,3),zu)},jn.forInRight=function(e,t){return null==e?e:yr(e,ii(t,3),zu)},jn.forOwn=function(e,t){return e&&mr(e,ii(t,3))},jn.forOwnRight=function(e,t){return e&&br(e,ii(t,3))},jn.get=Eu,jn.gt=Io,jn.gte=$o,jn.has=function(e,t){return null!=e&&pi(e,t,Er)},jn.hasIn=Cu,jn.head=Vi,jn.identity=nl,jn.includes=function(e,t,n,r){e=Ho(e)?e:Uu(e),n=n&&!r?du(n):0;var a=e.length;return n<0&&(n=gn(a+n,0)),ou(e)?n<=a&&e.indexOf(t,n)>-1:!!a&&Ut(e,t,n)>-1},jn.indexOf=function(e,t,n){var r=null==e?0:e.length;if(!r)return-1;var a=null==n?0:du(n);return a<0&&(a=gn(r+a,0)),Ut(e,t,a)},jn.inRange=function(e,t,n){return t=pu(t),n===a?(n=t,t=0):n=pu(n),function(e,t,n){return e>=yn(t,n)&&e<gn(t,n)}(e=vu(e),t,n)},jn.invoke=Nu,jn.isArguments=Bo,jn.isArray=Wo,jn.isArrayBuffer=Vo,jn.isArrayLike=Ho,jn.isArrayLikeObject=qo,jn.isBoolean=function(e){return!0===e||!1===e||eu(e)&&kr(e)==y},jn.isBuffer=Qo,jn.isDate=Yo,jn.isElement=function(e){return eu(e)&&1===e.nodeType&&!ru(e)},jn.isEmpty=function(e){if(null==e)return!0;if(Ho(e)&&(Wo(e)||\"string\"==typeof e||\"function\"==typeof e.splice||Qo(e)||lu(e)||Bo(e)))return!e.length;var t=fi(e);if(t==x||t==T)return!e.size;if(_i(e))return!Ar(e).length;for(var n in e)if(Oe.call(e,n))return!1;return!0},jn.isEqual=function(e,t){return Pr(e,t)},jn.isEqualWith=function(e,t,n){var r=(n=\"function\"==typeof n?n:a)?n(e,t):a;return r===a?Pr(e,t,a,n):!!r},jn.isError=Go,jn.isFinite=function(e){return\"number\"==typeof e&&Dt(e)},jn.isFunction=Ko,jn.isInteger=Zo,jn.isLength=Xo,jn.isMap=tu,jn.isMatch=function(e,t){return e===t||zr(e,t,ui(t))},jn.isMatchWith=function(e,t,n){return n=\"function\"==typeof n?n:a,zr(e,t,ui(t),n)},jn.isNaN=function(e){return nu(e)&&e!=+e},jn.isNative=function(e){if(bi(e))throw new we(\"Unsupported core-js use. Try https://npms.io/search?q=ponyfill.\");return Lr(e)},jn.isNil=function(e){return null==e},jn.isNull=function(e){return null===e},jn.isNumber=nu,jn.isObject=Jo,jn.isObjectLike=eu,jn.isPlainObject=ru,jn.isRegExp=au,jn.isSafeInteger=function(e){return Zo(e)&&e>=-9007199254740991&&e<=f},jn.isSet=iu,jn.isString=ou,jn.isSymbol=uu,jn.isTypedArray=lu,jn.isUndefined=function(e){return e===a},jn.isWeakMap=function(e){return eu(e)&&fi(e)==P},jn.isWeakSet=function(e){return eu(e)&&\"[object WeakSet]\"==kr(e)},jn.join=function(e,t){return null==e?\"\":Vt.call(e,t)},jn.kebabCase=Wu,jn.last=Yi,jn.lastIndexOf=function(e,t,n){var r=null==e?0:e.length;if(!r)return-1;var i=r;return n!==a&&(i=(i=du(n))<0?gn(r+i,0):yn(i,r-1)),t==t?function(e,t,n){for(var r=n+1;r--;)if(e[r]===t)return r;return r}(e,t,i):jt(e,$t,i,!0)},jn.lowerCase=Vu,jn.lowerFirst=Hu,jn.lt=su,jn.lte=cu,jn.max=function(e){return e&&e.length?dr(e,nl,Sr):a},jn.maxBy=function(e,t){return e&&e.length?dr(e,ii(t,2),Sr):a},jn.mean=function(e){return Bt(e,nl)},jn.meanBy=function(e,t){return Bt(e,ii(t,2))},jn.min=function(e){return e&&e.length?dr(e,nl,Fr):a},jn.minBy=function(e,t){return e&&e.length?dr(e,ii(t,2),Fr):a},jn.stubArray=hl,jn.stubFalse=vl,jn.stubObject=function(){return{}},jn.stubString=function(){return\"\"},jn.stubTrue=function(){return!0},jn.multiply=wl,jn.nth=function(e,t){return e&&e.length?Ir(e,du(t)):a},jn.noConflict=function(){return ft._===this&&(ft._=je),this},jn.noop=ul,jn.now=Eo,jn.pad=function(e,t,n){e=yu(e);var r=(t=du(t))?cn(e):0;if(!t||r>=t)return e;var a=(t-r)/2;return $a(dt(a),n)+e+$a(pt(a),n)},jn.padEnd=function(e,t,n){e=yu(e);var r=(t=du(t))?cn(e):0;return t&&r<t?e+$a(t-r,n):e},jn.padStart=function(e,t,n){e=yu(e);var r=(t=du(t))?cn(e):0;return t&&r<t?$a(t-r,n)+e:e},jn.parseInt=function(e,t,n){return n||null==t?t=0:t&&(t=+t),bn(yu(e).replace(re,\"\"),t||0)},jn.random=function(e,t,n){if(n&&\"boolean\"!=typeof n&&gi(e,t,n)&&(t=n=a),n===a&&(\"boolean\"==typeof t?(n=t,t=a):\"boolean\"==typeof e&&(n=e,e=a)),e===a&&t===a?(e=0,t=1):(e=pu(e),t===a?(t=e,e=0):t=pu(t)),e>t){var r=e;e=t,t=r}if(n||e%1||t%1){var i=_n();return yn(e+i*(t-e+ut(\"1e-\"+((i+\"\").length-1))),t)}return Hr(e,t)},jn.reduce=function(e,t,n){var r=Wo(e)?Ot:Ht,a=arguments.length<3;return r(e,ii(t,4),n,a,cr)},jn.reduceRight=function(e,t,n){var r=Wo(e)?At:Ht,a=arguments.length<3;return r(e,ii(t,4),n,a,fr)},jn.repeat=function(e,t,n){return t=(n?gi(e,t,n):t===a)?1:du(t),qr(yu(e),t)},jn.replace=function(){var e=arguments,t=yu(e[0]);return e.length<3?t:t.replace(e[1],e[2])},jn.result=function(e,t,n){var r=-1,i=(t=ga(t,e)).length;for(i||(i=1,e=a);++r<i;){var o=null==e?a:e[Fi(t[r])];o===a&&(r=i,o=n),e=Ko(o)?o.call(e):o}return e},jn.round=xl,jn.runInContext=e,jn.sample=function(e){return(Wo(e)?Gn:Yr)(e)},jn.size=function(e){if(null==e)return 0;if(Ho(e))return ou(e)?cn(e):e.length;var t=fi(e);return t==x||t==T?e.size:Ar(e).length},jn.snakeCase=qu,jn.some=function(e,t,n){var r=Wo(e)?Ft:ta;return n&&gi(e,t,n)&&(t=a),r(e,ii(t,3))},jn.sortedIndex=function(e,t){return na(e,t)},jn.sortedIndexBy=function(e,t,n){return ra(e,t,ii(n,2))},jn.sortedIndexOf=function(e,t){var n=null==e?0:e.length;if(n){var r=na(e,t);if(r<n&&Uo(e[r],t))return r}return-1},jn.sortedLastIndex=function(e,t){return na(e,t,!0)},jn.sortedLastIndexBy=function(e,t,n){return ra(e,t,ii(n,2),!0)},jn.sortedLastIndexOf=function(e,t){if(null!=e&&e.length){var n=na(e,t,!0)-1;if(Uo(e[n],t))return n}return-1},jn.startCase=Qu,jn.startsWith=function(e,t,n){return e=yu(e),n=null==n?0:ir(du(n),0,e.length),t=oa(t),e.slice(n,n+t.length)==t},jn.subtract=kl,jn.sum=function(e){return e&&e.length?qt(e,nl):0},jn.sumBy=function(e,t){return e&&e.length?qt(e,ii(t,2)):0},jn.template=function(e,t,n){var r=jn.templateSettings;n&&gi(e,t,n)&&(t=a),e=yu(e),t=_u({},t,r,Ga);var i,o,u=_u({},t.imports,r.imports,Ga),l=Pu(u),s=Kt(u,l),c=0,f=t.interpolate||be,p=\"__p += '\",d=Ee((t.escape||be).source+\"|\"+f.source+\"|\"+(f===Z?fe:be).source+\"|\"+(t.evaluate||be).source+\"|$\",\"g\"),h=\"//# sourceURL=\"+(Oe.call(t,\"sourceURL\")?(t.sourceURL+\"\").replace(/\\s/g,\" \"):\"lodash.templateSources[\"+ ++rt+\"]\")+\"\\n\";e.replace(d,(function(t,n,r,a,u,l){return r||(r=a),p+=e.slice(c,l).replace(_e,nn),n&&(i=!0,p+=\"' +\\n__e(\"+n+\") +\\n'\"),u&&(o=!0,p+=\"';\\n\"+u+\";\\n__p += '\"),r&&(p+=\"' +\\n((__t = (\"+r+\")) == null ? '' : __t) +\\n'\"),c=l+t.length,t})),p+=\"';\\n\";var v=Oe.call(t,\"variable\")&&t.variable;if(v){if(se.test(v))throw new we(\"Invalid `variable` option passed into `_.template`\")}else p=\"with (obj) {\\n\"+p+\"\\n}\\n\";p=(o?p.replace(B,\"\"):p).replace(W,\"$1\").replace(V,\"$1;\"),p=\"function(\"+(v||\"obj\")+\") {\\n\"+(v?\"\":\"obj || (obj = {});\\n\")+\"var __t, __p = ''\"+(i?\", __e = _.escape\":\"\")+(o?\", __j = Array.prototype.join;\\nfunction print() { __p += __j.call(arguments, '') }\\n\":\";\\n\")+p+\"return __p\\n}\";var g=Zu((function(){return xe(l,h+\"return \"+p).apply(a,s)}));if(g.source=p,Go(g))throw g;return g},jn.times=function(e,t){if((e=du(e))<1||e>f)return[];var n=d,r=yn(e,d);t=ii(t),e-=d;for(var a=Qt(r,t);++n<e;)t(n);return a},jn.toFinite=pu,jn.toInteger=du,jn.toLength=hu,jn.toLower=function(e){return yu(e).toLowerCase()},jn.toNumber=vu,jn.toSafeInteger=function(e){return e?ir(du(e),-9007199254740991,f):0===e?e:0},jn.toString=yu,jn.toUpper=function(e){return yu(e).toUpperCase()},jn.trim=function(e,t,n){if((e=yu(e))&&(n||t===a))return Yt(e);if(!e||!(t=oa(t)))return e;var r=fn(e),i=fn(t);return ma(r,Xt(r,i),Jt(r,i)+1).join(\"\")},jn.trimEnd=function(e,t,n){if((e=yu(e))&&(n||t===a))return e.slice(0,pn(e)+1);if(!e||!(t=oa(t)))return e;var r=fn(e);return ma(r,0,Jt(r,fn(t))+1).join(\"\")},jn.trimStart=function(e,t,n){if((e=yu(e))&&(n||t===a))return e.replace(re,\"\");if(!e||!(t=oa(t)))return e;var r=fn(e);return ma(r,Xt(r,fn(t))).join(\"\")},jn.truncate=function(e,t){var n=30,r=\"...\";if(Jo(t)){var i=\"separator\"in t?t.separator:i;n=\"length\"in t?du(t.length):n,r=\"omission\"in t?oa(t.omission):r}var o=(e=yu(e)).length;if(rn(e)){var u=fn(e);o=u.length}if(n>=o)return e;var l=n-cn(r);if(l<1)return r;var s=u?ma(u,0,l).join(\"\"):e.slice(0,l);if(i===a)return s+r;if(u&&(l+=s.length-l),au(i)){if(e.slice(l).search(i)){var c,f=s;for(i.global||(i=Ee(i.source,yu(pe.exec(i))+\"g\")),i.lastIndex=0;c=i.exec(f);)var p=c.index;s=s.slice(0,p===a?l:p)}}else if(e.indexOf(oa(i),l)!=l){var d=s.lastIndexOf(i);d>-1&&(s=s.slice(0,d))}return s+r},jn.unescape=function(e){return(e=yu(e))&&Q.test(e)?e.replace(H,dn):e},jn.uniqueId=function(e){var t=++Ae;return yu(e)+t},jn.upperCase=Yu,jn.upperFirst=Gu,jn.each=yo,jn.eachRight=mo,jn.first=Vi,ol(jn,(gl={},mr(jn,(function(e,t){Oe.call(jn.prototype,t)||(gl[t]=e)})),gl),{chain:!1}),jn.VERSION=\"4.17.21\",Et([\"bind\",\"bindKey\",\"curry\",\"curryRight\",\"partial\",\"partialRight\"],(function(e){jn[e].placeholder=jn})),Et([\"drop\",\"take\"],(function(e,t){Bn.prototype[e]=function(n){n=n===a?1:gn(du(n),0);var r=this.__filtered__&&!t?new Bn(this):this.clone();return r.__filtered__?r.__takeCount__=yn(n,r.__takeCount__):r.__views__.push({size:yn(n,d),type:e+(r.__dir__<0?\"Right\":\"\")}),r},Bn.prototype[e+\"Right\"]=function(t){return this.reverse()[e](t).reverse()}})),Et([\"filter\",\"map\",\"takeWhile\"],(function(e,t){var n=t+1,r=1==n||3==n;Bn.prototype[e]=function(e){var t=this.clone();return t.__iteratees__.push({iteratee:ii(e,3),type:n}),t.__filtered__=t.__filtered__||r,t}})),Et([\"head\",\"last\"],(function(e,t){var n=\"take\"+(t?\"Right\":\"\");Bn.prototype[e]=function(){return this[n](1).value()[0]}})),Et([\"initial\",\"tail\"],(function(e,t){var n=\"drop\"+(t?\"\":\"Right\");Bn.prototype[e]=function(){return this.__filtered__?new Bn(this):this[n](1)}})),Bn.prototype.compact=function(){return this.filter(nl)},Bn.prototype.find=function(e){return this.filter(e).head()},Bn.prototype.findLast=function(e){return this.reverse().find(e)},Bn.prototype.invokeMap=Qr((function(e,t){return\"function\"==typeof e?new Bn(this):this.map((function(n){return Mr(n,e,t)}))})),Bn.prototype.reject=function(e){return this.filter(Ao(ii(e)))},Bn.prototype.slice=function(e,t){e=du(e);var n=this;return n.__filtered__&&(e>0||t<0)?new Bn(n):(e<0?n=n.takeRight(-e):e&&(n=n.drop(e)),t!==a&&(n=(t=du(t))<0?n.dropRight(-t):n.take(t-e)),n)},Bn.prototype.takeRightWhile=function(e){return this.reverse().takeWhile(e).reverse()},Bn.prototype.toArray=function(){return this.take(d)},mr(Bn.prototype,(function(e,t){var n=/^(?:filter|find|map|reject)|While$/.test(t),r=/^(?:head|last)$/.test(t),i=jn[r?\"take\"+(\"last\"==t?\"Right\":\"\"):t],o=r||/^find/.test(t);i&&(jn.prototype[t]=function(){var t=this.__wrapped__,u=r?[1]:arguments,l=t instanceof Bn,s=u[0],c=l||Wo(t),f=function(e){var t=i.apply(jn,Lt([e],u));return r&&p?t[0]:t};c&&n&&\"function\"==typeof s&&1!=s.length&&(l=c=!1);var p=this.__chain__,d=!!this.__actions__.length,h=o&&!p,v=l&&!d;if(!o&&c){t=v?t:new Bn(this);var g=e.apply(t,u);return g.__actions__.push({func:fo,args:[f],thisArg:a}),new $n(g,p)}return h&&v?e.apply(this,u):(g=this.thru(f),h?r?g.value()[0]:g.value():g)})})),Et([\"pop\",\"push\",\"shift\",\"sort\",\"splice\",\"unshift\"],(function(e){var t=Me[e],n=/^(?:push|sort|unshift)$/.test(e)?\"tap\":\"thru\",r=/^(?:pop|shift)$/.test(e);jn.prototype[e]=function(){var e=arguments;if(r&&!this.__chain__){var a=this.value();return t.apply(Wo(a)?a:[],e)}return this[n]((function(n){return t.apply(Wo(n)?n:[],e)}))}})),mr(Bn.prototype,(function(e,t){var n=jn[t];if(n){var r=n.name+\"\";Oe.call(Nn,r)||(Nn[r]=[]),Nn[r].push({name:t,func:n})}})),Nn[Ra(a,2).name]=[{name:\"wrapper\",func:a}],Bn.prototype.clone=function(){var e=new Bn(this.__wrapped__);return e.__actions__=Ca(this.__actions__),e.__dir__=this.__dir__,e.__filtered__=this.__filtered__,e.__iteratees__=Ca(this.__iteratees__),e.__takeCount__=this.__takeCount__,e.__views__=Ca(this.__views__),e},Bn.prototype.reverse=function(){if(this.__filtered__){var e=new Bn(this);e.__dir__=-1,e.__filtered__=!0}else(e=this.clone()).__dir__*=-1;return e},Bn.prototype.value=function(){var e=this.__wrapped__.value(),t=this.__dir__,n=Wo(e),r=t<0,a=n?e.length:0,i=function(e,t,n){for(var r=-1,a=n.length;++r<a;){var i=n[r],o=i.size;switch(i.type){case\"drop\":e+=o;break;case\"dropRight\":t-=o;break;case\"take\":t=yn(t,e+o);break;case\"takeRight\":e=gn(e,t-o)}}return{start:e,end:t}}(0,a,this.__views__),o=i.start,u=i.end,l=u-o,s=r?u:o-1,c=this.__iteratees__,f=c.length,p=0,d=yn(l,this.__takeCount__);if(!n||!r&&a==l&&d==l)return fa(e,this.__actions__);var h=[];e:for(;l--&&p<d;){for(var v=-1,g=e[s+=t];++v<f;){var y=c[v],m=y.iteratee,b=y.type,_=m(g);if(2==b)g=_;else if(!_){if(1==b)continue e;break e}}h[p++]=g}return h},jn.prototype.at=po,jn.prototype.chain=function(){return co(this)},jn.prototype.commit=function(){return new $n(this.value(),this.__chain__)},jn.prototype.next=function(){this.__values__===a&&(this.__values__=fu(this.value()));var e=this.__index__>=this.__values__.length;return{done:e,value:e?a:this.__values__[this.__index__++]}},jn.prototype.plant=function(e){for(var t,n=this;n instanceof In;){var r=Ri(n);r.__index__=0,r.__values__=a,t?i.__wrapped__=r:t=r;var i=r;n=n.__wrapped__}return i.__wrapped__=e,t},jn.prototype.reverse=function(){var e=this.__wrapped__;if(e instanceof Bn){var t=e;return this.__actions__.length&&(t=new Bn(this)),(t=t.reverse()).__actions__.push({func:fo,args:[Xi],thisArg:a}),new $n(t,this.__chain__)}return this.thru(Xi)},jn.prototype.toJSON=jn.prototype.valueOf=jn.prototype.value=function(){return fa(this.__wrapped__,this.__actions__)},jn.prototype.first=jn.prototype.head,Ge&&(jn.prototype[Ge]=function(){return this}),jn}();ft._=hn,(r=function(){return hn}.call(t,n,t,e))===a||(e.exports=r)}.call(this)},448:(e,t,n)=>{\"use strict\";var r=n(294),a=n(840);function i(e){for(var t=\"https://reactjs.org/docs/error-decoder.html?invariant=\"+e,n=1;n<arguments.length;n++)t+=\"&args[]=\"+encodeURIComponent(arguments[n]);return\"Minified React error #\"+e+\"; visit \"+t+\" for the full message or use the non-minified dev environment for full errors and additional helpful warnings.\"}var o=new Set,u={};function l(e,t){s(e,t),s(e+\"Capture\",t)}function s(e,t){for(u[e]=t,e=0;e<t.length;e++)o.add(t[e])}var c=!(\"undefined\"==typeof window||void 0===window.document||void 0===window.document.createElement),f=Object.prototype.hasOwnProperty,p=/^[:A-Z_a-z\\u00C0-\\u00D6\\u00D8-\\u00F6\\u00F8-\\u02FF\\u0370-\\u037D\\u037F-\\u1FFF\\u200C-\\u200D\\u2070-\\u218F\\u2C00-\\u2FEF\\u3001-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFFD][:A-Z_a-z\\u00C0-\\u00D6\\u00D8-\\u00F6\\u00F8-\\u02FF\\u0370-\\u037D\\u037F-\\u1FFF\\u200C-\\u200D\\u2070-\\u218F\\u2C00-\\u2FEF\\u3001-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFFD\\-.0-9\\u00B7\\u0300-\\u036F\\u203F-\\u2040]*$/,d={},h={};function v(e,t,n,r,a,i,o){this.acceptsBooleans=2===t||3===t||4===t,this.attributeName=r,this.attributeNamespace=a,this.mustUseProperty=n,this.propertyName=e,this.type=t,this.sanitizeURL=i,this.removeEmptyString=o}var g={};\"children dangerouslySetInnerHTML defaultValue defaultChecked innerHTML suppressContentEditableWarning suppressHydrationWarning style\".split(\" \").forEach((function(e){g[e]=new v(e,0,!1,e,null,!1,!1)})),[[\"acceptCharset\",\"accept-charset\"],[\"className\",\"class\"],[\"htmlFor\",\"for\"],[\"httpEquiv\",\"http-equiv\"]].forEach((function(e){var t=e[0];g[t]=new v(t,1,!1,e[1],null,!1,!1)})),[\"contentEditable\",\"draggable\",\"spellCheck\",\"value\"].forEach((function(e){g[e]=new v(e,2,!1,e.toLowerCase(),null,!1,!1)})),[\"autoReverse\",\"externalResourcesRequired\",\"focusable\",\"preserveAlpha\"].forEach((function(e){g[e]=new v(e,2,!1,e,null,!1,!1)})),\"allowFullScreen async autoFocus autoPlay controls default defer disabled disablePictureInPicture disableRemotePlayback formNoValidate hidden loop noModule noValidate open playsInline readOnly required reversed scoped seamless itemScope\".split(\" \").forEach((function(e){g[e]=new v(e,3,!1,e.toLowerCase(),null,!1,!1)})),[\"checked\",\"multiple\",\"muted\",\"selected\"].forEach((function(e){g[e]=new v(e,3,!0,e,null,!1,!1)})),[\"capture\",\"download\"].forEach((function(e){g[e]=new v(e,4,!1,e,null,!1,!1)})),[\"cols\",\"rows\",\"size\",\"span\"].forEach((function(e){g[e]=new v(e,6,!1,e,null,!1,!1)})),[\"rowSpan\",\"start\"].forEach((function(e){g[e]=new v(e,5,!1,e.toLowerCase(),null,!1,!1)}));var y=/[\\-:]([a-z])/g;function m(e){return e[1].toUpperCase()}function b(e,t,n,r){var a=g.hasOwnProperty(t)?g[t]:null;(null!==a?0!==a.type:r||!(2<t.length)||\"o\"!==t[0]&&\"O\"!==t[0]||\"n\"!==t[1]&&\"N\"!==t[1])&&(function(e,t,n,r){if(null==t||function(e,t,n,r){if(null!==n&&0===n.type)return!1;switch(typeof t){case\"function\":case\"symbol\":return!0;case\"boolean\":return!r&&(null!==n?!n.acceptsBooleans:\"data-\"!==(e=e.toLowerCase().slice(0,5))&&\"aria-\"!==e);default:return!1}}(e,t,n,r))return!0;if(r)return!1;if(null!==n)switch(n.type){case 3:return!t;case 4:return!1===t;case 5:return isNaN(t);case 6:return isNaN(t)||1>t}return!1}(t,n,a,r)&&(n=null),r||null===a?function(e){return!!f.call(h,e)||!f.call(d,e)&&(p.test(e)?h[e]=!0:(d[e]=!0,!1))}(t)&&(null===n?e.removeAttribute(t):e.setAttribute(t,\"\"+n)):a.mustUseProperty?e[a.propertyName]=null===n?3!==a.type&&\"\":n:(t=a.attributeName,r=a.attributeNamespace,null===n?e.removeAttribute(t):(n=3===(a=a.type)||4===a&&!0===n?\"\":\"\"+n,r?e.setAttributeNS(r,t,n):e.setAttribute(t,n))))}\"accent-height alignment-baseline arabic-form baseline-shift cap-height clip-path clip-rule color-interpolation color-interpolation-filters color-profile color-rendering dominant-baseline enable-background fill-opacity fill-rule flood-color flood-opacity font-family font-size font-size-adjust font-stretch font-style font-variant font-weight glyph-name glyph-orientation-horizontal glyph-orientation-vertical horiz-adv-x horiz-origin-x image-rendering letter-spacing lighting-color marker-end marker-mid marker-start overline-position overline-thickness paint-order panose-1 pointer-events rendering-intent shape-rendering stop-color stop-opacity strikethrough-position strikethrough-thickness stroke-dasharray stroke-dashoffset stroke-linecap stroke-linejoin stroke-miterlimit stroke-opacity stroke-width text-anchor text-decoration text-rendering underline-position underline-thickness unicode-bidi unicode-range units-per-em v-alphabetic v-hanging v-ideographic v-mathematical vector-effect vert-adv-y vert-origin-x vert-origin-y word-spacing writing-mode xmlns:xlink x-height\".split(\" \").forEach((function(e){var t=e.replace(y,m);g[t]=new v(t,1,!1,e,null,!1,!1)})),\"xlink:actuate xlink:arcrole xlink:role xlink:show xlink:title xlink:type\".split(\" \").forEach((function(e){var t=e.replace(y,m);g[t]=new v(t,1,!1,e,\"http://www.w3.org/1999/xlink\",!1,!1)})),[\"xml:base\",\"xml:lang\",\"xml:space\"].forEach((function(e){var t=e.replace(y,m);g[t]=new v(t,1,!1,e,\"http://www.w3.org/XML/1998/namespace\",!1,!1)})),[\"tabIndex\",\"crossOrigin\"].forEach((function(e){g[e]=new v(e,1,!1,e.toLowerCase(),null,!1,!1)})),g.xlinkHref=new v(\"xlinkHref\",1,!1,\"xlink:href\",\"http://www.w3.org/1999/xlink\",!0,!1),[\"src\",\"href\",\"action\",\"formAction\"].forEach((function(e){g[e]=new v(e,1,!1,e.toLowerCase(),null,!0,!0)}));var _=r.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED,w=Symbol.for(\"react.element\"),x=Symbol.for(\"react.portal\"),k=Symbol.for(\"react.fragment\"),S=Symbol.for(\"react.strict_mode\"),E=Symbol.for(\"react.profiler\"),C=Symbol.for(\"react.provider\"),T=Symbol.for(\"react.context\"),M=Symbol.for(\"react.forward_ref\"),N=Symbol.for(\"react.suspense\"),P=Symbol.for(\"react.suspense_list\"),z=Symbol.for(\"react.memo\"),L=Symbol.for(\"react.lazy\");Symbol.for(\"react.scope\"),Symbol.for(\"react.debug_trace_mode\");var O=Symbol.for(\"react.offscreen\");Symbol.for(\"react.legacy_hidden\"),Symbol.for(\"react.cache\"),Symbol.for(\"react.tracing_marker\");var A=Symbol.iterator;function F(e){return null===e||\"object\"!=typeof e?null:\"function\"==typeof(e=A&&e[A]||e[\"@@iterator\"])?e:null}var D,R=Object.assign;function j(e){if(void 0===D)try{throw Error()}catch(e){var t=e.stack.trim().match(/\\n( *(at )?)/);D=t&&t[1]||\"\"}return\"\\n\"+D+e}var U=!1;function I(e,t){if(!e||U)return\"\";U=!0;var n=Error.prepareStackTrace;Error.prepareStackTrace=void 0;try{if(t)if(t=function(){throw Error()},Object.defineProperty(t.prototype,\"props\",{set:function(){throw Error()}}),\"object\"==typeof Reflect&&Reflect.construct){try{Reflect.construct(t,[])}catch(e){var r=e}Reflect.construct(e,[],t)}else{try{t.call()}catch(e){r=e}e.call(t.prototype)}else{try{throw Error()}catch(e){r=e}e()}}catch(t){if(t&&r&&\"string\"==typeof t.stack){for(var a=t.stack.split(\"\\n\"),i=r.stack.split(\"\\n\"),o=a.length-1,u=i.length-1;1<=o&&0<=u&&a[o]!==i[u];)u--;for(;1<=o&&0<=u;o--,u--)if(a[o]!==i[u]){if(1!==o||1!==u)do{if(o--,0>--u||a[o]!==i[u]){var l=\"\\n\"+a[o].replace(\" at new \",\" at \");return e.displayName&&l.includes(\"<anonymous>\")&&(l=l.replace(\"<anonymous>\",e.displayName)),l}}while(1<=o&&0<=u);break}}}finally{U=!1,Error.prepareStackTrace=n}return(e=e?e.displayName||e.name:\"\")?j(e):\"\"}function $(e){switch(e.tag){case 5:return j(e.type);case 16:return j(\"Lazy\");case 13:return j(\"Suspense\");case 19:return j(\"SuspenseList\");case 0:case 2:case 15:return I(e.type,!1);case 11:return I(e.type.render,!1);case 1:return I(e.type,!0);default:return\"\"}}function B(e){if(null==e)return null;if(\"function\"==typeof e)return e.displayName||e.name||null;if(\"string\"==typeof e)return e;switch(e){case k:return\"Fragment\";case x:return\"Portal\";case E:return\"Profiler\";case S:return\"StrictMode\";case N:return\"Suspense\";case P:return\"SuspenseList\"}if(\"object\"==typeof e)switch(e.$$typeof){case T:return(e.displayName||\"Context\")+\".Consumer\";case C:return(e._context.displayName||\"Context\")+\".Provider\";case M:var t=e.render;return(e=e.displayName)||(e=\"\"!==(e=t.displayName||t.name||\"\")?\"ForwardRef(\"+e+\")\":\"ForwardRef\"),e;case z:return null!==(t=e.displayName||null)?t:B(e.type)||\"Memo\";case L:t=e._payload,e=e._init;try{return B(e(t))}catch(e){}}return null}function W(e){var t=e.type;switch(e.tag){case 24:return\"Cache\";case 9:return(t.displayName||\"Context\")+\".Consumer\";case 10:return(t._context.displayName||\"Context\")+\".Provider\";case 18:return\"DehydratedFragment\";case 11:return e=(e=t.render).displayName||e.name||\"\",t.displayName||(\"\"!==e?\"ForwardRef(\"+e+\")\":\"ForwardRef\");case 7:return\"Fragment\";case 5:return t;case 4:return\"Portal\";case 3:return\"Root\";case 6:return\"Text\";case 16:return B(t);case 8:return t===S?\"StrictMode\":\"Mode\";case 22:return\"Offscreen\";case 12:return\"Profiler\";case 21:return\"Scope\";case 13:return\"Suspense\";case 19:return\"SuspenseList\";case 25:return\"TracingMarker\";case 1:case 0:case 17:case 2:case 14:case 15:if(\"function\"==typeof t)return t.displayName||t.name||null;if(\"string\"==typeof t)return t}return null}function V(e){switch(typeof e){case\"boolean\":case\"number\":case\"string\":case\"undefined\":case\"object\":return e;default:return\"\"}}function H(e){var t=e.type;return(e=e.nodeName)&&\"input\"===e.toLowerCase()&&(\"checkbox\"===t||\"radio\"===t)}function q(e){e._valueTracker||(e._valueTracker=function(e){var t=H(e)?\"checked\":\"value\",n=Object.getOwnPropertyDescriptor(e.constructor.prototype,t),r=\"\"+e[t];if(!e.hasOwnProperty(t)&&void 0!==n&&\"function\"==typeof n.get&&\"function\"==typeof n.set){var a=n.get,i=n.set;return Object.defineProperty(e,t,{configurable:!0,get:function(){return a.call(this)},set:function(e){r=\"\"+e,i.call(this,e)}}),Object.defineProperty(e,t,{enumerable:n.enumerable}),{getValue:function(){return r},setValue:function(e){r=\"\"+e},stopTracking:function(){e._valueTracker=null,delete e[t]}}}}(e))}function Q(e){if(!e)return!1;var t=e._valueTracker;if(!t)return!0;var n=t.getValue(),r=\"\";return e&&(r=H(e)?e.checked?\"true\":\"false\":e.value),(e=r)!==n&&(t.setValue(e),!0)}function Y(e){if(void 0===(e=e||(\"undefined\"!=typeof document?document:void 0)))return null;try{return e.activeElement||e.body}catch(t){return e.body}}function G(e,t){var n=t.checked;return R({},t,{defaultChecked:void 0,defaultValue:void 0,value:void 0,checked:null!=n?n:e._wrapperState.initialChecked})}function K(e,t){var n=null==t.defaultValue?\"\":t.defaultValue,r=null!=t.checked?t.checked:t.defaultChecked;n=V(null!=t.value?t.value:n),e._wrapperState={initialChecked:r,initialValue:n,controlled:\"checkbox\"===t.type||\"radio\"===t.type?null!=t.checked:null!=t.value}}function Z(e,t){null!=(t=t.checked)&&b(e,\"checked\",t,!1)}function X(e,t){Z(e,t);var n=V(t.value),r=t.type;if(null!=n)\"number\"===r?(0===n&&\"\"===e.value||e.value!=n)&&(e.value=\"\"+n):e.value!==\"\"+n&&(e.value=\"\"+n);else if(\"submit\"===r||\"reset\"===r)return void e.removeAttribute(\"value\");t.hasOwnProperty(\"value\")?ee(e,t.type,n):t.hasOwnProperty(\"defaultValue\")&&ee(e,t.type,V(t.defaultValue)),null==t.checked&&null!=t.defaultChecked&&(e.defaultChecked=!!t.defaultChecked)}function J(e,t,n){if(t.hasOwnProperty(\"value\")||t.hasOwnProperty(\"defaultValue\")){var r=t.type;if(!(\"submit\"!==r&&\"reset\"!==r||void 0!==t.value&&null!==t.value))return;t=\"\"+e._wrapperState.initialValue,n||t===e.value||(e.value=t),e.defaultValue=t}\"\"!==(n=e.name)&&(e.name=\"\"),e.defaultChecked=!!e._wrapperState.initialChecked,\"\"!==n&&(e.name=n)}function ee(e,t,n){\"number\"===t&&Y(e.ownerDocument)===e||(null==n?e.defaultValue=\"\"+e._wrapperState.initialValue:e.defaultValue!==\"\"+n&&(e.defaultValue=\"\"+n))}var te=Array.isArray;function ne(e,t,n,r){if(e=e.options,t){t={};for(var a=0;a<n.length;a++)t[\"$\"+n[a]]=!0;for(n=0;n<e.length;n++)a=t.hasOwnProperty(\"$\"+e[n].value),e[n].selected!==a&&(e[n].selected=a),a&&r&&(e[n].defaultSelected=!0)}else{for(n=\"\"+V(n),t=null,a=0;a<e.length;a++){if(e[a].value===n)return e[a].selected=!0,void(r&&(e[a].defaultSelected=!0));null!==t||e[a].disabled||(t=e[a])}null!==t&&(t.selected=!0)}}function re(e,t){if(null!=t.dangerouslySetInnerHTML)throw Error(i(91));return R({},t,{value:void 0,defaultValue:void 0,children:\"\"+e._wrapperState.initialValue})}function ae(e,t){var n=t.value;if(null==n){if(n=t.children,t=t.defaultValue,null!=n){if(null!=t)throw Error(i(92));if(te(n)){if(1<n.length)throw Error(i(93));n=n[0]}t=n}null==t&&(t=\"\"),n=t}e._wrapperState={initialValue:V(n)}}function ie(e,t){var n=V(t.value),r=V(t.defaultValue);null!=n&&((n=\"\"+n)!==e.value&&(e.value=n),null==t.defaultValue&&e.defaultValue!==n&&(e.defaultValue=n)),null!=r&&(e.defaultValue=\"\"+r)}function oe(e){var t=e.textContent;t===e._wrapperState.initialValue&&\"\"!==t&&null!==t&&(e.value=t)}function ue(e){switch(e){case\"svg\":return\"http://www.w3.org/2000/svg\";case\"math\":return\"http://www.w3.org/1998/Math/MathML\";default:return\"http://www.w3.org/1999/xhtml\"}}function le(e,t){return null==e||\"http://www.w3.org/1999/xhtml\"===e?ue(t):\"http://www.w3.org/2000/svg\"===e&&\"foreignObject\"===t?\"http://www.w3.org/1999/xhtml\":e}var se,ce,fe=(ce=function(e,t){if(\"http://www.w3.org/2000/svg\"!==e.namespaceURI||\"innerHTML\"in e)e.innerHTML=t;else{for((se=se||document.createElement(\"div\")).innerHTML=\"<svg>\"+t.valueOf().toString()+\"</svg>\",t=se.firstChild;e.firstChild;)e.removeChild(e.firstChild);for(;t.firstChild;)e.appendChild(t.firstChild)}},\"undefined\"!=typeof MSApp&&MSApp.execUnsafeLocalFunction?function(e,t,n,r){MSApp.execUnsafeLocalFunction((function(){return ce(e,t)}))}:ce);function pe(e,t){if(t){var n=e.firstChild;if(n&&n===e.lastChild&&3===n.nodeType)return void(n.nodeValue=t)}e.textContent=t}var de={animationIterationCount:!0,aspectRatio:!0,borderImageOutset:!0,borderImageSlice:!0,borderImageWidth:!0,boxFlex:!0,boxFlexGroup:!0,boxOrdinalGroup:!0,columnCount:!0,columns:!0,flex:!0,flexGrow:!0,flexPositive:!0,flexShrink:!0,flexNegative:!0,flexOrder:!0,gridArea:!0,gridRow:!0,gridRowEnd:!0,gridRowSpan:!0,gridRowStart:!0,gridColumn:!0,gridColumnEnd:!0,gridColumnSpan:!0,gridColumnStart:!0,fontWeight:!0,lineClamp:!0,lineHeight:!0,opacity:!0,order:!0,orphans:!0,tabSize:!0,widows:!0,zIndex:!0,zoom:!0,fillOpacity:!0,floodOpacity:!0,stopOpacity:!0,strokeDasharray:!0,strokeDashoffset:!0,strokeMiterlimit:!0,strokeOpacity:!0,strokeWidth:!0},he=[\"Webkit\",\"ms\",\"Moz\",\"O\"];function ve(e,t,n){return null==t||\"boolean\"==typeof t||\"\"===t?\"\":n||\"number\"!=typeof t||0===t||de.hasOwnProperty(e)&&de[e]?(\"\"+t).trim():t+\"px\"}function ge(e,t){for(var n in e=e.style,t)if(t.hasOwnProperty(n)){var r=0===n.indexOf(\"--\"),a=ve(n,t[n],r);\"float\"===n&&(n=\"cssFloat\"),r?e.setProperty(n,a):e[n]=a}}Object.keys(de).forEach((function(e){he.forEach((function(t){t=t+e.charAt(0).toUpperCase()+e.substring(1),de[t]=de[e]}))}));var ye=R({menuitem:!0},{area:!0,base:!0,br:!0,col:!0,embed:!0,hr:!0,img:!0,input:!0,keygen:!0,link:!0,meta:!0,param:!0,source:!0,track:!0,wbr:!0});function me(e,t){if(t){if(ye[e]&&(null!=t.children||null!=t.dangerouslySetInnerHTML))throw Error(i(137,e));if(null!=t.dangerouslySetInnerHTML){if(null!=t.children)throw Error(i(60));if(\"object\"!=typeof t.dangerouslySetInnerHTML||!(\"__html\"in t.dangerouslySetInnerHTML))throw Error(i(61))}if(null!=t.style&&\"object\"!=typeof t.style)throw Error(i(62))}}function be(e,t){if(-1===e.indexOf(\"-\"))return\"string\"==typeof t.is;switch(e){case\"annotation-xml\":case\"color-profile\":case\"font-face\":case\"font-face-src\":case\"font-face-uri\":case\"font-face-format\":case\"font-face-name\":case\"missing-glyph\":return!1;default:return!0}}var _e=null;function we(e){return(e=e.target||e.srcElement||window).correspondingUseElement&&(e=e.correspondingUseElement),3===e.nodeType?e.parentNode:e}var xe=null,ke=null,Se=null;function Ee(e){if(e=ba(e)){if(\"function\"!=typeof xe)throw Error(i(280));var t=e.stateNode;t&&(t=wa(t),xe(e.stateNode,e.type,t))}}function Ce(e){ke?Se?Se.push(e):Se=[e]:ke=e}function Te(){if(ke){var e=ke,t=Se;if(Se=ke=null,Ee(e),t)for(e=0;e<t.length;e++)Ee(t[e])}}function Me(e,t){return e(t)}function Ne(){}var Pe=!1;function ze(e,t,n){if(Pe)return e(t,n);Pe=!0;try{return Me(e,t,n)}finally{Pe=!1,(null!==ke||null!==Se)&&(Ne(),Te())}}function Le(e,t){var n=e.stateNode;if(null===n)return null;var r=wa(n);if(null===r)return null;n=r[t];e:switch(t){case\"onClick\":case\"onClickCapture\":case\"onDoubleClick\":case\"onDoubleClickCapture\":case\"onMouseDown\":case\"onMouseDownCapture\":case\"onMouseMove\":case\"onMouseMoveCapture\":case\"onMouseUp\":case\"onMouseUpCapture\":case\"onMouseEnter\":(r=!r.disabled)||(r=!(\"button\"===(e=e.type)||\"input\"===e||\"select\"===e||\"textarea\"===e)),e=!r;break e;default:e=!1}if(e)return null;if(n&&\"function\"!=typeof n)throw Error(i(231,t,typeof n));return n}var Oe=!1;if(c)try{var Ae={};Object.defineProperty(Ae,\"passive\",{get:function(){Oe=!0}}),window.addEventListener(\"test\",Ae,Ae),window.removeEventListener(\"test\",Ae,Ae)}catch(ce){Oe=!1}function Fe(e,t,n,r,a,i,o,u,l){var s=Array.prototype.slice.call(arguments,3);try{t.apply(n,s)}catch(e){this.onError(e)}}var De=!1,Re=null,je=!1,Ue=null,Ie={onError:function(e){De=!0,Re=e}};function $e(e,t,n,r,a,i,o,u,l){De=!1,Re=null,Fe.apply(Ie,arguments)}function Be(e){var t=e,n=e;if(e.alternate)for(;t.return;)t=t.return;else{e=t;do{0!=(4098&(t=e).flags)&&(n=t.return),e=t.return}while(e)}return 3===t.tag?n:null}function We(e){if(13===e.tag){var t=e.memoizedState;if(null===t&&null!==(e=e.alternate)&&(t=e.memoizedState),null!==t)return t.dehydrated}return null}function Ve(e){if(Be(e)!==e)throw Error(i(188))}function He(e){return null!==(e=function(e){var t=e.alternate;if(!t){if(null===(t=Be(e)))throw Error(i(188));return t!==e?null:e}for(var n=e,r=t;;){var a=n.return;if(null===a)break;var o=a.alternate;if(null===o){if(null!==(r=a.return)){n=r;continue}break}if(a.child===o.child){for(o=a.child;o;){if(o===n)return Ve(a),e;if(o===r)return Ve(a),t;o=o.sibling}throw Error(i(188))}if(n.return!==r.return)n=a,r=o;else{for(var u=!1,l=a.child;l;){if(l===n){u=!0,n=a,r=o;break}if(l===r){u=!0,r=a,n=o;break}l=l.sibling}if(!u){for(l=o.child;l;){if(l===n){u=!0,n=o,r=a;break}if(l===r){u=!0,r=o,n=a;break}l=l.sibling}if(!u)throw Error(i(189))}}if(n.alternate!==r)throw Error(i(190))}if(3!==n.tag)throw Error(i(188));return n.stateNode.current===n?e:t}(e))?qe(e):null}function qe(e){if(5===e.tag||6===e.tag)return e;for(e=e.child;null!==e;){var t=qe(e);if(null!==t)return t;e=e.sibling}return null}var Qe=a.unstable_scheduleCallback,Ye=a.unstable_cancelCallback,Ge=a.unstable_shouldYield,Ke=a.unstable_requestPaint,Ze=a.unstable_now,Xe=a.unstable_getCurrentPriorityLevel,Je=a.unstable_ImmediatePriority,et=a.unstable_UserBlockingPriority,tt=a.unstable_NormalPriority,nt=a.unstable_LowPriority,rt=a.unstable_IdlePriority,at=null,it=null,ot=Math.clz32?Math.clz32:function(e){return 0===(e>>>=0)?32:31-(ut(e)/lt|0)|0},ut=Math.log,lt=Math.LN2,st=64,ct=4194304;function ft(e){switch(e&-e){case 1:return 1;case 2:return 2;case 4:return 4;case 8:return 8;case 16:return 16;case 32:return 32;case 64:case 128:case 256:case 512:case 1024:case 2048:case 4096:case 8192:case 16384:case 32768:case 65536:case 131072:case 262144:case 524288:case 1048576:case 2097152:return 4194240&e;case 4194304:case 8388608:case 16777216:case 33554432:case 67108864:return 130023424&e;case 134217728:return 134217728;case 268435456:return 268435456;case 536870912:return 536870912;case 1073741824:return 1073741824;default:return e}}function pt(e,t){var n=e.pendingLanes;if(0===n)return 0;var r=0,a=e.suspendedLanes,i=e.pingedLanes,o=268435455&n;if(0!==o){var u=o&~a;0!==u?r=ft(u):0!=(i&=o)&&(r=ft(i))}else 0!=(o=n&~a)?r=ft(o):0!==i&&(r=ft(i));if(0===r)return 0;if(0!==t&&t!==r&&0==(t&a)&&((a=r&-r)>=(i=t&-t)||16===a&&0!=(4194240&i)))return t;if(0!=(4&r)&&(r|=16&n),0!==(t=e.entangledLanes))for(e=e.entanglements,t&=r;0<t;)a=1<<(n=31-ot(t)),r|=e[n],t&=~a;return r}function dt(e,t){switch(e){case 1:case 2:case 4:return t+250;case 8:case 16:case 32:case 64:case 128:case 256:case 512:case 1024:case 2048:case 4096:case 8192:case 16384:case 32768:case 65536:case 131072:case 262144:case 524288:case 1048576:case 2097152:return t+5e3;default:return-1}}function ht(e){return 0!=(e=-1073741825&e.pendingLanes)?e:1073741824&e?1073741824:0}function vt(){var e=st;return 0==(4194240&(st<<=1))&&(st=64),e}function gt(e){for(var t=[],n=0;31>n;n++)t.push(e);return t}function yt(e,t,n){e.pendingLanes|=t,536870912!==t&&(e.suspendedLanes=0,e.pingedLanes=0),(e=e.eventTimes)[t=31-ot(t)]=n}function mt(e,t){var n=e.entangledLanes|=t;for(e=e.entanglements;n;){var r=31-ot(n),a=1<<r;a&t|e[r]&t&&(e[r]|=t),n&=~a}}var bt=0;function _t(e){return 1<(e&=-e)?4<e?0!=(268435455&e)?16:536870912:4:1}var wt,xt,kt,St,Et,Ct=!1,Tt=[],Mt=null,Nt=null,Pt=null,zt=new Map,Lt=new Map,Ot=[],At=\"mousedown mouseup touchcancel touchend touchstart auxclick dblclick pointercancel pointerdown pointerup dragend dragstart drop compositionend compositionstart keydown keypress keyup input textInput copy cut paste click change contextmenu reset submit\".split(\" \");function Ft(e,t){switch(e){case\"focusin\":case\"focusout\":Mt=null;break;case\"dragenter\":case\"dragleave\":Nt=null;break;case\"mouseover\":case\"mouseout\":Pt=null;break;case\"pointerover\":case\"pointerout\":zt.delete(t.pointerId);break;case\"gotpointercapture\":case\"lostpointercapture\":Lt.delete(t.pointerId)}}function Dt(e,t,n,r,a,i){return null===e||e.nativeEvent!==i?(e={blockedOn:t,domEventName:n,eventSystemFlags:r,nativeEvent:i,targetContainers:[a]},null!==t&&null!==(t=ba(t))&&xt(t),e):(e.eventSystemFlags|=r,t=e.targetContainers,null!==a&&-1===t.indexOf(a)&&t.push(a),e)}function Rt(e){var t=ma(e.target);if(null!==t){var n=Be(t);if(null!==n)if(13===(t=n.tag)){if(null!==(t=We(n)))return e.blockedOn=t,void Et(e.priority,(function(){kt(n)}))}else if(3===t&&n.stateNode.current.memoizedState.isDehydrated)return void(e.blockedOn=3===n.tag?n.stateNode.containerInfo:null)}e.blockedOn=null}function jt(e){if(null!==e.blockedOn)return!1;for(var t=e.targetContainers;0<t.length;){var n=Gt(e.domEventName,e.eventSystemFlags,t[0],e.nativeEvent);if(null!==n)return null!==(t=ba(n))&&xt(t),e.blockedOn=n,!1;var r=new(n=e.nativeEvent).constructor(n.type,n);_e=r,n.target.dispatchEvent(r),_e=null,t.shift()}return!0}function Ut(e,t,n){jt(e)&&n.delete(t)}function It(){Ct=!1,null!==Mt&&jt(Mt)&&(Mt=null),null!==Nt&&jt(Nt)&&(Nt=null),null!==Pt&&jt(Pt)&&(Pt=null),zt.forEach(Ut),Lt.forEach(Ut)}function $t(e,t){e.blockedOn===t&&(e.blockedOn=null,Ct||(Ct=!0,a.unstable_scheduleCallback(a.unstable_NormalPriority,It)))}function Bt(e){function t(t){return $t(t,e)}if(0<Tt.length){$t(Tt[0],e);for(var n=1;n<Tt.length;n++){var r=Tt[n];r.blockedOn===e&&(r.blockedOn=null)}}for(null!==Mt&&$t(Mt,e),null!==Nt&&$t(Nt,e),null!==Pt&&$t(Pt,e),zt.forEach(t),Lt.forEach(t),n=0;n<Ot.length;n++)(r=Ot[n]).blockedOn===e&&(r.blockedOn=null);for(;0<Ot.length&&null===(n=Ot[0]).blockedOn;)Rt(n),null===n.blockedOn&&Ot.shift()}var Wt=_.ReactCurrentBatchConfig,Vt=!0;function Ht(e,t,n,r){var a=bt,i=Wt.transition;Wt.transition=null;try{bt=1,Qt(e,t,n,r)}finally{bt=a,Wt.transition=i}}function qt(e,t,n,r){var a=bt,i=Wt.transition;Wt.transition=null;try{bt=4,Qt(e,t,n,r)}finally{bt=a,Wt.transition=i}}function Qt(e,t,n,r){if(Vt){var a=Gt(e,t,n,r);if(null===a)Vr(e,t,r,Yt,n),Ft(e,r);else if(function(e,t,n,r,a){switch(t){case\"focusin\":return Mt=Dt(Mt,e,t,n,r,a),!0;case\"dragenter\":return Nt=Dt(Nt,e,t,n,r,a),!0;case\"mouseover\":return Pt=Dt(Pt,e,t,n,r,a),!0;case\"pointerover\":var i=a.pointerId;return zt.set(i,Dt(zt.get(i)||null,e,t,n,r,a)),!0;case\"gotpointercapture\":return i=a.pointerId,Lt.set(i,Dt(Lt.get(i)||null,e,t,n,r,a)),!0}return!1}(a,e,t,n,r))r.stopPropagation();else if(Ft(e,r),4&t&&-1<At.indexOf(e)){for(;null!==a;){var i=ba(a);if(null!==i&&wt(i),null===(i=Gt(e,t,n,r))&&Vr(e,t,r,Yt,n),i===a)break;a=i}null!==a&&r.stopPropagation()}else Vr(e,t,r,null,n)}}var Yt=null;function Gt(e,t,n,r){if(Yt=null,null!==(e=ma(e=we(r))))if(null===(t=Be(e)))e=null;else if(13===(n=t.tag)){if(null!==(e=We(t)))return e;e=null}else if(3===n){if(t.stateNode.current.memoizedState.isDehydrated)return 3===t.tag?t.stateNode.containerInfo:null;e=null}else t!==e&&(e=null);return Yt=e,null}function Kt(e){switch(e){case\"cancel\":case\"click\":case\"close\":case\"contextmenu\":case\"copy\":case\"cut\":case\"auxclick\":case\"dblclick\":case\"dragend\":case\"dragstart\":case\"drop\":case\"focusin\":case\"focusout\":case\"input\":case\"invalid\":case\"keydown\":case\"keypress\":case\"keyup\":case\"mousedown\":case\"mouseup\":case\"paste\":case\"pause\":case\"play\":case\"pointercancel\":case\"pointerdown\":case\"pointerup\":case\"ratechange\":case\"reset\":case\"resize\":case\"seeked\":case\"submit\":case\"touchcancel\":case\"touchend\":case\"touchstart\":case\"volumechange\":case\"change\":case\"selectionchange\":case\"textInput\":case\"compositionstart\":case\"compositionend\":case\"compositionupdate\":case\"beforeblur\":case\"afterblur\":case\"beforeinput\":case\"blur\":case\"fullscreenchange\":case\"focus\":case\"hashchange\":case\"popstate\":case\"select\":case\"selectstart\":return 1;case\"drag\":case\"dragenter\":case\"dragexit\":case\"dragleave\":case\"dragover\":case\"mousemove\":case\"mouseout\":case\"mouseover\":case\"pointermove\":case\"pointerout\":case\"pointerover\":case\"scroll\":case\"toggle\":case\"touchmove\":case\"wheel\":case\"mouseenter\":case\"mouseleave\":case\"pointerenter\":case\"pointerleave\":return 4;case\"message\":switch(Xe()){case Je:return 1;case et:return 4;case tt:case nt:return 16;case rt:return 536870912;default:return 16}default:return 16}}var Zt=null,Xt=null,Jt=null;function en(){if(Jt)return Jt;var e,t,n=Xt,r=n.length,a=\"value\"in Zt?Zt.value:Zt.textContent,i=a.length;for(e=0;e<r&&n[e]===a[e];e++);var o=r-e;for(t=1;t<=o&&n[r-t]===a[i-t];t++);return Jt=a.slice(e,1<t?1-t:void 0)}function tn(e){var t=e.keyCode;return\"charCode\"in e?0===(e=e.charCode)&&13===t&&(e=13):e=t,10===e&&(e=13),32<=e||13===e?e:0}function nn(){return!0}function rn(){return!1}function an(e){function t(t,n,r,a,i){for(var o in this._reactName=t,this._targetInst=r,this.type=n,this.nativeEvent=a,this.target=i,this.currentTarget=null,e)e.hasOwnProperty(o)&&(t=e[o],this[o]=t?t(a):a[o]);return this.isDefaultPrevented=(null!=a.defaultPrevented?a.defaultPrevented:!1===a.returnValue)?nn:rn,this.isPropagationStopped=rn,this}return R(t.prototype,{preventDefault:function(){this.defaultPrevented=!0;var e=this.nativeEvent;e&&(e.preventDefault?e.preventDefault():\"unknown\"!=typeof e.returnValue&&(e.returnValue=!1),this.isDefaultPrevented=nn)},stopPropagation:function(){var e=this.nativeEvent;e&&(e.stopPropagation?e.stopPropagation():\"unknown\"!=typeof e.cancelBubble&&(e.cancelBubble=!0),this.isPropagationStopped=nn)},persist:function(){},isPersistent:nn}),t}var on,un,ln,sn={eventPhase:0,bubbles:0,cancelable:0,timeStamp:function(e){return e.timeStamp||Date.now()},defaultPrevented:0,isTrusted:0},cn=an(sn),fn=R({},sn,{view:0,detail:0}),pn=an(fn),dn=R({},fn,{screenX:0,screenY:0,clientX:0,clientY:0,pageX:0,pageY:0,ctrlKey:0,shiftKey:0,altKey:0,metaKey:0,getModifierState:En,button:0,buttons:0,relatedTarget:function(e){return void 0===e.relatedTarget?e.fromElement===e.srcElement?e.toElement:e.fromElement:e.relatedTarget},movementX:function(e){return\"movementX\"in e?e.movementX:(e!==ln&&(ln&&\"mousemove\"===e.type?(on=e.screenX-ln.screenX,un=e.screenY-ln.screenY):un=on=0,ln=e),on)},movementY:function(e){return\"movementY\"in e?e.movementY:un}}),hn=an(dn),vn=an(R({},dn,{dataTransfer:0})),gn=an(R({},fn,{relatedTarget:0})),yn=an(R({},sn,{animationName:0,elapsedTime:0,pseudoElement:0})),mn=R({},sn,{clipboardData:function(e){return\"clipboardData\"in e?e.clipboardData:window.clipboardData}}),bn=an(mn),_n=an(R({},sn,{data:0})),wn={Esc:\"Escape\",Spacebar:\" \",Left:\"ArrowLeft\",Up:\"ArrowUp\",Right:\"ArrowRight\",Down:\"ArrowDown\",Del:\"Delete\",Win:\"OS\",Menu:\"ContextMenu\",Apps:\"ContextMenu\",Scroll:\"ScrollLock\",MozPrintableKey:\"Unidentified\"},xn={8:\"Backspace\",9:\"Tab\",12:\"Clear\",13:\"Enter\",16:\"Shift\",17:\"Control\",18:\"Alt\",19:\"Pause\",20:\"CapsLock\",27:\"Escape\",32:\" \",33:\"PageUp\",34:\"PageDown\",35:\"End\",36:\"Home\",37:\"ArrowLeft\",38:\"ArrowUp\",39:\"ArrowRight\",40:\"ArrowDown\",45:\"Insert\",46:\"Delete\",112:\"F1\",113:\"F2\",114:\"F3\",115:\"F4\",116:\"F5\",117:\"F6\",118:\"F7\",119:\"F8\",120:\"F9\",121:\"F10\",122:\"F11\",123:\"F12\",144:\"NumLock\",145:\"ScrollLock\",224:\"Meta\"},kn={Alt:\"altKey\",Control:\"ctrlKey\",Meta:\"metaKey\",Shift:\"shiftKey\"};function Sn(e){var t=this.nativeEvent;return t.getModifierState?t.getModifierState(e):!!(e=kn[e])&&!!t[e]}function En(){return Sn}var Cn=R({},fn,{key:function(e){if(e.key){var t=wn[e.key]||e.key;if(\"Unidentified\"!==t)return t}return\"keypress\"===e.type?13===(e=tn(e))?\"Enter\":String.fromCharCode(e):\"keydown\"===e.type||\"keyup\"===e.type?xn[e.keyCode]||\"Unidentified\":\"\"},code:0,location:0,ctrlKey:0,shiftKey:0,altKey:0,metaKey:0,repeat:0,locale:0,getModifierState:En,charCode:function(e){return\"keypress\"===e.type?tn(e):0},keyCode:function(e){return\"keydown\"===e.type||\"keyup\"===e.type?e.keyCode:0},which:function(e){return\"keypress\"===e.type?tn(e):\"keydown\"===e.type||\"keyup\"===e.type?e.keyCode:0}}),Tn=an(Cn),Mn=an(R({},dn,{pointerId:0,width:0,height:0,pressure:0,tangentialPressure:0,tiltX:0,tiltY:0,twist:0,pointerType:0,isPrimary:0})),Nn=an(R({},fn,{touches:0,targetTouches:0,changedTouches:0,altKey:0,metaKey:0,ctrlKey:0,shiftKey:0,getModifierState:En})),Pn=an(R({},sn,{propertyName:0,elapsedTime:0,pseudoElement:0})),zn=R({},dn,{deltaX:function(e){return\"deltaX\"in e?e.deltaX:\"wheelDeltaX\"in e?-e.wheelDeltaX:0},deltaY:function(e){return\"deltaY\"in e?e.deltaY:\"wheelDeltaY\"in e?-e.wheelDeltaY:\"wheelDelta\"in e?-e.wheelDelta:0},deltaZ:0,deltaMode:0}),Ln=an(zn),On=[9,13,27,32],An=c&&\"CompositionEvent\"in window,Fn=null;c&&\"documentMode\"in document&&(Fn=document.documentMode);var Dn=c&&\"TextEvent\"in window&&!Fn,Rn=c&&(!An||Fn&&8<Fn&&11>=Fn),jn=String.fromCharCode(32),Un=!1;function In(e,t){switch(e){case\"keyup\":return-1!==On.indexOf(t.keyCode);case\"keydown\":return 229!==t.keyCode;case\"keypress\":case\"mousedown\":case\"focusout\":return!0;default:return!1}}function $n(e){return\"object\"==typeof(e=e.detail)&&\"data\"in e?e.data:null}var Bn=!1,Wn={color:!0,date:!0,datetime:!0,\"datetime-local\":!0,email:!0,month:!0,number:!0,password:!0,range:!0,search:!0,tel:!0,text:!0,time:!0,url:!0,week:!0};function Vn(e){var t=e&&e.nodeName&&e.nodeName.toLowerCase();return\"input\"===t?!!Wn[e.type]:\"textarea\"===t}function Hn(e,t,n,r){Ce(r),0<(t=qr(t,\"onChange\")).length&&(n=new cn(\"onChange\",\"change\",null,n,r),e.push({event:n,listeners:t}))}var qn=null,Qn=null;function Yn(e){jr(e,0)}function Gn(e){if(Q(_a(e)))return e}function Kn(e,t){if(\"change\"===e)return t}var Zn=!1;if(c){var Xn;if(c){var Jn=\"oninput\"in document;if(!Jn){var er=document.createElement(\"div\");er.setAttribute(\"oninput\",\"return;\"),Jn=\"function\"==typeof er.oninput}Xn=Jn}else Xn=!1;Zn=Xn&&(!document.documentMode||9<document.documentMode)}function tr(){qn&&(qn.detachEvent(\"onpropertychange\",nr),Qn=qn=null)}function nr(e){if(\"value\"===e.propertyName&&Gn(Qn)){var t=[];Hn(t,Qn,e,we(e)),ze(Yn,t)}}function rr(e,t,n){\"focusin\"===e?(tr(),Qn=n,(qn=t).attachEvent(\"onpropertychange\",nr)):\"focusout\"===e&&tr()}function ar(e){if(\"selectionchange\"===e||\"keyup\"===e||\"keydown\"===e)return Gn(Qn)}function ir(e,t){if(\"click\"===e)return Gn(t)}function or(e,t){if(\"input\"===e||\"change\"===e)return Gn(t)}var ur=\"function\"==typeof Object.is?Object.is:function(e,t){return e===t&&(0!==e||1/e==1/t)||e!=e&&t!=t};function lr(e,t){if(ur(e,t))return!0;if(\"object\"!=typeof e||null===e||\"object\"!=typeof t||null===t)return!1;var n=Object.keys(e),r=Object.keys(t);if(n.length!==r.length)return!1;for(r=0;r<n.length;r++){var a=n[r];if(!f.call(t,a)||!ur(e[a],t[a]))return!1}return!0}function sr(e){for(;e&&e.firstChild;)e=e.firstChild;return e}function cr(e,t){var n,r=sr(e);for(e=0;r;){if(3===r.nodeType){if(n=e+r.textContent.length,e<=t&&n>=t)return{node:r,offset:t-e};e=n}e:{for(;r;){if(r.nextSibling){r=r.nextSibling;break e}r=r.parentNode}r=void 0}r=sr(r)}}function fr(e,t){return!(!e||!t)&&(e===t||(!e||3!==e.nodeType)&&(t&&3===t.nodeType?fr(e,t.parentNode):\"contains\"in e?e.contains(t):!!e.compareDocumentPosition&&!!(16&e.compareDocumentPosition(t))))}function pr(){for(var e=window,t=Y();t instanceof e.HTMLIFrameElement;){try{var n=\"string\"==typeof t.contentWindow.location.href}catch(e){n=!1}if(!n)break;t=Y((e=t.contentWindow).document)}return t}function dr(e){var t=e&&e.nodeName&&e.nodeName.toLowerCase();return t&&(\"input\"===t&&(\"text\"===e.type||\"search\"===e.type||\"tel\"===e.type||\"url\"===e.type||\"password\"===e.type)||\"textarea\"===t||\"true\"===e.contentEditable)}function hr(e){var t=pr(),n=e.focusedElem,r=e.selectionRange;if(t!==n&&n&&n.ownerDocument&&fr(n.ownerDocument.documentElement,n)){if(null!==r&&dr(n))if(t=r.start,void 0===(e=r.end)&&(e=t),\"selectionStart\"in n)n.selectionStart=t,n.selectionEnd=Math.min(e,n.value.length);else if((e=(t=n.ownerDocument||document)&&t.defaultView||window).getSelection){e=e.getSelection();var a=n.textContent.length,i=Math.min(r.start,a);r=void 0===r.end?i:Math.min(r.end,a),!e.extend&&i>r&&(a=r,r=i,i=a),a=cr(n,i);var o=cr(n,r);a&&o&&(1!==e.rangeCount||e.anchorNode!==a.node||e.anchorOffset!==a.offset||e.focusNode!==o.node||e.focusOffset!==o.offset)&&((t=t.createRange()).setStart(a.node,a.offset),e.removeAllRanges(),i>r?(e.addRange(t),e.extend(o.node,o.offset)):(t.setEnd(o.node,o.offset),e.addRange(t)))}for(t=[],e=n;e=e.parentNode;)1===e.nodeType&&t.push({element:e,left:e.scrollLeft,top:e.scrollTop});for(\"function\"==typeof n.focus&&n.focus(),n=0;n<t.length;n++)(e=t[n]).element.scrollLeft=e.left,e.element.scrollTop=e.top}}var vr=c&&\"documentMode\"in document&&11>=document.documentMode,gr=null,yr=null,mr=null,br=!1;function _r(e,t,n){var r=n.window===n?n.document:9===n.nodeType?n:n.ownerDocument;br||null==gr||gr!==Y(r)||(r=\"selectionStart\"in(r=gr)&&dr(r)?{start:r.selectionStart,end:r.selectionEnd}:{anchorNode:(r=(r.ownerDocument&&r.ownerDocument.defaultView||window).getSelection()).anchorNode,anchorOffset:r.anchorOffset,focusNode:r.focusNode,focusOffset:r.focusOffset},mr&&lr(mr,r)||(mr=r,0<(r=qr(yr,\"onSelect\")).length&&(t=new cn(\"onSelect\",\"select\",null,t,n),e.push({event:t,listeners:r}),t.target=gr)))}function wr(e,t){var n={};return n[e.toLowerCase()]=t.toLowerCase(),n[\"Webkit\"+e]=\"webkit\"+t,n[\"Moz\"+e]=\"moz\"+t,n}var xr={animationend:wr(\"Animation\",\"AnimationEnd\"),animationiteration:wr(\"Animation\",\"AnimationIteration\"),animationstart:wr(\"Animation\",\"AnimationStart\"),transitionend:wr(\"Transition\",\"TransitionEnd\")},kr={},Sr={};function Er(e){if(kr[e])return kr[e];if(!xr[e])return e;var t,n=xr[e];for(t in n)if(n.hasOwnProperty(t)&&t in Sr)return kr[e]=n[t];return e}c&&(Sr=document.createElement(\"div\").style,\"AnimationEvent\"in window||(delete xr.animationend.animation,delete xr.animationiteration.animation,delete xr.animationstart.animation),\"TransitionEvent\"in window||delete xr.transitionend.transition);var Cr=Er(\"animationend\"),Tr=Er(\"animationiteration\"),Mr=Er(\"animationstart\"),Nr=Er(\"transitionend\"),Pr=new Map,zr=\"abort auxClick cancel canPlay canPlayThrough click close contextMenu copy cut drag dragEnd dragEnter dragExit dragLeave dragOver dragStart drop durationChange emptied encrypted ended error gotPointerCapture input invalid keyDown keyPress keyUp load loadedData loadedMetadata loadStart lostPointerCapture mouseDown mouseMove mouseOut mouseOver mouseUp paste pause play playing pointerCancel pointerDown pointerMove pointerOut pointerOver pointerUp progress rateChange reset resize seeked seeking stalled submit suspend timeUpdate touchCancel touchEnd touchStart volumeChange scroll toggle touchMove waiting wheel\".split(\" \");function Lr(e,t){Pr.set(e,t),l(t,[e])}for(var Or=0;Or<zr.length;Or++){var Ar=zr[Or];Lr(Ar.toLowerCase(),\"on\"+(Ar[0].toUpperCase()+Ar.slice(1)))}Lr(Cr,\"onAnimationEnd\"),Lr(Tr,\"onAnimationIteration\"),Lr(Mr,\"onAnimationStart\"),Lr(\"dblclick\",\"onDoubleClick\"),Lr(\"focusin\",\"onFocus\"),Lr(\"focusout\",\"onBlur\"),Lr(Nr,\"onTransitionEnd\"),s(\"onMouseEnter\",[\"mouseout\",\"mouseover\"]),s(\"onMouseLeave\",[\"mouseout\",\"mouseover\"]),s(\"onPointerEnter\",[\"pointerout\",\"pointerover\"]),s(\"onPointerLeave\",[\"pointerout\",\"pointerover\"]),l(\"onChange\",\"change click focusin focusout input keydown keyup selectionchange\".split(\" \")),l(\"onSelect\",\"focusout contextmenu dragend focusin keydown keyup mousedown mouseup selectionchange\".split(\" \")),l(\"onBeforeInput\",[\"compositionend\",\"keypress\",\"textInput\",\"paste\"]),l(\"onCompositionEnd\",\"compositionend focusout keydown keypress keyup mousedown\".split(\" \")),l(\"onCompositionStart\",\"compositionstart focusout keydown keypress keyup mousedown\".split(\" \")),l(\"onCompositionUpdate\",\"compositionupdate focusout keydown keypress keyup mousedown\".split(\" \"));var Fr=\"abort canplay canplaythrough durationchange emptied encrypted ended error loadeddata loadedmetadata loadstart pause play playing progress ratechange resize seeked seeking stalled suspend timeupdate volumechange waiting\".split(\" \"),Dr=new Set(\"cancel close invalid load scroll toggle\".split(\" \").concat(Fr));function Rr(e,t,n){var r=e.type||\"unknown-event\";e.currentTarget=n,function(e,t,n,r,a,o,u,l,s){if($e.apply(this,arguments),De){if(!De)throw Error(i(198));var c=Re;De=!1,Re=null,je||(je=!0,Ue=c)}}(r,t,void 0,e),e.currentTarget=null}function jr(e,t){t=0!=(4&t);for(var n=0;n<e.length;n++){var r=e[n],a=r.event;r=r.listeners;e:{var i=void 0;if(t)for(var o=r.length-1;0<=o;o--){var u=r[o],l=u.instance,s=u.currentTarget;if(u=u.listener,l!==i&&a.isPropagationStopped())break e;Rr(a,u,s),i=l}else for(o=0;o<r.length;o++){if(l=(u=r[o]).instance,s=u.currentTarget,u=u.listener,l!==i&&a.isPropagationStopped())break e;Rr(a,u,s),i=l}}}if(je)throw e=Ue,je=!1,Ue=null,e}function Ur(e,t){var n=t[va];void 0===n&&(n=t[va]=new Set);var r=e+\"__bubble\";n.has(r)||(Wr(t,e,2,!1),n.add(r))}function Ir(e,t,n){var r=0;t&&(r|=4),Wr(n,e,r,t)}var $r=\"_reactListening\"+Math.random().toString(36).slice(2);function Br(e){if(!e[$r]){e[$r]=!0,o.forEach((function(t){\"selectionchange\"!==t&&(Dr.has(t)||Ir(t,!1,e),Ir(t,!0,e))}));var t=9===e.nodeType?e:e.ownerDocument;null===t||t[$r]||(t[$r]=!0,Ir(\"selectionchange\",!1,t))}}function Wr(e,t,n,r){switch(Kt(t)){case 1:var a=Ht;break;case 4:a=qt;break;default:a=Qt}n=a.bind(null,t,n,e),a=void 0,!Oe||\"touchstart\"!==t&&\"touchmove\"!==t&&\"wheel\"!==t||(a=!0),r?void 0!==a?e.addEventListener(t,n,{capture:!0,passive:a}):e.addEventListener(t,n,!0):void 0!==a?e.addEventListener(t,n,{passive:a}):e.addEventListener(t,n,!1)}function Vr(e,t,n,r,a){var i=r;if(0==(1&t)&&0==(2&t)&&null!==r)e:for(;;){if(null===r)return;var o=r.tag;if(3===o||4===o){var u=r.stateNode.containerInfo;if(u===a||8===u.nodeType&&u.parentNode===a)break;if(4===o)for(o=r.return;null!==o;){var l=o.tag;if((3===l||4===l)&&((l=o.stateNode.containerInfo)===a||8===l.nodeType&&l.parentNode===a))return;o=o.return}for(;null!==u;){if(null===(o=ma(u)))return;if(5===(l=o.tag)||6===l){r=i=o;continue e}u=u.parentNode}}r=r.return}ze((function(){var r=i,a=we(n),o=[];e:{var u=Pr.get(e);if(void 0!==u){var l=cn,s=e;switch(e){case\"keypress\":if(0===tn(n))break e;case\"keydown\":case\"keyup\":l=Tn;break;case\"focusin\":s=\"focus\",l=gn;break;case\"focusout\":s=\"blur\",l=gn;break;case\"beforeblur\":case\"afterblur\":l=gn;break;case\"click\":if(2===n.button)break e;case\"auxclick\":case\"dblclick\":case\"mousedown\":case\"mousemove\":case\"mouseup\":case\"mouseout\":case\"mouseover\":case\"contextmenu\":l=hn;break;case\"drag\":case\"dragend\":case\"dragenter\":case\"dragexit\":case\"dragleave\":case\"dragover\":case\"dragstart\":case\"drop\":l=vn;break;case\"touchcancel\":case\"touchend\":case\"touchmove\":case\"touchstart\":l=Nn;break;case Cr:case Tr:case Mr:l=yn;break;case Nr:l=Pn;break;case\"scroll\":l=pn;break;case\"wheel\":l=Ln;break;case\"copy\":case\"cut\":case\"paste\":l=bn;break;case\"gotpointercapture\":case\"lostpointercapture\":case\"pointercancel\":case\"pointerdown\":case\"pointermove\":case\"pointerout\":case\"pointerover\":case\"pointerup\":l=Mn}var c=0!=(4&t),f=!c&&\"scroll\"===e,p=c?null!==u?u+\"Capture\":null:u;c=[];for(var d,h=r;null!==h;){var v=(d=h).stateNode;if(5===d.tag&&null!==v&&(d=v,null!==p&&null!=(v=Le(h,p))&&c.push(Hr(h,v,d))),f)break;h=h.return}0<c.length&&(u=new l(u,s,null,n,a),o.push({event:u,listeners:c}))}}if(0==(7&t)){if(l=\"mouseout\"===e||\"pointerout\"===e,(!(u=\"mouseover\"===e||\"pointerover\"===e)||n===_e||!(s=n.relatedTarget||n.fromElement)||!ma(s)&&!s[ha])&&(l||u)&&(u=a.window===a?a:(u=a.ownerDocument)?u.defaultView||u.parentWindow:window,l?(l=r,null!==(s=(s=n.relatedTarget||n.toElement)?ma(s):null)&&(s!==(f=Be(s))||5!==s.tag&&6!==s.tag)&&(s=null)):(l=null,s=r),l!==s)){if(c=hn,v=\"onMouseLeave\",p=\"onMouseEnter\",h=\"mouse\",\"pointerout\"!==e&&\"pointerover\"!==e||(c=Mn,v=\"onPointerLeave\",p=\"onPointerEnter\",h=\"pointer\"),f=null==l?u:_a(l),d=null==s?u:_a(s),(u=new c(v,h+\"leave\",l,n,a)).target=f,u.relatedTarget=d,v=null,ma(a)===r&&((c=new c(p,h+\"enter\",s,n,a)).target=d,c.relatedTarget=f,v=c),f=v,l&&s)e:{for(p=s,h=0,d=c=l;d;d=Qr(d))h++;for(d=0,v=p;v;v=Qr(v))d++;for(;0<h-d;)c=Qr(c),h--;for(;0<d-h;)p=Qr(p),d--;for(;h--;){if(c===p||null!==p&&c===p.alternate)break e;c=Qr(c),p=Qr(p)}c=null}else c=null;null!==l&&Yr(o,u,l,c,!1),null!==s&&null!==f&&Yr(o,f,s,c,!0)}if(\"select\"===(l=(u=r?_a(r):window).nodeName&&u.nodeName.toLowerCase())||\"input\"===l&&\"file\"===u.type)var g=Kn;else if(Vn(u))if(Zn)g=or;else{g=ar;var y=rr}else(l=u.nodeName)&&\"input\"===l.toLowerCase()&&(\"checkbox\"===u.type||\"radio\"===u.type)&&(g=ir);switch(g&&(g=g(e,r))?Hn(o,g,n,a):(y&&y(e,u,r),\"focusout\"===e&&(y=u._wrapperState)&&y.controlled&&\"number\"===u.type&&ee(u,\"number\",u.value)),y=r?_a(r):window,e){case\"focusin\":(Vn(y)||\"true\"===y.contentEditable)&&(gr=y,yr=r,mr=null);break;case\"focusout\":mr=yr=gr=null;break;case\"mousedown\":br=!0;break;case\"contextmenu\":case\"mouseup\":case\"dragend\":br=!1,_r(o,n,a);break;case\"selectionchange\":if(vr)break;case\"keydown\":case\"keyup\":_r(o,n,a)}var m;if(An)e:{switch(e){case\"compositionstart\":var b=\"onCompositionStart\";break e;case\"compositionend\":b=\"onCompositionEnd\";break e;case\"compositionupdate\":b=\"onCompositionUpdate\";break e}b=void 0}else Bn?In(e,n)&&(b=\"onCompositionEnd\"):\"keydown\"===e&&229===n.keyCode&&(b=\"onCompositionStart\");b&&(Rn&&\"ko\"!==n.locale&&(Bn||\"onCompositionStart\"!==b?\"onCompositionEnd\"===b&&Bn&&(m=en()):(Xt=\"value\"in(Zt=a)?Zt.value:Zt.textContent,Bn=!0)),0<(y=qr(r,b)).length&&(b=new _n(b,e,null,n,a),o.push({event:b,listeners:y}),(m||null!==(m=$n(n)))&&(b.data=m))),(m=Dn?function(e,t){switch(e){case\"compositionend\":return $n(t);case\"keypress\":return 32!==t.which?null:(Un=!0,jn);case\"textInput\":return(e=t.data)===jn&&Un?null:e;default:return null}}(e,n):function(e,t){if(Bn)return\"compositionend\"===e||!An&&In(e,t)?(e=en(),Jt=Xt=Zt=null,Bn=!1,e):null;switch(e){case\"paste\":default:return null;case\"keypress\":if(!(t.ctrlKey||t.altKey||t.metaKey)||t.ctrlKey&&t.altKey){if(t.char&&1<t.char.length)return t.char;if(t.which)return String.fromCharCode(t.which)}return null;case\"compositionend\":return Rn&&\"ko\"!==t.locale?null:t.data}}(e,n))&&0<(r=qr(r,\"onBeforeInput\")).length&&(a=new _n(\"onBeforeInput\",\"beforeinput\",null,n,a),o.push({event:a,listeners:r}),a.data=m)}jr(o,t)}))}function Hr(e,t,n){return{instance:e,listener:t,currentTarget:n}}function qr(e,t){for(var n=t+\"Capture\",r=[];null!==e;){var a=e,i=a.stateNode;5===a.tag&&null!==i&&(a=i,null!=(i=Le(e,n))&&r.unshift(Hr(e,i,a)),null!=(i=Le(e,t))&&r.push(Hr(e,i,a))),e=e.return}return r}function Qr(e){if(null===e)return null;do{e=e.return}while(e&&5!==e.tag);return e||null}function Yr(e,t,n,r,a){for(var i=t._reactName,o=[];null!==n&&n!==r;){var u=n,l=u.alternate,s=u.stateNode;if(null!==l&&l===r)break;5===u.tag&&null!==s&&(u=s,a?null!=(l=Le(n,i))&&o.unshift(Hr(n,l,u)):a||null!=(l=Le(n,i))&&o.push(Hr(n,l,u))),n=n.return}0!==o.length&&e.push({event:t,listeners:o})}var Gr=/\\r\\n?/g,Kr=/\\u0000|\\uFFFD/g;function Zr(e){return(\"string\"==typeof e?e:\"\"+e).replace(Gr,\"\\n\").replace(Kr,\"\")}function Xr(e,t,n){if(t=Zr(t),Zr(e)!==t&&n)throw Error(i(425))}function Jr(){}var ea=null,ta=null;function na(e,t){return\"textarea\"===e||\"noscript\"===e||\"string\"==typeof t.children||\"number\"==typeof t.children||\"object\"==typeof t.dangerouslySetInnerHTML&&null!==t.dangerouslySetInnerHTML&&null!=t.dangerouslySetInnerHTML.__html}var ra=\"function\"==typeof setTimeout?setTimeout:void 0,aa=\"function\"==typeof clearTimeout?clearTimeout:void 0,ia=\"function\"==typeof Promise?Promise:void 0,oa=\"function\"==typeof queueMicrotask?queueMicrotask:void 0!==ia?function(e){return ia.resolve(null).then(e).catch(ua)}:ra;function ua(e){setTimeout((function(){throw e}))}function la(e,t){var n=t,r=0;do{var a=n.nextSibling;if(e.removeChild(n),a&&8===a.nodeType)if(\"/$\"===(n=a.data)){if(0===r)return e.removeChild(a),void Bt(t);r--}else\"$\"!==n&&\"$?\"!==n&&\"$!\"!==n||r++;n=a}while(n);Bt(t)}function sa(e){for(;null!=e;e=e.nextSibling){var t=e.nodeType;if(1===t||3===t)break;if(8===t){if(\"$\"===(t=e.data)||\"$!\"===t||\"$?\"===t)break;if(\"/$\"===t)return null}}return e}function ca(e){e=e.previousSibling;for(var t=0;e;){if(8===e.nodeType){var n=e.data;if(\"$\"===n||\"$!\"===n||\"$?\"===n){if(0===t)return e;t--}else\"/$\"===n&&t++}e=e.previousSibling}return null}var fa=Math.random().toString(36).slice(2),pa=\"__reactFiber$\"+fa,da=\"__reactProps$\"+fa,ha=\"__reactContainer$\"+fa,va=\"__reactEvents$\"+fa,ga=\"__reactListeners$\"+fa,ya=\"__reactHandles$\"+fa;function ma(e){var t=e[pa];if(t)return t;for(var n=e.parentNode;n;){if(t=n[ha]||n[pa]){if(n=t.alternate,null!==t.child||null!==n&&null!==n.child)for(e=ca(e);null!==e;){if(n=e[pa])return n;e=ca(e)}return t}n=(e=n).parentNode}return null}function ba(e){return!(e=e[pa]||e[ha])||5!==e.tag&&6!==e.tag&&13!==e.tag&&3!==e.tag?null:e}function _a(e){if(5===e.tag||6===e.tag)return e.stateNode;throw Error(i(33))}function wa(e){return e[da]||null}var xa=[],ka=-1;function Sa(e){return{current:e}}function Ea(e){0>ka||(e.current=xa[ka],xa[ka]=null,ka--)}function Ca(e,t){ka++,xa[ka]=e.current,e.current=t}var Ta={},Ma=Sa(Ta),Na=Sa(!1),Pa=Ta;function za(e,t){var n=e.type.contextTypes;if(!n)return Ta;var r=e.stateNode;if(r&&r.__reactInternalMemoizedUnmaskedChildContext===t)return r.__reactInternalMemoizedMaskedChildContext;var a,i={};for(a in n)i[a]=t[a];return r&&((e=e.stateNode).__reactInternalMemoizedUnmaskedChildContext=t,e.__reactInternalMemoizedMaskedChildContext=i),i}function La(e){return null!=e.childContextTypes}function Oa(){Ea(Na),Ea(Ma)}function Aa(e,t,n){if(Ma.current!==Ta)throw Error(i(168));Ca(Ma,t),Ca(Na,n)}function Fa(e,t,n){var r=e.stateNode;if(t=t.childContextTypes,\"function\"!=typeof r.getChildContext)return n;for(var a in r=r.getChildContext())if(!(a in t))throw Error(i(108,W(e)||\"Unknown\",a));return R({},n,r)}function Da(e){return e=(e=e.stateNode)&&e.__reactInternalMemoizedMergedChildContext||Ta,Pa=Ma.current,Ca(Ma,e),Ca(Na,Na.current),!0}function Ra(e,t,n){var r=e.stateNode;if(!r)throw Error(i(169));n?(e=Fa(e,t,Pa),r.__reactInternalMemoizedMergedChildContext=e,Ea(Na),Ea(Ma),Ca(Ma,e)):Ea(Na),Ca(Na,n)}var ja=null,Ua=!1,Ia=!1;function $a(e){null===ja?ja=[e]:ja.push(e)}function Ba(){if(!Ia&&null!==ja){Ia=!0;var e=0,t=bt;try{var n=ja;for(bt=1;e<n.length;e++){var r=n[e];do{r=r(!0)}while(null!==r)}ja=null,Ua=!1}catch(t){throw null!==ja&&(ja=ja.slice(e+1)),Qe(Je,Ba),t}finally{bt=t,Ia=!1}}return null}var Wa=[],Va=0,Ha=null,qa=0,Qa=[],Ya=0,Ga=null,Ka=1,Za=\"\";function Xa(e,t){Wa[Va++]=qa,Wa[Va++]=Ha,Ha=e,qa=t}function Ja(e,t,n){Qa[Ya++]=Ka,Qa[Ya++]=Za,Qa[Ya++]=Ga,Ga=e;var r=Ka;e=Za;var a=32-ot(r)-1;r&=~(1<<a),n+=1;var i=32-ot(t)+a;if(30<i){var o=a-a%5;i=(r&(1<<o)-1).toString(32),r>>=o,a-=o,Ka=1<<32-ot(t)+a|n<<a|r,Za=i+e}else Ka=1<<i|n<<a|r,Za=e}function ei(e){null!==e.return&&(Xa(e,1),Ja(e,1,0))}function ti(e){for(;e===Ha;)Ha=Wa[--Va],Wa[Va]=null,qa=Wa[--Va],Wa[Va]=null;for(;e===Ga;)Ga=Qa[--Ya],Qa[Ya]=null,Za=Qa[--Ya],Qa[Ya]=null,Ka=Qa[--Ya],Qa[Ya]=null}var ni=null,ri=null,ai=!1,ii=null;function oi(e,t){var n=Ls(5,null,null,0);n.elementType=\"DELETED\",n.stateNode=t,n.return=e,null===(t=e.deletions)?(e.deletions=[n],e.flags|=16):t.push(n)}function ui(e,t){switch(e.tag){case 5:var n=e.type;return null!==(t=1!==t.nodeType||n.toLowerCase()!==t.nodeName.toLowerCase()?null:t)&&(e.stateNode=t,ni=e,ri=sa(t.firstChild),!0);case 6:return null!==(t=\"\"===e.pendingProps||3!==t.nodeType?null:t)&&(e.stateNode=t,ni=e,ri=null,!0);case 13:return null!==(t=8!==t.nodeType?null:t)&&(n=null!==Ga?{id:Ka,overflow:Za}:null,e.memoizedState={dehydrated:t,treeContext:n,retryLane:1073741824},(n=Ls(18,null,null,0)).stateNode=t,n.return=e,e.child=n,ni=e,ri=null,!0);default:return!1}}function li(e){return 0!=(1&e.mode)&&0==(128&e.flags)}function si(e){if(ai){var t=ri;if(t){var n=t;if(!ui(e,t)){if(li(e))throw Error(i(418));t=sa(n.nextSibling);var r=ni;t&&ui(e,t)?oi(r,n):(e.flags=-4097&e.flags|2,ai=!1,ni=e)}}else{if(li(e))throw Error(i(418));e.flags=-4097&e.flags|2,ai=!1,ni=e}}}function ci(e){for(e=e.return;null!==e&&5!==e.tag&&3!==e.tag&&13!==e.tag;)e=e.return;ni=e}function fi(e){if(e!==ni)return!1;if(!ai)return ci(e),ai=!0,!1;var t;if((t=3!==e.tag)&&!(t=5!==e.tag)&&(t=\"head\"!==(t=e.type)&&\"body\"!==t&&!na(e.type,e.memoizedProps)),t&&(t=ri)){if(li(e))throw pi(),Error(i(418));for(;t;)oi(e,t),t=sa(t.nextSibling)}if(ci(e),13===e.tag){if(!(e=null!==(e=e.memoizedState)?e.dehydrated:null))throw Error(i(317));e:{for(e=e.nextSibling,t=0;e;){if(8===e.nodeType){var n=e.data;if(\"/$\"===n){if(0===t){ri=sa(e.nextSibling);break e}t--}else\"$\"!==n&&\"$!\"!==n&&\"$?\"!==n||t++}e=e.nextSibling}ri=null}}else ri=ni?sa(e.stateNode.nextSibling):null;return!0}function pi(){for(var e=ri;e;)e=sa(e.nextSibling)}function di(){ri=ni=null,ai=!1}function hi(e){null===ii?ii=[e]:ii.push(e)}var vi=_.ReactCurrentBatchConfig;function gi(e,t){if(e&&e.defaultProps){for(var n in t=R({},t),e=e.defaultProps)void 0===t[n]&&(t[n]=e[n]);return t}return t}var yi=Sa(null),mi=null,bi=null,_i=null;function wi(){_i=bi=mi=null}function xi(e){var t=yi.current;Ea(yi),e._currentValue=t}function ki(e,t,n){for(;null!==e;){var r=e.alternate;if((e.childLanes&t)!==t?(e.childLanes|=t,null!==r&&(r.childLanes|=t)):null!==r&&(r.childLanes&t)!==t&&(r.childLanes|=t),e===n)break;e=e.return}}function Si(e,t){mi=e,_i=bi=null,null!==(e=e.dependencies)&&null!==e.firstContext&&(0!=(e.lanes&t)&&(_u=!0),e.firstContext=null)}function Ei(e){var t=e._currentValue;if(_i!==e)if(e={context:e,memoizedValue:t,next:null},null===bi){if(null===mi)throw Error(i(308));bi=e,mi.dependencies={lanes:0,firstContext:e}}else bi=bi.next=e;return t}var Ci=null;function Ti(e){null===Ci?Ci=[e]:Ci.push(e)}function Mi(e,t,n,r){var a=t.interleaved;return null===a?(n.next=n,Ti(t)):(n.next=a.next,a.next=n),t.interleaved=n,Ni(e,r)}function Ni(e,t){e.lanes|=t;var n=e.alternate;for(null!==n&&(n.lanes|=t),n=e,e=e.return;null!==e;)e.childLanes|=t,null!==(n=e.alternate)&&(n.childLanes|=t),n=e,e=e.return;return 3===n.tag?n.stateNode:null}var Pi=!1;function zi(e){e.updateQueue={baseState:e.memoizedState,firstBaseUpdate:null,lastBaseUpdate:null,shared:{pending:null,interleaved:null,lanes:0},effects:null}}function Li(e,t){e=e.updateQueue,t.updateQueue===e&&(t.updateQueue={baseState:e.baseState,firstBaseUpdate:e.firstBaseUpdate,lastBaseUpdate:e.lastBaseUpdate,shared:e.shared,effects:e.effects})}function Oi(e,t){return{eventTime:e,lane:t,tag:0,payload:null,callback:null,next:null}}function Ai(e,t,n){var r=e.updateQueue;if(null===r)return null;if(r=r.shared,0!=(2&Nl)){var a=r.pending;return null===a?t.next=t:(t.next=a.next,a.next=t),r.pending=t,Ni(e,n)}return null===(a=r.interleaved)?(t.next=t,Ti(r)):(t.next=a.next,a.next=t),r.interleaved=t,Ni(e,n)}function Fi(e,t,n){if(null!==(t=t.updateQueue)&&(t=t.shared,0!=(4194240&n))){var r=t.lanes;n|=r&=e.pendingLanes,t.lanes=n,mt(e,n)}}function Di(e,t){var n=e.updateQueue,r=e.alternate;if(null!==r&&n===(r=r.updateQueue)){var a=null,i=null;if(null!==(n=n.firstBaseUpdate)){do{var o={eventTime:n.eventTime,lane:n.lane,tag:n.tag,payload:n.payload,callback:n.callback,next:null};null===i?a=i=o:i=i.next=o,n=n.next}while(null!==n);null===i?a=i=t:i=i.next=t}else a=i=t;return n={baseState:r.baseState,firstBaseUpdate:a,lastBaseUpdate:i,shared:r.shared,effects:r.effects},void(e.updateQueue=n)}null===(e=n.lastBaseUpdate)?n.firstBaseUpdate=t:e.next=t,n.lastBaseUpdate=t}function Ri(e,t,n,r){var a=e.updateQueue;Pi=!1;var i=a.firstBaseUpdate,o=a.lastBaseUpdate,u=a.shared.pending;if(null!==u){a.shared.pending=null;var l=u,s=l.next;l.next=null,null===o?i=s:o.next=s,o=l;var c=e.alternate;null!==c&&(u=(c=c.updateQueue).lastBaseUpdate)!==o&&(null===u?c.firstBaseUpdate=s:u.next=s,c.lastBaseUpdate=l)}if(null!==i){var f=a.baseState;for(o=0,c=s=l=null,u=i;;){var p=u.lane,d=u.eventTime;if((r&p)===p){null!==c&&(c=c.next={eventTime:d,lane:0,tag:u.tag,payload:u.payload,callback:u.callback,next:null});e:{var h=e,v=u;switch(p=t,d=n,v.tag){case 1:if(\"function\"==typeof(h=v.payload)){f=h.call(d,f,p);break e}f=h;break e;case 3:h.flags=-65537&h.flags|128;case 0:if(null==(p=\"function\"==typeof(h=v.payload)?h.call(d,f,p):h))break e;f=R({},f,p);break e;case 2:Pi=!0}}null!==u.callback&&0!==u.lane&&(e.flags|=64,null===(p=a.effects)?a.effects=[u]:p.push(u))}else d={eventTime:d,lane:p,tag:u.tag,payload:u.payload,callback:u.callback,next:null},null===c?(s=c=d,l=f):c=c.next=d,o|=p;if(null===(u=u.next)){if(null===(u=a.shared.pending))break;u=(p=u).next,p.next=null,a.lastBaseUpdate=p,a.shared.pending=null}}if(null===c&&(l=f),a.baseState=l,a.firstBaseUpdate=s,a.lastBaseUpdate=c,null!==(t=a.shared.interleaved)){a=t;do{o|=a.lane,a=a.next}while(a!==t)}else null===i&&(a.shared.lanes=0);Rl|=o,e.lanes=o,e.memoizedState=f}}function ji(e,t,n){if(e=t.effects,t.effects=null,null!==e)for(t=0;t<e.length;t++){var r=e[t],a=r.callback;if(null!==a){if(r.callback=null,r=n,\"function\"!=typeof a)throw Error(i(191,a));a.call(r)}}}var Ui=(new r.Component).refs;function Ii(e,t,n,r){n=null==(n=n(r,t=e.memoizedState))?t:R({},t,n),e.memoizedState=n,0===e.lanes&&(e.updateQueue.baseState=n)}var $i={isMounted:function(e){return!!(e=e._reactInternals)&&Be(e)===e},enqueueSetState:function(e,t,n){e=e._reactInternals;var r=ts(),a=ns(e),i=Oi(r,a);i.payload=t,null!=n&&(i.callback=n),null!==(t=Ai(e,i,a))&&(rs(t,e,a,r),Fi(t,e,a))},enqueueReplaceState:function(e,t,n){e=e._reactInternals;var r=ts(),a=ns(e),i=Oi(r,a);i.tag=1,i.payload=t,null!=n&&(i.callback=n),null!==(t=Ai(e,i,a))&&(rs(t,e,a,r),Fi(t,e,a))},enqueueForceUpdate:function(e,t){e=e._reactInternals;var n=ts(),r=ns(e),a=Oi(n,r);a.tag=2,null!=t&&(a.callback=t),null!==(t=Ai(e,a,r))&&(rs(t,e,r,n),Fi(t,e,r))}};function Bi(e,t,n,r,a,i,o){return\"function\"==typeof(e=e.stateNode).shouldComponentUpdate?e.shouldComponentUpdate(r,i,o):!(t.prototype&&t.prototype.isPureReactComponent&&lr(n,r)&&lr(a,i))}function Wi(e,t,n){var r=!1,a=Ta,i=t.contextType;return\"object\"==typeof i&&null!==i?i=Ei(i):(a=La(t)?Pa:Ma.current,i=(r=null!=(r=t.contextTypes))?za(e,a):Ta),t=new t(n,i),e.memoizedState=null!==t.state&&void 0!==t.state?t.state:null,t.updater=$i,e.stateNode=t,t._reactInternals=e,r&&((e=e.stateNode).__reactInternalMemoizedUnmaskedChildContext=a,e.__reactInternalMemoizedMaskedChildContext=i),t}function Vi(e,t,n,r){e=t.state,\"function\"==typeof t.componentWillReceiveProps&&t.componentWillReceiveProps(n,r),\"function\"==typeof t.UNSAFE_componentWillReceiveProps&&t.UNSAFE_componentWillReceiveProps(n,r),t.state!==e&&$i.enqueueReplaceState(t,t.state,null)}function Hi(e,t,n,r){var a=e.stateNode;a.props=n,a.state=e.memoizedState,a.refs=Ui,zi(e);var i=t.contextType;\"object\"==typeof i&&null!==i?a.context=Ei(i):(i=La(t)?Pa:Ma.current,a.context=za(e,i)),a.state=e.memoizedState,\"function\"==typeof(i=t.getDerivedStateFromProps)&&(Ii(e,t,i,n),a.state=e.memoizedState),\"function\"==typeof t.getDerivedStateFromProps||\"function\"==typeof a.getSnapshotBeforeUpdate||\"function\"!=typeof a.UNSAFE_componentWillMount&&\"function\"!=typeof a.componentWillMount||(t=a.state,\"function\"==typeof a.componentWillMount&&a.componentWillMount(),\"function\"==typeof a.UNSAFE_componentWillMount&&a.UNSAFE_componentWillMount(),t!==a.state&&$i.enqueueReplaceState(a,a.state,null),Ri(e,n,a,r),a.state=e.memoizedState),\"function\"==typeof a.componentDidMount&&(e.flags|=4194308)}function qi(e,t,n){if(null!==(e=n.ref)&&\"function\"!=typeof e&&\"object\"!=typeof e){if(n._owner){if(n=n._owner){if(1!==n.tag)throw Error(i(309));var r=n.stateNode}if(!r)throw Error(i(147,e));var a=r,o=\"\"+e;return null!==t&&null!==t.ref&&\"function\"==typeof t.ref&&t.ref._stringRef===o?t.ref:(t=function(e){var t=a.refs;t===Ui&&(t=a.refs={}),null===e?delete t[o]:t[o]=e},t._stringRef=o,t)}if(\"string\"!=typeof e)throw Error(i(284));if(!n._owner)throw Error(i(290,e))}return e}function Qi(e,t){throw e=Object.prototype.toString.call(t),Error(i(31,\"[object Object]\"===e?\"object with keys {\"+Object.keys(t).join(\", \")+\"}\":e))}function Yi(e){return(0,e._init)(e._payload)}function Gi(e){function t(t,n){if(e){var r=t.deletions;null===r?(t.deletions=[n],t.flags|=16):r.push(n)}}function n(n,r){if(!e)return null;for(;null!==r;)t(n,r),r=r.sibling;return null}function r(e,t){for(e=new Map;null!==t;)null!==t.key?e.set(t.key,t):e.set(t.index,t),t=t.sibling;return e}function a(e,t){return(e=As(e,t)).index=0,e.sibling=null,e}function o(t,n,r){return t.index=r,e?null!==(r=t.alternate)?(r=r.index)<n?(t.flags|=2,n):r:(t.flags|=2,n):(t.flags|=1048576,n)}function u(t){return e&&null===t.alternate&&(t.flags|=2),t}function l(e,t,n,r){return null===t||6!==t.tag?((t=js(n,e.mode,r)).return=e,t):((t=a(t,n)).return=e,t)}function s(e,t,n,r){var i=n.type;return i===k?f(e,t,n.props.children,r,n.key):null!==t&&(t.elementType===i||\"object\"==typeof i&&null!==i&&i.$$typeof===L&&Yi(i)===t.type)?((r=a(t,n.props)).ref=qi(e,t,n),r.return=e,r):((r=Fs(n.type,n.key,n.props,null,e.mode,r)).ref=qi(e,t,n),r.return=e,r)}function c(e,t,n,r){return null===t||4!==t.tag||t.stateNode.containerInfo!==n.containerInfo||t.stateNode.implementation!==n.implementation?((t=Us(n,e.mode,r)).return=e,t):((t=a(t,n.children||[])).return=e,t)}function f(e,t,n,r,i){return null===t||7!==t.tag?((t=Ds(n,e.mode,r,i)).return=e,t):((t=a(t,n)).return=e,t)}function p(e,t,n){if(\"string\"==typeof t&&\"\"!==t||\"number\"==typeof t)return(t=js(\"\"+t,e.mode,n)).return=e,t;if(\"object\"==typeof t&&null!==t){switch(t.$$typeof){case w:return(n=Fs(t.type,t.key,t.props,null,e.mode,n)).ref=qi(e,null,t),n.return=e,n;case x:return(t=Us(t,e.mode,n)).return=e,t;case L:return p(e,(0,t._init)(t._payload),n)}if(te(t)||F(t))return(t=Ds(t,e.mode,n,null)).return=e,t;Qi(e,t)}return null}function d(e,t,n,r){var a=null!==t?t.key:null;if(\"string\"==typeof n&&\"\"!==n||\"number\"==typeof n)return null!==a?null:l(e,t,\"\"+n,r);if(\"object\"==typeof n&&null!==n){switch(n.$$typeof){case w:return n.key===a?s(e,t,n,r):null;case x:return n.key===a?c(e,t,n,r):null;case L:return d(e,t,(a=n._init)(n._payload),r)}if(te(n)||F(n))return null!==a?null:f(e,t,n,r,null);Qi(e,n)}return null}function h(e,t,n,r,a){if(\"string\"==typeof r&&\"\"!==r||\"number\"==typeof r)return l(t,e=e.get(n)||null,\"\"+r,a);if(\"object\"==typeof r&&null!==r){switch(r.$$typeof){case w:return s(t,e=e.get(null===r.key?n:r.key)||null,r,a);case x:return c(t,e=e.get(null===r.key?n:r.key)||null,r,a);case L:return h(e,t,n,(0,r._init)(r._payload),a)}if(te(r)||F(r))return f(t,e=e.get(n)||null,r,a,null);Qi(t,r)}return null}function v(a,i,u,l){for(var s=null,c=null,f=i,v=i=0,g=null;null!==f&&v<u.length;v++){f.index>v?(g=f,f=null):g=f.sibling;var y=d(a,f,u[v],l);if(null===y){null===f&&(f=g);break}e&&f&&null===y.alternate&&t(a,f),i=o(y,i,v),null===c?s=y:c.sibling=y,c=y,f=g}if(v===u.length)return n(a,f),ai&&Xa(a,v),s;if(null===f){for(;v<u.length;v++)null!==(f=p(a,u[v],l))&&(i=o(f,i,v),null===c?s=f:c.sibling=f,c=f);return ai&&Xa(a,v),s}for(f=r(a,f);v<u.length;v++)null!==(g=h(f,a,v,u[v],l))&&(e&&null!==g.alternate&&f.delete(null===g.key?v:g.key),i=o(g,i,v),null===c?s=g:c.sibling=g,c=g);return e&&f.forEach((function(e){return t(a,e)})),ai&&Xa(a,v),s}function g(a,u,l,s){var c=F(l);if(\"function\"!=typeof c)throw Error(i(150));if(null==(l=c.call(l)))throw Error(i(151));for(var f=c=null,v=u,g=u=0,y=null,m=l.next();null!==v&&!m.done;g++,m=l.next()){v.index>g?(y=v,v=null):y=v.sibling;var b=d(a,v,m.value,s);if(null===b){null===v&&(v=y);break}e&&v&&null===b.alternate&&t(a,v),u=o(b,u,g),null===f?c=b:f.sibling=b,f=b,v=y}if(m.done)return n(a,v),ai&&Xa(a,g),c;if(null===v){for(;!m.done;g++,m=l.next())null!==(m=p(a,m.value,s))&&(u=o(m,u,g),null===f?c=m:f.sibling=m,f=m);return ai&&Xa(a,g),c}for(v=r(a,v);!m.done;g++,m=l.next())null!==(m=h(v,a,g,m.value,s))&&(e&&null!==m.alternate&&v.delete(null===m.key?g:m.key),u=o(m,u,g),null===f?c=m:f.sibling=m,f=m);return e&&v.forEach((function(e){return t(a,e)})),ai&&Xa(a,g),c}return function e(r,i,o,l){if(\"object\"==typeof o&&null!==o&&o.type===k&&null===o.key&&(o=o.props.children),\"object\"==typeof o&&null!==o){switch(o.$$typeof){case w:e:{for(var s=o.key,c=i;null!==c;){if(c.key===s){if((s=o.type)===k){if(7===c.tag){n(r,c.sibling),(i=a(c,o.props.children)).return=r,r=i;break e}}else if(c.elementType===s||\"object\"==typeof s&&null!==s&&s.$$typeof===L&&Yi(s)===c.type){n(r,c.sibling),(i=a(c,o.props)).ref=qi(r,c,o),i.return=r,r=i;break e}n(r,c);break}t(r,c),c=c.sibling}o.type===k?((i=Ds(o.props.children,r.mode,l,o.key)).return=r,r=i):((l=Fs(o.type,o.key,o.props,null,r.mode,l)).ref=qi(r,i,o),l.return=r,r=l)}return u(r);case x:e:{for(c=o.key;null!==i;){if(i.key===c){if(4===i.tag&&i.stateNode.containerInfo===o.containerInfo&&i.stateNode.implementation===o.implementation){n(r,i.sibling),(i=a(i,o.children||[])).return=r,r=i;break e}n(r,i);break}t(r,i),i=i.sibling}(i=Us(o,r.mode,l)).return=r,r=i}return u(r);case L:return e(r,i,(c=o._init)(o._payload),l)}if(te(o))return v(r,i,o,l);if(F(o))return g(r,i,o,l);Qi(r,o)}return\"string\"==typeof o&&\"\"!==o||\"number\"==typeof o?(o=\"\"+o,null!==i&&6===i.tag?(n(r,i.sibling),(i=a(i,o)).return=r,r=i):(n(r,i),(i=js(o,r.mode,l)).return=r,r=i),u(r)):n(r,i)}}var Ki=Gi(!0),Zi=Gi(!1),Xi={},Ji=Sa(Xi),eo=Sa(Xi),to=Sa(Xi);function no(e){if(e===Xi)throw Error(i(174));return e}function ro(e,t){switch(Ca(to,t),Ca(eo,e),Ca(Ji,Xi),e=t.nodeType){case 9:case 11:t=(t=t.documentElement)?t.namespaceURI:le(null,\"\");break;default:t=le(t=(e=8===e?t.parentNode:t).namespaceURI||null,e=e.tagName)}Ea(Ji),Ca(Ji,t)}function ao(){Ea(Ji),Ea(eo),Ea(to)}function io(e){no(to.current);var t=no(Ji.current),n=le(t,e.type);t!==n&&(Ca(eo,e),Ca(Ji,n))}function oo(e){eo.current===e&&(Ea(Ji),Ea(eo))}var uo=Sa(0);function lo(e){for(var t=e;null!==t;){if(13===t.tag){var n=t.memoizedState;if(null!==n&&(null===(n=n.dehydrated)||\"$?\"===n.data||\"$!\"===n.data))return t}else if(19===t.tag&&void 0!==t.memoizedProps.revealOrder){if(0!=(128&t.flags))return t}else if(null!==t.child){t.child.return=t,t=t.child;continue}if(t===e)break;for(;null===t.sibling;){if(null===t.return||t.return===e)return null;t=t.return}t.sibling.return=t.return,t=t.sibling}return null}var so=[];function co(){for(var e=0;e<so.length;e++)so[e]._workInProgressVersionPrimary=null;so.length=0}var fo=_.ReactCurrentDispatcher,po=_.ReactCurrentBatchConfig,ho=0,vo=null,go=null,yo=null,mo=!1,bo=!1,_o=0,wo=0;function xo(){throw Error(i(321))}function ko(e,t){if(null===t)return!1;for(var n=0;n<t.length&&n<e.length;n++)if(!ur(e[n],t[n]))return!1;return!0}function So(e,t,n,r,a,o){if(ho=o,vo=t,t.memoizedState=null,t.updateQueue=null,t.lanes=0,fo.current=null===e||null===e.memoizedState?uu:lu,e=n(r,a),bo){o=0;do{if(bo=!1,_o=0,25<=o)throw Error(i(301));o+=1,yo=go=null,t.updateQueue=null,fo.current=su,e=n(r,a)}while(bo)}if(fo.current=ou,t=null!==go&&null!==go.next,ho=0,yo=go=vo=null,mo=!1,t)throw Error(i(300));return e}function Eo(){var e=0!==_o;return _o=0,e}function Co(){var e={memoizedState:null,baseState:null,baseQueue:null,queue:null,next:null};return null===yo?vo.memoizedState=yo=e:yo=yo.next=e,yo}function To(){if(null===go){var e=vo.alternate;e=null!==e?e.memoizedState:null}else e=go.next;var t=null===yo?vo.memoizedState:yo.next;if(null!==t)yo=t,go=e;else{if(null===e)throw Error(i(310));e={memoizedState:(go=e).memoizedState,baseState:go.baseState,baseQueue:go.baseQueue,queue:go.queue,next:null},null===yo?vo.memoizedState=yo=e:yo=yo.next=e}return yo}function Mo(e,t){return\"function\"==typeof t?t(e):t}function No(e){var t=To(),n=t.queue;if(null===n)throw Error(i(311));n.lastRenderedReducer=e;var r=go,a=r.baseQueue,o=n.pending;if(null!==o){if(null!==a){var u=a.next;a.next=o.next,o.next=u}r.baseQueue=a=o,n.pending=null}if(null!==a){o=a.next,r=r.baseState;var l=u=null,s=null,c=o;do{var f=c.lane;if((ho&f)===f)null!==s&&(s=s.next={lane:0,action:c.action,hasEagerState:c.hasEagerState,eagerState:c.eagerState,next:null}),r=c.hasEagerState?c.eagerState:e(r,c.action);else{var p={lane:f,action:c.action,hasEagerState:c.hasEagerState,eagerState:c.eagerState,next:null};null===s?(l=s=p,u=r):s=s.next=p,vo.lanes|=f,Rl|=f}c=c.next}while(null!==c&&c!==o);null===s?u=r:s.next=l,ur(r,t.memoizedState)||(_u=!0),t.memoizedState=r,t.baseState=u,t.baseQueue=s,n.lastRenderedState=r}if(null!==(e=n.interleaved)){a=e;do{o=a.lane,vo.lanes|=o,Rl|=o,a=a.next}while(a!==e)}else null===a&&(n.lanes=0);return[t.memoizedState,n.dispatch]}function Po(e){var t=To(),n=t.queue;if(null===n)throw Error(i(311));n.lastRenderedReducer=e;var r=n.dispatch,a=n.pending,o=t.memoizedState;if(null!==a){n.pending=null;var u=a=a.next;do{o=e(o,u.action),u=u.next}while(u!==a);ur(o,t.memoizedState)||(_u=!0),t.memoizedState=o,null===t.baseQueue&&(t.baseState=o),n.lastRenderedState=o}return[o,r]}function zo(){}function Lo(e,t){var n=vo,r=To(),a=t(),o=!ur(r.memoizedState,a);if(o&&(r.memoizedState=a,_u=!0),r=r.queue,Vo(Fo.bind(null,n,r,e),[e]),r.getSnapshot!==t||o||null!==yo&&1&yo.memoizedState.tag){if(n.flags|=2048,Uo(9,Ao.bind(null,n,r,a,t),void 0,null),null===Pl)throw Error(i(349));0!=(30&ho)||Oo(n,t,a)}return a}function Oo(e,t,n){e.flags|=16384,e={getSnapshot:t,value:n},null===(t=vo.updateQueue)?(t={lastEffect:null,stores:null},vo.updateQueue=t,t.stores=[e]):null===(n=t.stores)?t.stores=[e]:n.push(e)}function Ao(e,t,n,r){t.value=n,t.getSnapshot=r,Do(t)&&Ro(e)}function Fo(e,t,n){return n((function(){Do(t)&&Ro(e)}))}function Do(e){var t=e.getSnapshot;e=e.value;try{var n=t();return!ur(e,n)}catch(e){return!0}}function Ro(e){var t=Ni(e,1);null!==t&&rs(t,e,1,-1)}function jo(e){var t=Co();return\"function\"==typeof e&&(e=e()),t.memoizedState=t.baseState=e,e={pending:null,interleaved:null,lanes:0,dispatch:null,lastRenderedReducer:Mo,lastRenderedState:e},t.queue=e,e=e.dispatch=nu.bind(null,vo,e),[t.memoizedState,e]}function Uo(e,t,n,r){return e={tag:e,create:t,destroy:n,deps:r,next:null},null===(t=vo.updateQueue)?(t={lastEffect:null,stores:null},vo.updateQueue=t,t.lastEffect=e.next=e):null===(n=t.lastEffect)?t.lastEffect=e.next=e:(r=n.next,n.next=e,e.next=r,t.lastEffect=e),e}function Io(){return To().memoizedState}function $o(e,t,n,r){var a=Co();vo.flags|=e,a.memoizedState=Uo(1|t,n,void 0,void 0===r?null:r)}function Bo(e,t,n,r){var a=To();r=void 0===r?null:r;var i=void 0;if(null!==go){var o=go.memoizedState;if(i=o.destroy,null!==r&&ko(r,o.deps))return void(a.memoizedState=Uo(t,n,i,r))}vo.flags|=e,a.memoizedState=Uo(1|t,n,i,r)}function Wo(e,t){return $o(8390656,8,e,t)}function Vo(e,t){return Bo(2048,8,e,t)}function Ho(e,t){return Bo(4,2,e,t)}function qo(e,t){return Bo(4,4,e,t)}function Qo(e,t){return\"function\"==typeof t?(e=e(),t(e),function(){t(null)}):null!=t?(e=e(),t.current=e,function(){t.current=null}):void 0}function Yo(e,t,n){return n=null!=n?n.concat([e]):null,Bo(4,4,Qo.bind(null,t,e),n)}function Go(){}function Ko(e,t){var n=To();t=void 0===t?null:t;var r=n.memoizedState;return null!==r&&null!==t&&ko(t,r[1])?r[0]:(n.memoizedState=[e,t],e)}function Zo(e,t){var n=To();t=void 0===t?null:t;var r=n.memoizedState;return null!==r&&null!==t&&ko(t,r[1])?r[0]:(e=e(),n.memoizedState=[e,t],e)}function Xo(e,t,n){return 0==(21&ho)?(e.baseState&&(e.baseState=!1,_u=!0),e.memoizedState=n):(ur(n,t)||(n=vt(),vo.lanes|=n,Rl|=n,e.baseState=!0),t)}function Jo(e,t){var n=bt;bt=0!==n&&4>n?n:4,e(!0);var r=po.transition;po.transition={};try{e(!1),t()}finally{bt=n,po.transition=r}}function eu(){return To().memoizedState}function tu(e,t,n){var r=ns(e);n={lane:r,action:n,hasEagerState:!1,eagerState:null,next:null},ru(e)?au(t,n):null!==(n=Mi(e,t,n,r))&&(rs(n,e,r,ts()),iu(n,t,r))}function nu(e,t,n){var r=ns(e),a={lane:r,action:n,hasEagerState:!1,eagerState:null,next:null};if(ru(e))au(t,a);else{var i=e.alternate;if(0===e.lanes&&(null===i||0===i.lanes)&&null!==(i=t.lastRenderedReducer))try{var o=t.lastRenderedState,u=i(o,n);if(a.hasEagerState=!0,a.eagerState=u,ur(u,o)){var l=t.interleaved;return null===l?(a.next=a,Ti(t)):(a.next=l.next,l.next=a),void(t.interleaved=a)}}catch(e){}null!==(n=Mi(e,t,a,r))&&(rs(n,e,r,a=ts()),iu(n,t,r))}}function ru(e){var t=e.alternate;return e===vo||null!==t&&t===vo}function au(e,t){bo=mo=!0;var n=e.pending;null===n?t.next=t:(t.next=n.next,n.next=t),e.pending=t}function iu(e,t,n){if(0!=(4194240&n)){var r=t.lanes;n|=r&=e.pendingLanes,t.lanes=n,mt(e,n)}}var ou={readContext:Ei,useCallback:xo,useContext:xo,useEffect:xo,useImperativeHandle:xo,useInsertionEffect:xo,useLayoutEffect:xo,useMemo:xo,useReducer:xo,useRef:xo,useState:xo,useDebugValue:xo,useDeferredValue:xo,useTransition:xo,useMutableSource:xo,useSyncExternalStore:xo,useId:xo,unstable_isNewReconciler:!1},uu={readContext:Ei,useCallback:function(e,t){return Co().memoizedState=[e,void 0===t?null:t],e},useContext:Ei,useEffect:Wo,useImperativeHandle:function(e,t,n){return n=null!=n?n.concat([e]):null,$o(4194308,4,Qo.bind(null,t,e),n)},useLayoutEffect:function(e,t){return $o(4194308,4,e,t)},useInsertionEffect:function(e,t){return $o(4,2,e,t)},useMemo:function(e,t){var n=Co();return t=void 0===t?null:t,e=e(),n.memoizedState=[e,t],e},useReducer:function(e,t,n){var r=Co();return t=void 0!==n?n(t):t,r.memoizedState=r.baseState=t,e={pending:null,interleaved:null,lanes:0,dispatch:null,lastRenderedReducer:e,lastRenderedState:t},r.queue=e,e=e.dispatch=tu.bind(null,vo,e),[r.memoizedState,e]},useRef:function(e){return e={current:e},Co().memoizedState=e},useState:jo,useDebugValue:Go,useDeferredValue:function(e){return Co().memoizedState=e},useTransition:function(){var e=jo(!1),t=e[0];return e=Jo.bind(null,e[1]),Co().memoizedState=e,[t,e]},useMutableSource:function(){},useSyncExternalStore:function(e,t,n){var r=vo,a=Co();if(ai){if(void 0===n)throw Error(i(407));n=n()}else{if(n=t(),null===Pl)throw Error(i(349));0!=(30&ho)||Oo(r,t,n)}a.memoizedState=n;var o={value:n,getSnapshot:t};return a.queue=o,Wo(Fo.bind(null,r,o,e),[e]),r.flags|=2048,Uo(9,Ao.bind(null,r,o,n,t),void 0,null),n},useId:function(){var e=Co(),t=Pl.identifierPrefix;if(ai){var n=Za;t=\":\"+t+\"R\"+(n=(Ka&~(1<<32-ot(Ka)-1)).toString(32)+n),0<(n=_o++)&&(t+=\"H\"+n.toString(32)),t+=\":\"}else t=\":\"+t+\"r\"+(n=wo++).toString(32)+\":\";return e.memoizedState=t},unstable_isNewReconciler:!1},lu={readContext:Ei,useCallback:Ko,useContext:Ei,useEffect:Vo,useImperativeHandle:Yo,useInsertionEffect:Ho,useLayoutEffect:qo,useMemo:Zo,useReducer:No,useRef:Io,useState:function(){return No(Mo)},useDebugValue:Go,useDeferredValue:function(e){return Xo(To(),go.memoizedState,e)},useTransition:function(){return[No(Mo)[0],To().memoizedState]},useMutableSource:zo,useSyncExternalStore:Lo,useId:eu,unstable_isNewReconciler:!1},su={readContext:Ei,useCallback:Ko,useContext:Ei,useEffect:Vo,useImperativeHandle:Yo,useInsertionEffect:Ho,useLayoutEffect:qo,useMemo:Zo,useReducer:Po,useRef:Io,useState:function(){return Po(Mo)},useDebugValue:Go,useDeferredValue:function(e){var t=To();return null===go?t.memoizedState=e:Xo(t,go.memoizedState,e)},useTransition:function(){return[Po(Mo)[0],To().memoizedState]},useMutableSource:zo,useSyncExternalStore:Lo,useId:eu,unstable_isNewReconciler:!1};function cu(e,t){try{var n=\"\",r=t;do{n+=$(r),r=r.return}while(r);var a=n}catch(e){a=\"\\nError generating stack: \"+e.message+\"\\n\"+e.stack}return{value:e,source:t,stack:a,digest:null}}function fu(e,t,n){return{value:e,source:null,stack:null!=n?n:null,digest:null!=t?t:null}}function pu(e,t){try{console.error(t.value)}catch(e){setTimeout((function(){throw e}))}}var du=\"function\"==typeof WeakMap?WeakMap:Map;function hu(e,t,n){(n=Oi(-1,n)).tag=3,n.payload={element:null};var r=t.value;return n.callback=function(){Hl||(Hl=!0,ql=r),pu(0,t)},n}function vu(e,t,n){(n=Oi(-1,n)).tag=3;var r=e.type.getDerivedStateFromError;if(\"function\"==typeof r){var a=t.value;n.payload=function(){return r(a)},n.callback=function(){pu(0,t)}}var i=e.stateNode;return null!==i&&\"function\"==typeof i.componentDidCatch&&(n.callback=function(){pu(0,t),\"function\"!=typeof r&&(null===Ql?Ql=new Set([this]):Ql.add(this));var e=t.stack;this.componentDidCatch(t.value,{componentStack:null!==e?e:\"\"})}),n}function gu(e,t,n){var r=e.pingCache;if(null===r){r=e.pingCache=new du;var a=new Set;r.set(t,a)}else void 0===(a=r.get(t))&&(a=new Set,r.set(t,a));a.has(n)||(a.add(n),e=Cs.bind(null,e,t,n),t.then(e,e))}function yu(e){do{var t;if((t=13===e.tag)&&(t=null===(t=e.memoizedState)||null!==t.dehydrated),t)return e;e=e.return}while(null!==e);return null}function mu(e,t,n,r,a){return 0==(1&e.mode)?(e===t?e.flags|=65536:(e.flags|=128,n.flags|=131072,n.flags&=-52805,1===n.tag&&(null===n.alternate?n.tag=17:((t=Oi(-1,1)).tag=2,Ai(n,t,1))),n.lanes|=1),e):(e.flags|=65536,e.lanes=a,e)}var bu=_.ReactCurrentOwner,_u=!1;function wu(e,t,n,r){t.child=null===e?Zi(t,null,n,r):Ki(t,e.child,n,r)}function xu(e,t,n,r,a){n=n.render;var i=t.ref;return Si(t,a),r=So(e,t,n,r,i,a),n=Eo(),null===e||_u?(ai&&n&&ei(t),t.flags|=1,wu(e,t,r,a),t.child):(t.updateQueue=e.updateQueue,t.flags&=-2053,e.lanes&=~a,Hu(e,t,a))}function ku(e,t,n,r,a){if(null===e){var i=n.type;return\"function\"!=typeof i||Os(i)||void 0!==i.defaultProps||null!==n.compare||void 0!==n.defaultProps?((e=Fs(n.type,null,r,t,t.mode,a)).ref=t.ref,e.return=t,t.child=e):(t.tag=15,t.type=i,Su(e,t,i,r,a))}if(i=e.child,0==(e.lanes&a)){var o=i.memoizedProps;if((n=null!==(n=n.compare)?n:lr)(o,r)&&e.ref===t.ref)return Hu(e,t,a)}return t.flags|=1,(e=As(i,r)).ref=t.ref,e.return=t,t.child=e}function Su(e,t,n,r,a){if(null!==e){var i=e.memoizedProps;if(lr(i,r)&&e.ref===t.ref){if(_u=!1,t.pendingProps=r=i,0==(e.lanes&a))return t.lanes=e.lanes,Hu(e,t,a);0!=(131072&e.flags)&&(_u=!0)}}return Tu(e,t,n,r,a)}function Eu(e,t,n){var r=t.pendingProps,a=r.children,i=null!==e?e.memoizedState:null;if(\"hidden\"===r.mode)if(0==(1&t.mode))t.memoizedState={baseLanes:0,cachePool:null,transitions:null},Ca(Al,Ol),Ol|=n;else{if(0==(1073741824&n))return e=null!==i?i.baseLanes|n:n,t.lanes=t.childLanes=1073741824,t.memoizedState={baseLanes:e,cachePool:null,transitions:null},t.updateQueue=null,Ca(Al,Ol),Ol|=e,null;t.memoizedState={baseLanes:0,cachePool:null,transitions:null},r=null!==i?i.baseLanes:n,Ca(Al,Ol),Ol|=r}else null!==i?(r=i.baseLanes|n,t.memoizedState=null):r=n,Ca(Al,Ol),Ol|=r;return wu(e,t,a,n),t.child}function Cu(e,t){var n=t.ref;(null===e&&null!==n||null!==e&&e.ref!==n)&&(t.flags|=512,t.flags|=2097152)}function Tu(e,t,n,r,a){var i=La(n)?Pa:Ma.current;return i=za(t,i),Si(t,a),n=So(e,t,n,r,i,a),r=Eo(),null===e||_u?(ai&&r&&ei(t),t.flags|=1,wu(e,t,n,a),t.child):(t.updateQueue=e.updateQueue,t.flags&=-2053,e.lanes&=~a,Hu(e,t,a))}function Mu(e,t,n,r,a){if(La(n)){var i=!0;Da(t)}else i=!1;if(Si(t,a),null===t.stateNode)Vu(e,t),Wi(t,n,r),Hi(t,n,r,a),r=!0;else if(null===e){var o=t.stateNode,u=t.memoizedProps;o.props=u;var l=o.context,s=n.contextType;s=\"object\"==typeof s&&null!==s?Ei(s):za(t,s=La(n)?Pa:Ma.current);var c=n.getDerivedStateFromProps,f=\"function\"==typeof c||\"function\"==typeof o.getSnapshotBeforeUpdate;f||\"function\"!=typeof o.UNSAFE_componentWillReceiveProps&&\"function\"!=typeof o.componentWillReceiveProps||(u!==r||l!==s)&&Vi(t,o,r,s),Pi=!1;var p=t.memoizedState;o.state=p,Ri(t,r,o,a),l=t.memoizedState,u!==r||p!==l||Na.current||Pi?(\"function\"==typeof c&&(Ii(t,n,c,r),l=t.memoizedState),(u=Pi||Bi(t,n,u,r,p,l,s))?(f||\"function\"!=typeof o.UNSAFE_componentWillMount&&\"function\"!=typeof o.componentWillMount||(\"function\"==typeof o.componentWillMount&&o.componentWillMount(),\"function\"==typeof o.UNSAFE_componentWillMount&&o.UNSAFE_componentWillMount()),\"function\"==typeof o.componentDidMount&&(t.flags|=4194308)):(\"function\"==typeof o.componentDidMount&&(t.flags|=4194308),t.memoizedProps=r,t.memoizedState=l),o.props=r,o.state=l,o.context=s,r=u):(\"function\"==typeof o.componentDidMount&&(t.flags|=4194308),r=!1)}else{o=t.stateNode,Li(e,t),u=t.memoizedProps,s=t.type===t.elementType?u:gi(t.type,u),o.props=s,f=t.pendingProps,p=o.context,l=\"object\"==typeof(l=n.contextType)&&null!==l?Ei(l):za(t,l=La(n)?Pa:Ma.current);var d=n.getDerivedStateFromProps;(c=\"function\"==typeof d||\"function\"==typeof o.getSnapshotBeforeUpdate)||\"function\"!=typeof o.UNSAFE_componentWillReceiveProps&&\"function\"!=typeof o.componentWillReceiveProps||(u!==f||p!==l)&&Vi(t,o,r,l),Pi=!1,p=t.memoizedState,o.state=p,Ri(t,r,o,a);var h=t.memoizedState;u!==f||p!==h||Na.current||Pi?(\"function\"==typeof d&&(Ii(t,n,d,r),h=t.memoizedState),(s=Pi||Bi(t,n,s,r,p,h,l)||!1)?(c||\"function\"!=typeof o.UNSAFE_componentWillUpdate&&\"function\"!=typeof o.componentWillUpdate||(\"function\"==typeof o.componentWillUpdate&&o.componentWillUpdate(r,h,l),\"function\"==typeof o.UNSAFE_componentWillUpdate&&o.UNSAFE_componentWillUpdate(r,h,l)),\"function\"==typeof o.componentDidUpdate&&(t.flags|=4),\"function\"==typeof o.getSnapshotBeforeUpdate&&(t.flags|=1024)):(\"function\"!=typeof o.componentDidUpdate||u===e.memoizedProps&&p===e.memoizedState||(t.flags|=4),\"function\"!=typeof o.getSnapshotBeforeUpdate||u===e.memoizedProps&&p===e.memoizedState||(t.flags|=1024),t.memoizedProps=r,t.memoizedState=h),o.props=r,o.state=h,o.context=l,r=s):(\"function\"!=typeof o.componentDidUpdate||u===e.memoizedProps&&p===e.memoizedState||(t.flags|=4),\"function\"!=typeof o.getSnapshotBeforeUpdate||u===e.memoizedProps&&p===e.memoizedState||(t.flags|=1024),r=!1)}return Nu(e,t,n,r,i,a)}function Nu(e,t,n,r,a,i){Cu(e,t);var o=0!=(128&t.flags);if(!r&&!o)return a&&Ra(t,n,!1),Hu(e,t,i);r=t.stateNode,bu.current=t;var u=o&&\"function\"!=typeof n.getDerivedStateFromError?null:r.render();return t.flags|=1,null!==e&&o?(t.child=Ki(t,e.child,null,i),t.child=Ki(t,null,u,i)):wu(e,t,u,i),t.memoizedState=r.state,a&&Ra(t,n,!0),t.child}function Pu(e){var t=e.stateNode;t.pendingContext?Aa(0,t.pendingContext,t.pendingContext!==t.context):t.context&&Aa(0,t.context,!1),ro(e,t.containerInfo)}function zu(e,t,n,r,a){return di(),hi(a),t.flags|=256,wu(e,t,n,r),t.child}var Lu,Ou,Au,Fu,Du={dehydrated:null,treeContext:null,retryLane:0};function Ru(e){return{baseLanes:e,cachePool:null,transitions:null}}function ju(e,t,n){var r,a=t.pendingProps,o=uo.current,u=!1,l=0!=(128&t.flags);if((r=l)||(r=(null===e||null!==e.memoizedState)&&0!=(2&o)),r?(u=!0,t.flags&=-129):null!==e&&null===e.memoizedState||(o|=1),Ca(uo,1&o),null===e)return si(t),null!==(e=t.memoizedState)&&null!==(e=e.dehydrated)?(0==(1&t.mode)?t.lanes=1:\"$!\"===e.data?t.lanes=8:t.lanes=1073741824,null):(l=a.children,e=a.fallback,u?(a=t.mode,u=t.child,l={mode:\"hidden\",children:l},0==(1&a)&&null!==u?(u.childLanes=0,u.pendingProps=l):u=Rs(l,a,0,null),e=Ds(e,a,n,null),u.return=t,e.return=t,u.sibling=e,t.child=u,t.child.memoizedState=Ru(n),t.memoizedState=Du,e):Uu(t,l));if(null!==(o=e.memoizedState)&&null!==(r=o.dehydrated))return function(e,t,n,r,a,o,u){if(n)return 256&t.flags?(t.flags&=-257,Iu(e,t,u,r=fu(Error(i(422))))):null!==t.memoizedState?(t.child=e.child,t.flags|=128,null):(o=r.fallback,a=t.mode,r=Rs({mode:\"visible\",children:r.children},a,0,null),(o=Ds(o,a,u,null)).flags|=2,r.return=t,o.return=t,r.sibling=o,t.child=r,0!=(1&t.mode)&&Ki(t,e.child,null,u),t.child.memoizedState=Ru(u),t.memoizedState=Du,o);if(0==(1&t.mode))return Iu(e,t,u,null);if(\"$!\"===a.data){if(r=a.nextSibling&&a.nextSibling.dataset)var l=r.dgst;return r=l,Iu(e,t,u,r=fu(o=Error(i(419)),r,void 0))}if(l=0!=(u&e.childLanes),_u||l){if(null!==(r=Pl)){switch(u&-u){case 4:a=2;break;case 16:a=8;break;case 64:case 128:case 256:case 512:case 1024:case 2048:case 4096:case 8192:case 16384:case 32768:case 65536:case 131072:case 262144:case 524288:case 1048576:case 2097152:case 4194304:case 8388608:case 16777216:case 33554432:case 67108864:a=32;break;case 536870912:a=268435456;break;default:a=0}0!==(a=0!=(a&(r.suspendedLanes|u))?0:a)&&a!==o.retryLane&&(o.retryLane=a,Ni(e,a),rs(r,e,a,-1))}return gs(),Iu(e,t,u,r=fu(Error(i(421))))}return\"$?\"===a.data?(t.flags|=128,t.child=e.child,t=Ms.bind(null,e),a._reactRetry=t,null):(e=o.treeContext,ri=sa(a.nextSibling),ni=t,ai=!0,ii=null,null!==e&&(Qa[Ya++]=Ka,Qa[Ya++]=Za,Qa[Ya++]=Ga,Ka=e.id,Za=e.overflow,Ga=t),(t=Uu(t,r.children)).flags|=4096,t)}(e,t,l,a,r,o,n);if(u){u=a.fallback,l=t.mode,r=(o=e.child).sibling;var s={mode:\"hidden\",children:a.children};return 0==(1&l)&&t.child!==o?((a=t.child).childLanes=0,a.pendingProps=s,t.deletions=null):(a=As(o,s)).subtreeFlags=14680064&o.subtreeFlags,null!==r?u=As(r,u):(u=Ds(u,l,n,null)).flags|=2,u.return=t,a.return=t,a.sibling=u,t.child=a,a=u,u=t.child,l=null===(l=e.child.memoizedState)?Ru(n):{baseLanes:l.baseLanes|n,cachePool:null,transitions:l.transitions},u.memoizedState=l,u.childLanes=e.childLanes&~n,t.memoizedState=Du,a}return e=(u=e.child).sibling,a=As(u,{mode:\"visible\",children:a.children}),0==(1&t.mode)&&(a.lanes=n),a.return=t,a.sibling=null,null!==e&&(null===(n=t.deletions)?(t.deletions=[e],t.flags|=16):n.push(e)),t.child=a,t.memoizedState=null,a}function Uu(e,t){return(t=Rs({mode:\"visible\",children:t},e.mode,0,null)).return=e,e.child=t}function Iu(e,t,n,r){return null!==r&&hi(r),Ki(t,e.child,null,n),(e=Uu(t,t.pendingProps.children)).flags|=2,t.memoizedState=null,e}function $u(e,t,n){e.lanes|=t;var r=e.alternate;null!==r&&(r.lanes|=t),ki(e.return,t,n)}function Bu(e,t,n,r,a){var i=e.memoizedState;null===i?e.memoizedState={isBackwards:t,rendering:null,renderingStartTime:0,last:r,tail:n,tailMode:a}:(i.isBackwards=t,i.rendering=null,i.renderingStartTime=0,i.last=r,i.tail=n,i.tailMode=a)}function Wu(e,t,n){var r=t.pendingProps,a=r.revealOrder,i=r.tail;if(wu(e,t,r.children,n),0!=(2&(r=uo.current)))r=1&r|2,t.flags|=128;else{if(null!==e&&0!=(128&e.flags))e:for(e=t.child;null!==e;){if(13===e.tag)null!==e.memoizedState&&$u(e,n,t);else if(19===e.tag)$u(e,n,t);else if(null!==e.child){e.child.return=e,e=e.child;continue}if(e===t)break e;for(;null===e.sibling;){if(null===e.return||e.return===t)break e;e=e.return}e.sibling.return=e.return,e=e.sibling}r&=1}if(Ca(uo,r),0==(1&t.mode))t.memoizedState=null;else switch(a){case\"forwards\":for(n=t.child,a=null;null!==n;)null!==(e=n.alternate)&&null===lo(e)&&(a=n),n=n.sibling;null===(n=a)?(a=t.child,t.child=null):(a=n.sibling,n.sibling=null),Bu(t,!1,a,n,i);break;case\"backwards\":for(n=null,a=t.child,t.child=null;null!==a;){if(null!==(e=a.alternate)&&null===lo(e)){t.child=a;break}e=a.sibling,a.sibling=n,n=a,a=e}Bu(t,!0,n,null,i);break;case\"together\":Bu(t,!1,null,null,void 0);break;default:t.memoizedState=null}return t.child}function Vu(e,t){0==(1&t.mode)&&null!==e&&(e.alternate=null,t.alternate=null,t.flags|=2)}function Hu(e,t,n){if(null!==e&&(t.dependencies=e.dependencies),Rl|=t.lanes,0==(n&t.childLanes))return null;if(null!==e&&t.child!==e.child)throw Error(i(153));if(null!==t.child){for(n=As(e=t.child,e.pendingProps),t.child=n,n.return=t;null!==e.sibling;)e=e.sibling,(n=n.sibling=As(e,e.pendingProps)).return=t;n.sibling=null}return t.child}function qu(e,t){if(!ai)switch(e.tailMode){case\"hidden\":t=e.tail;for(var n=null;null!==t;)null!==t.alternate&&(n=t),t=t.sibling;null===n?e.tail=null:n.sibling=null;break;case\"collapsed\":n=e.tail;for(var r=null;null!==n;)null!==n.alternate&&(r=n),n=n.sibling;null===r?t||null===e.tail?e.tail=null:e.tail.sibling=null:r.sibling=null}}function Qu(e){var t=null!==e.alternate&&e.alternate.child===e.child,n=0,r=0;if(t)for(var a=e.child;null!==a;)n|=a.lanes|a.childLanes,r|=14680064&a.subtreeFlags,r|=14680064&a.flags,a.return=e,a=a.sibling;else for(a=e.child;null!==a;)n|=a.lanes|a.childLanes,r|=a.subtreeFlags,r|=a.flags,a.return=e,a=a.sibling;return e.subtreeFlags|=r,e.childLanes=n,t}function Yu(e,t,n){var r=t.pendingProps;switch(ti(t),t.tag){case 2:case 16:case 15:case 0:case 11:case 7:case 8:case 12:case 9:case 14:return Qu(t),null;case 1:case 17:return La(t.type)&&Oa(),Qu(t),null;case 3:return r=t.stateNode,ao(),Ea(Na),Ea(Ma),co(),r.pendingContext&&(r.context=r.pendingContext,r.pendingContext=null),null!==e&&null!==e.child||(fi(t)?t.flags|=4:null===e||e.memoizedState.isDehydrated&&0==(256&t.flags)||(t.flags|=1024,null!==ii&&(us(ii),ii=null))),Ou(e,t),Qu(t),null;case 5:oo(t);var a=no(to.current);if(n=t.type,null!==e&&null!=t.stateNode)Au(e,t,n,r,a),e.ref!==t.ref&&(t.flags|=512,t.flags|=2097152);else{if(!r){if(null===t.stateNode)throw Error(i(166));return Qu(t),null}if(e=no(Ji.current),fi(t)){r=t.stateNode,n=t.type;var o=t.memoizedProps;switch(r[pa]=t,r[da]=o,e=0!=(1&t.mode),n){case\"dialog\":Ur(\"cancel\",r),Ur(\"close\",r);break;case\"iframe\":case\"object\":case\"embed\":Ur(\"load\",r);break;case\"video\":case\"audio\":for(a=0;a<Fr.length;a++)Ur(Fr[a],r);break;case\"source\":Ur(\"error\",r);break;case\"img\":case\"image\":case\"link\":Ur(\"error\",r),Ur(\"load\",r);break;case\"details\":Ur(\"toggle\",r);break;case\"input\":K(r,o),Ur(\"invalid\",r);break;case\"select\":r._wrapperState={wasMultiple:!!o.multiple},Ur(\"invalid\",r);break;case\"textarea\":ae(r,o),Ur(\"invalid\",r)}for(var l in me(n,o),a=null,o)if(o.hasOwnProperty(l)){var s=o[l];\"children\"===l?\"string\"==typeof s?r.textContent!==s&&(!0!==o.suppressHydrationWarning&&Xr(r.textContent,s,e),a=[\"children\",s]):\"number\"==typeof s&&r.textContent!==\"\"+s&&(!0!==o.suppressHydrationWarning&&Xr(r.textContent,s,e),a=[\"children\",\"\"+s]):u.hasOwnProperty(l)&&null!=s&&\"onScroll\"===l&&Ur(\"scroll\",r)}switch(n){case\"input\":q(r),J(r,o,!0);break;case\"textarea\":q(r),oe(r);break;case\"select\":case\"option\":break;default:\"function\"==typeof o.onClick&&(r.onclick=Jr)}r=a,t.updateQueue=r,null!==r&&(t.flags|=4)}else{l=9===a.nodeType?a:a.ownerDocument,\"http://www.w3.org/1999/xhtml\"===e&&(e=ue(n)),\"http://www.w3.org/1999/xhtml\"===e?\"script\"===n?((e=l.createElement(\"div\")).innerHTML=\"<script><\\/script>\",e=e.removeChild(e.firstChild)):\"string\"==typeof r.is?e=l.createElement(n,{is:r.is}):(e=l.createElement(n),\"select\"===n&&(l=e,r.multiple?l.multiple=!0:r.size&&(l.size=r.size))):e=l.createElementNS(e,n),e[pa]=t,e[da]=r,Lu(e,t,!1,!1),t.stateNode=e;e:{switch(l=be(n,r),n){case\"dialog\":Ur(\"cancel\",e),Ur(\"close\",e),a=r;break;case\"iframe\":case\"object\":case\"embed\":Ur(\"load\",e),a=r;break;case\"video\":case\"audio\":for(a=0;a<Fr.length;a++)Ur(Fr[a],e);a=r;break;case\"source\":Ur(\"error\",e),a=r;break;case\"img\":case\"image\":case\"link\":Ur(\"error\",e),Ur(\"load\",e),a=r;break;case\"details\":Ur(\"toggle\",e),a=r;break;case\"input\":K(e,r),a=G(e,r),Ur(\"invalid\",e);break;case\"option\":default:a=r;break;case\"select\":e._wrapperState={wasMultiple:!!r.multiple},a=R({},r,{value:void 0}),Ur(\"invalid\",e);break;case\"textarea\":ae(e,r),a=re(e,r),Ur(\"invalid\",e)}for(o in me(n,a),s=a)if(s.hasOwnProperty(o)){var c=s[o];\"style\"===o?ge(e,c):\"dangerouslySetInnerHTML\"===o?null!=(c=c?c.__html:void 0)&&fe(e,c):\"children\"===o?\"string\"==typeof c?(\"textarea\"!==n||\"\"!==c)&&pe(e,c):\"number\"==typeof c&&pe(e,\"\"+c):\"suppressContentEditableWarning\"!==o&&\"suppressHydrationWarning\"!==o&&\"autoFocus\"!==o&&(u.hasOwnProperty(o)?null!=c&&\"onScroll\"===o&&Ur(\"scroll\",e):null!=c&&b(e,o,c,l))}switch(n){case\"input\":q(e),J(e,r,!1);break;case\"textarea\":q(e),oe(e);break;case\"option\":null!=r.value&&e.setAttribute(\"value\",\"\"+V(r.value));break;case\"select\":e.multiple=!!r.multiple,null!=(o=r.value)?ne(e,!!r.multiple,o,!1):null!=r.defaultValue&&ne(e,!!r.multiple,r.defaultValue,!0);break;default:\"function\"==typeof a.onClick&&(e.onclick=Jr)}switch(n){case\"button\":case\"input\":case\"select\":case\"textarea\":r=!!r.autoFocus;break e;case\"img\":r=!0;break e;default:r=!1}}r&&(t.flags|=4)}null!==t.ref&&(t.flags|=512,t.flags|=2097152)}return Qu(t),null;case 6:if(e&&null!=t.stateNode)Fu(e,t,e.memoizedProps,r);else{if(\"string\"!=typeof r&&null===t.stateNode)throw Error(i(166));if(n=no(to.current),no(Ji.current),fi(t)){if(r=t.stateNode,n=t.memoizedProps,r[pa]=t,(o=r.nodeValue!==n)&&null!==(e=ni))switch(e.tag){case 3:Xr(r.nodeValue,n,0!=(1&e.mode));break;case 5:!0!==e.memoizedProps.suppressHydrationWarning&&Xr(r.nodeValue,n,0!=(1&e.mode))}o&&(t.flags|=4)}else(r=(9===n.nodeType?n:n.ownerDocument).createTextNode(r))[pa]=t,t.stateNode=r}return Qu(t),null;case 13:if(Ea(uo),r=t.memoizedState,null===e||null!==e.memoizedState&&null!==e.memoizedState.dehydrated){if(ai&&null!==ri&&0!=(1&t.mode)&&0==(128&t.flags))pi(),di(),t.flags|=98560,o=!1;else if(o=fi(t),null!==r&&null!==r.dehydrated){if(null===e){if(!o)throw Error(i(318));if(!(o=null!==(o=t.memoizedState)?o.dehydrated:null))throw Error(i(317));o[pa]=t}else di(),0==(128&t.flags)&&(t.memoizedState=null),t.flags|=4;Qu(t),o=!1}else null!==ii&&(us(ii),ii=null),o=!0;if(!o)return 65536&t.flags?t:null}return 0!=(128&t.flags)?(t.lanes=n,t):((r=null!==r)!=(null!==e&&null!==e.memoizedState)&&r&&(t.child.flags|=8192,0!=(1&t.mode)&&(null===e||0!=(1&uo.current)?0===Fl&&(Fl=3):gs())),null!==t.updateQueue&&(t.flags|=4),Qu(t),null);case 4:return ao(),Ou(e,t),null===e&&Br(t.stateNode.containerInfo),Qu(t),null;case 10:return xi(t.type._context),Qu(t),null;case 19:if(Ea(uo),null===(o=t.memoizedState))return Qu(t),null;if(r=0!=(128&t.flags),null===(l=o.rendering))if(r)qu(o,!1);else{if(0!==Fl||null!==e&&0!=(128&e.flags))for(e=t.child;null!==e;){if(null!==(l=lo(e))){for(t.flags|=128,qu(o,!1),null!==(r=l.updateQueue)&&(t.updateQueue=r,t.flags|=4),t.subtreeFlags=0,r=n,n=t.child;null!==n;)e=r,(o=n).flags&=14680066,null===(l=o.alternate)?(o.childLanes=0,o.lanes=e,o.child=null,o.subtreeFlags=0,o.memoizedProps=null,o.memoizedState=null,o.updateQueue=null,o.dependencies=null,o.stateNode=null):(o.childLanes=l.childLanes,o.lanes=l.lanes,o.child=l.child,o.subtreeFlags=0,o.deletions=null,o.memoizedProps=l.memoizedProps,o.memoizedState=l.memoizedState,o.updateQueue=l.updateQueue,o.type=l.type,e=l.dependencies,o.dependencies=null===e?null:{lanes:e.lanes,firstContext:e.firstContext}),n=n.sibling;return Ca(uo,1&uo.current|2),t.child}e=e.sibling}null!==o.tail&&Ze()>Wl&&(t.flags|=128,r=!0,qu(o,!1),t.lanes=4194304)}else{if(!r)if(null!==(e=lo(l))){if(t.flags|=128,r=!0,null!==(n=e.updateQueue)&&(t.updateQueue=n,t.flags|=4),qu(o,!0),null===o.tail&&\"hidden\"===o.tailMode&&!l.alternate&&!ai)return Qu(t),null}else 2*Ze()-o.renderingStartTime>Wl&&1073741824!==n&&(t.flags|=128,r=!0,qu(o,!1),t.lanes=4194304);o.isBackwards?(l.sibling=t.child,t.child=l):(null!==(n=o.last)?n.sibling=l:t.child=l,o.last=l)}return null!==o.tail?(t=o.tail,o.rendering=t,o.tail=t.sibling,o.renderingStartTime=Ze(),t.sibling=null,n=uo.current,Ca(uo,r?1&n|2:1&n),t):(Qu(t),null);case 22:case 23:return ps(),r=null!==t.memoizedState,null!==e&&null!==e.memoizedState!==r&&(t.flags|=8192),r&&0!=(1&t.mode)?0!=(1073741824&Ol)&&(Qu(t),6&t.subtreeFlags&&(t.flags|=8192)):Qu(t),null;case 24:case 25:return null}throw Error(i(156,t.tag))}function Gu(e,t){switch(ti(t),t.tag){case 1:return La(t.type)&&Oa(),65536&(e=t.flags)?(t.flags=-65537&e|128,t):null;case 3:return ao(),Ea(Na),Ea(Ma),co(),0!=(65536&(e=t.flags))&&0==(128&e)?(t.flags=-65537&e|128,t):null;case 5:return oo(t),null;case 13:if(Ea(uo),null!==(e=t.memoizedState)&&null!==e.dehydrated){if(null===t.alternate)throw Error(i(340));di()}return 65536&(e=t.flags)?(t.flags=-65537&e|128,t):null;case 19:return Ea(uo),null;case 4:return ao(),null;case 10:return xi(t.type._context),null;case 22:case 23:return ps(),null;default:return null}}Lu=function(e,t){for(var n=t.child;null!==n;){if(5===n.tag||6===n.tag)e.appendChild(n.stateNode);else if(4!==n.tag&&null!==n.child){n.child.return=n,n=n.child;continue}if(n===t)break;for(;null===n.sibling;){if(null===n.return||n.return===t)return;n=n.return}n.sibling.return=n.return,n=n.sibling}},Ou=function(){},Au=function(e,t,n,r){var a=e.memoizedProps;if(a!==r){e=t.stateNode,no(Ji.current);var i,o=null;switch(n){case\"input\":a=G(e,a),r=G(e,r),o=[];break;case\"select\":a=R({},a,{value:void 0}),r=R({},r,{value:void 0}),o=[];break;case\"textarea\":a=re(e,a),r=re(e,r),o=[];break;default:\"function\"!=typeof a.onClick&&\"function\"==typeof r.onClick&&(e.onclick=Jr)}for(c in me(n,r),n=null,a)if(!r.hasOwnProperty(c)&&a.hasOwnProperty(c)&&null!=a[c])if(\"style\"===c){var l=a[c];for(i in l)l.hasOwnProperty(i)&&(n||(n={}),n[i]=\"\")}else\"dangerouslySetInnerHTML\"!==c&&\"children\"!==c&&\"suppressContentEditableWarning\"!==c&&\"suppressHydrationWarning\"!==c&&\"autoFocus\"!==c&&(u.hasOwnProperty(c)?o||(o=[]):(o=o||[]).push(c,null));for(c in r){var s=r[c];if(l=null!=a?a[c]:void 0,r.hasOwnProperty(c)&&s!==l&&(null!=s||null!=l))if(\"style\"===c)if(l){for(i in l)!l.hasOwnProperty(i)||s&&s.hasOwnProperty(i)||(n||(n={}),n[i]=\"\");for(i in s)s.hasOwnProperty(i)&&l[i]!==s[i]&&(n||(n={}),n[i]=s[i])}else n||(o||(o=[]),o.push(c,n)),n=s;else\"dangerouslySetInnerHTML\"===c?(s=s?s.__html:void 0,l=l?l.__html:void 0,null!=s&&l!==s&&(o=o||[]).push(c,s)):\"children\"===c?\"string\"!=typeof s&&\"number\"!=typeof s||(o=o||[]).push(c,\"\"+s):\"suppressContentEditableWarning\"!==c&&\"suppressHydrationWarning\"!==c&&(u.hasOwnProperty(c)?(null!=s&&\"onScroll\"===c&&Ur(\"scroll\",e),o||l===s||(o=[])):(o=o||[]).push(c,s))}n&&(o=o||[]).push(\"style\",n);var c=o;(t.updateQueue=c)&&(t.flags|=4)}},Fu=function(e,t,n,r){n!==r&&(t.flags|=4)};var Ku=!1,Zu=!1,Xu=\"function\"==typeof WeakSet?WeakSet:Set,Ju=null;function el(e,t){var n=e.ref;if(null!==n)if(\"function\"==typeof n)try{n(null)}catch(n){Es(e,t,n)}else n.current=null}function tl(e,t,n){try{n()}catch(n){Es(e,t,n)}}var nl=!1;function rl(e,t,n){var r=t.updateQueue;if(null!==(r=null!==r?r.lastEffect:null)){var a=r=r.next;do{if((a.tag&e)===e){var i=a.destroy;a.destroy=void 0,void 0!==i&&tl(t,n,i)}a=a.next}while(a!==r)}}function al(e,t){if(null!==(t=null!==(t=t.updateQueue)?t.lastEffect:null)){var n=t=t.next;do{if((n.tag&e)===e){var r=n.create;n.destroy=r()}n=n.next}while(n!==t)}}function il(e){var t=e.ref;if(null!==t){var n=e.stateNode;e.tag,e=n,\"function\"==typeof t?t(e):t.current=e}}function ol(e){var t=e.alternate;null!==t&&(e.alternate=null,ol(t)),e.child=null,e.deletions=null,e.sibling=null,5===e.tag&&null!==(t=e.stateNode)&&(delete t[pa],delete t[da],delete t[va],delete t[ga],delete t[ya]),e.stateNode=null,e.return=null,e.dependencies=null,e.memoizedProps=null,e.memoizedState=null,e.pendingProps=null,e.stateNode=null,e.updateQueue=null}function ul(e){return 5===e.tag||3===e.tag||4===e.tag}function ll(e){e:for(;;){for(;null===e.sibling;){if(null===e.return||ul(e.return))return null;e=e.return}for(e.sibling.return=e.return,e=e.sibling;5!==e.tag&&6!==e.tag&&18!==e.tag;){if(2&e.flags)continue e;if(null===e.child||4===e.tag)continue e;e.child.return=e,e=e.child}if(!(2&e.flags))return e.stateNode}}function sl(e,t,n){var r=e.tag;if(5===r||6===r)e=e.stateNode,t?8===n.nodeType?n.parentNode.insertBefore(e,t):n.insertBefore(e,t):(8===n.nodeType?(t=n.parentNode).insertBefore(e,n):(t=n).appendChild(e),null!=(n=n._reactRootContainer)||null!==t.onclick||(t.onclick=Jr));else if(4!==r&&null!==(e=e.child))for(sl(e,t,n),e=e.sibling;null!==e;)sl(e,t,n),e=e.sibling}function cl(e,t,n){var r=e.tag;if(5===r||6===r)e=e.stateNode,t?n.insertBefore(e,t):n.appendChild(e);else if(4!==r&&null!==(e=e.child))for(cl(e,t,n),e=e.sibling;null!==e;)cl(e,t,n),e=e.sibling}var fl=null,pl=!1;function dl(e,t,n){for(n=n.child;null!==n;)hl(e,t,n),n=n.sibling}function hl(e,t,n){if(it&&\"function\"==typeof it.onCommitFiberUnmount)try{it.onCommitFiberUnmount(at,n)}catch(e){}switch(n.tag){case 5:Zu||el(n,t);case 6:var r=fl,a=pl;fl=null,dl(e,t,n),pl=a,null!==(fl=r)&&(pl?(e=fl,n=n.stateNode,8===e.nodeType?e.parentNode.removeChild(n):e.removeChild(n)):fl.removeChild(n.stateNode));break;case 18:null!==fl&&(pl?(e=fl,n=n.stateNode,8===e.nodeType?la(e.parentNode,n):1===e.nodeType&&la(e,n),Bt(e)):la(fl,n.stateNode));break;case 4:r=fl,a=pl,fl=n.stateNode.containerInfo,pl=!0,dl(e,t,n),fl=r,pl=a;break;case 0:case 11:case 14:case 15:if(!Zu&&null!==(r=n.updateQueue)&&null!==(r=r.lastEffect)){a=r=r.next;do{var i=a,o=i.destroy;i=i.tag,void 0!==o&&(0!=(2&i)||0!=(4&i))&&tl(n,t,o),a=a.next}while(a!==r)}dl(e,t,n);break;case 1:if(!Zu&&(el(n,t),\"function\"==typeof(r=n.stateNode).componentWillUnmount))try{r.props=n.memoizedProps,r.state=n.memoizedState,r.componentWillUnmount()}catch(e){Es(n,t,e)}dl(e,t,n);break;case 21:dl(e,t,n);break;case 22:1&n.mode?(Zu=(r=Zu)||null!==n.memoizedState,dl(e,t,n),Zu=r):dl(e,t,n);break;default:dl(e,t,n)}}function vl(e){var t=e.updateQueue;if(null!==t){e.updateQueue=null;var n=e.stateNode;null===n&&(n=e.stateNode=new Xu),t.forEach((function(t){var r=Ns.bind(null,e,t);n.has(t)||(n.add(t),t.then(r,r))}))}}function gl(e,t){var n=t.deletions;if(null!==n)for(var r=0;r<n.length;r++){var a=n[r];try{var o=e,u=t,l=u;e:for(;null!==l;){switch(l.tag){case 5:fl=l.stateNode,pl=!1;break e;case 3:case 4:fl=l.stateNode.containerInfo,pl=!0;break e}l=l.return}if(null===fl)throw Error(i(160));hl(o,u,a),fl=null,pl=!1;var s=a.alternate;null!==s&&(s.return=null),a.return=null}catch(e){Es(a,t,e)}}if(12854&t.subtreeFlags)for(t=t.child;null!==t;)yl(t,e),t=t.sibling}function yl(e,t){var n=e.alternate,r=e.flags;switch(e.tag){case 0:case 11:case 14:case 15:if(gl(t,e),ml(e),4&r){try{rl(3,e,e.return),al(3,e)}catch(t){Es(e,e.return,t)}try{rl(5,e,e.return)}catch(t){Es(e,e.return,t)}}break;case 1:gl(t,e),ml(e),512&r&&null!==n&&el(n,n.return);break;case 5:if(gl(t,e),ml(e),512&r&&null!==n&&el(n,n.return),32&e.flags){var a=e.stateNode;try{pe(a,\"\")}catch(t){Es(e,e.return,t)}}if(4&r&&null!=(a=e.stateNode)){var o=e.memoizedProps,u=null!==n?n.memoizedProps:o,l=e.type,s=e.updateQueue;if(e.updateQueue=null,null!==s)try{\"input\"===l&&\"radio\"===o.type&&null!=o.name&&Z(a,o),be(l,u);var c=be(l,o);for(u=0;u<s.length;u+=2){var f=s[u],p=s[u+1];\"style\"===f?ge(a,p):\"dangerouslySetInnerHTML\"===f?fe(a,p):\"children\"===f?pe(a,p):b(a,f,p,c)}switch(l){case\"input\":X(a,o);break;case\"textarea\":ie(a,o);break;case\"select\":var d=a._wrapperState.wasMultiple;a._wrapperState.wasMultiple=!!o.multiple;var h=o.value;null!=h?ne(a,!!o.multiple,h,!1):d!==!!o.multiple&&(null!=o.defaultValue?ne(a,!!o.multiple,o.defaultValue,!0):ne(a,!!o.multiple,o.multiple?[]:\"\",!1))}a[da]=o}catch(t){Es(e,e.return,t)}}break;case 6:if(gl(t,e),ml(e),4&r){if(null===e.stateNode)throw Error(i(162));a=e.stateNode,o=e.memoizedProps;try{a.nodeValue=o}catch(t){Es(e,e.return,t)}}break;case 3:if(gl(t,e),ml(e),4&r&&null!==n&&n.memoizedState.isDehydrated)try{Bt(t.containerInfo)}catch(t){Es(e,e.return,t)}break;case 4:default:gl(t,e),ml(e);break;case 13:gl(t,e),ml(e),8192&(a=e.child).flags&&(o=null!==a.memoizedState,a.stateNode.isHidden=o,!o||null!==a.alternate&&null!==a.alternate.memoizedState||(Bl=Ze())),4&r&&vl(e);break;case 22:if(f=null!==n&&null!==n.memoizedState,1&e.mode?(Zu=(c=Zu)||f,gl(t,e),Zu=c):gl(t,e),ml(e),8192&r){if(c=null!==e.memoizedState,(e.stateNode.isHidden=c)&&!f&&0!=(1&e.mode))for(Ju=e,f=e.child;null!==f;){for(p=Ju=f;null!==Ju;){switch(h=(d=Ju).child,d.tag){case 0:case 11:case 14:case 15:rl(4,d,d.return);break;case 1:el(d,d.return);var v=d.stateNode;if(\"function\"==typeof v.componentWillUnmount){r=d,n=d.return;try{t=r,v.props=t.memoizedProps,v.state=t.memoizedState,v.componentWillUnmount()}catch(e){Es(r,n,e)}}break;case 5:el(d,d.return);break;case 22:if(null!==d.memoizedState){xl(p);continue}}null!==h?(h.return=d,Ju=h):xl(p)}f=f.sibling}e:for(f=null,p=e;;){if(5===p.tag){if(null===f){f=p;try{a=p.stateNode,c?\"function\"==typeof(o=a.style).setProperty?o.setProperty(\"display\",\"none\",\"important\"):o.display=\"none\":(l=p.stateNode,u=null!=(s=p.memoizedProps.style)&&s.hasOwnProperty(\"display\")?s.display:null,l.style.display=ve(\"display\",u))}catch(t){Es(e,e.return,t)}}}else if(6===p.tag){if(null===f)try{p.stateNode.nodeValue=c?\"\":p.memoizedProps}catch(t){Es(e,e.return,t)}}else if((22!==p.tag&&23!==p.tag||null===p.memoizedState||p===e)&&null!==p.child){p.child.return=p,p=p.child;continue}if(p===e)break e;for(;null===p.sibling;){if(null===p.return||p.return===e)break e;f===p&&(f=null),p=p.return}f===p&&(f=null),p.sibling.return=p.return,p=p.sibling}}break;case 19:gl(t,e),ml(e),4&r&&vl(e);case 21:}}function ml(e){var t=e.flags;if(2&t){try{e:{for(var n=e.return;null!==n;){if(ul(n)){var r=n;break e}n=n.return}throw Error(i(160))}switch(r.tag){case 5:var a=r.stateNode;32&r.flags&&(pe(a,\"\"),r.flags&=-33),cl(e,ll(e),a);break;case 3:case 4:var o=r.stateNode.containerInfo;sl(e,ll(e),o);break;default:throw Error(i(161))}}catch(t){Es(e,e.return,t)}e.flags&=-3}4096&t&&(e.flags&=-4097)}function bl(e,t,n){Ju=e,_l(e,t,n)}function _l(e,t,n){for(var r=0!=(1&e.mode);null!==Ju;){var a=Ju,i=a.child;if(22===a.tag&&r){var o=null!==a.memoizedState||Ku;if(!o){var u=a.alternate,l=null!==u&&null!==u.memoizedState||Zu;u=Ku;var s=Zu;if(Ku=o,(Zu=l)&&!s)for(Ju=a;null!==Ju;)l=(o=Ju).child,22===o.tag&&null!==o.memoizedState?kl(a):null!==l?(l.return=o,Ju=l):kl(a);for(;null!==i;)Ju=i,_l(i,t,n),i=i.sibling;Ju=a,Ku=u,Zu=s}wl(e)}else 0!=(8772&a.subtreeFlags)&&null!==i?(i.return=a,Ju=i):wl(e)}}function wl(e){for(;null!==Ju;){var t=Ju;if(0!=(8772&t.flags)){var n=t.alternate;try{if(0!=(8772&t.flags))switch(t.tag){case 0:case 11:case 15:Zu||al(5,t);break;case 1:var r=t.stateNode;if(4&t.flags&&!Zu)if(null===n)r.componentDidMount();else{var a=t.elementType===t.type?n.memoizedProps:gi(t.type,n.memoizedProps);r.componentDidUpdate(a,n.memoizedState,r.__reactInternalSnapshotBeforeUpdate)}var o=t.updateQueue;null!==o&&ji(t,o,r);break;case 3:var u=t.updateQueue;if(null!==u){if(n=null,null!==t.child)switch(t.child.tag){case 5:case 1:n=t.child.stateNode}ji(t,u,n)}break;case 5:var l=t.stateNode;if(null===n&&4&t.flags){n=l;var s=t.memoizedProps;switch(t.type){case\"button\":case\"input\":case\"select\":case\"textarea\":s.autoFocus&&n.focus();break;case\"img\":s.src&&(n.src=s.src)}}break;case 6:case 4:case 12:case 19:case 17:case 21:case 22:case 23:case 25:break;case 13:if(null===t.memoizedState){var c=t.alternate;if(null!==c){var f=c.memoizedState;if(null!==f){var p=f.dehydrated;null!==p&&Bt(p)}}}break;default:throw Error(i(163))}Zu||512&t.flags&&il(t)}catch(e){Es(t,t.return,e)}}if(t===e){Ju=null;break}if(null!==(n=t.sibling)){n.return=t.return,Ju=n;break}Ju=t.return}}function xl(e){for(;null!==Ju;){var t=Ju;if(t===e){Ju=null;break}var n=t.sibling;if(null!==n){n.return=t.return,Ju=n;break}Ju=t.return}}function kl(e){for(;null!==Ju;){var t=Ju;try{switch(t.tag){case 0:case 11:case 15:var n=t.return;try{al(4,t)}catch(e){Es(t,n,e)}break;case 1:var r=t.stateNode;if(\"function\"==typeof r.componentDidMount){var a=t.return;try{r.componentDidMount()}catch(e){Es(t,a,e)}}var i=t.return;try{il(t)}catch(e){Es(t,i,e)}break;case 5:var o=t.return;try{il(t)}catch(e){Es(t,o,e)}}}catch(e){Es(t,t.return,e)}if(t===e){Ju=null;break}var u=t.sibling;if(null!==u){u.return=t.return,Ju=u;break}Ju=t.return}}var Sl,El=Math.ceil,Cl=_.ReactCurrentDispatcher,Tl=_.ReactCurrentOwner,Ml=_.ReactCurrentBatchConfig,Nl=0,Pl=null,zl=null,Ll=0,Ol=0,Al=Sa(0),Fl=0,Dl=null,Rl=0,jl=0,Ul=0,Il=null,$l=null,Bl=0,Wl=1/0,Vl=null,Hl=!1,ql=null,Ql=null,Yl=!1,Gl=null,Kl=0,Zl=0,Xl=null,Jl=-1,es=0;function ts(){return 0!=(6&Nl)?Ze():-1!==Jl?Jl:Jl=Ze()}function ns(e){return 0==(1&e.mode)?1:0!=(2&Nl)&&0!==Ll?Ll&-Ll:null!==vi.transition?(0===es&&(es=vt()),es):0!==(e=bt)?e:e=void 0===(e=window.event)?16:Kt(e.type)}function rs(e,t,n,r){if(50<Zl)throw Zl=0,Xl=null,Error(i(185));yt(e,n,r),0!=(2&Nl)&&e===Pl||(e===Pl&&(0==(2&Nl)&&(jl|=n),4===Fl&&ls(e,Ll)),as(e,r),1===n&&0===Nl&&0==(1&t.mode)&&(Wl=Ze()+500,Ua&&Ba()))}function as(e,t){var n=e.callbackNode;!function(e,t){for(var n=e.suspendedLanes,r=e.pingedLanes,a=e.expirationTimes,i=e.pendingLanes;0<i;){var o=31-ot(i),u=1<<o,l=a[o];-1===l?0!=(u&n)&&0==(u&r)||(a[o]=dt(u,t)):l<=t&&(e.expiredLanes|=u),i&=~u}}(e,t);var r=pt(e,e===Pl?Ll:0);if(0===r)null!==n&&Ye(n),e.callbackNode=null,e.callbackPriority=0;else if(t=r&-r,e.callbackPriority!==t){if(null!=n&&Ye(n),1===t)0===e.tag?function(e){Ua=!0,$a(e)}(ss.bind(null,e)):$a(ss.bind(null,e)),oa((function(){0==(6&Nl)&&Ba()})),n=null;else{switch(_t(r)){case 1:n=Je;break;case 4:n=et;break;case 16:default:n=tt;break;case 536870912:n=rt}n=Ps(n,is.bind(null,e))}e.callbackPriority=t,e.callbackNode=n}}function is(e,t){if(Jl=-1,es=0,0!=(6&Nl))throw Error(i(327));var n=e.callbackNode;if(ks()&&e.callbackNode!==n)return null;var r=pt(e,e===Pl?Ll:0);if(0===r)return null;if(0!=(30&r)||0!=(r&e.expiredLanes)||t)t=ys(e,r);else{t=r;var a=Nl;Nl|=2;var o=vs();for(Pl===e&&Ll===t||(Vl=null,Wl=Ze()+500,ds(e,t));;)try{bs();break}catch(t){hs(e,t)}wi(),Cl.current=o,Nl=a,null!==zl?t=0:(Pl=null,Ll=0,t=Fl)}if(0!==t){if(2===t&&0!==(a=ht(e))&&(r=a,t=os(e,a)),1===t)throw n=Dl,ds(e,0),ls(e,r),as(e,Ze()),n;if(6===t)ls(e,r);else{if(a=e.current.alternate,0==(30&r)&&!function(e){for(var t=e;;){if(16384&t.flags){var n=t.updateQueue;if(null!==n&&null!==(n=n.stores))for(var r=0;r<n.length;r++){var a=n[r],i=a.getSnapshot;a=a.value;try{if(!ur(i(),a))return!1}catch(e){return!1}}}if(n=t.child,16384&t.subtreeFlags&&null!==n)n.return=t,t=n;else{if(t===e)break;for(;null===t.sibling;){if(null===t.return||t.return===e)return!0;t=t.return}t.sibling.return=t.return,t=t.sibling}}return!0}(a)&&(2===(t=ys(e,r))&&0!==(o=ht(e))&&(r=o,t=os(e,o)),1===t))throw n=Dl,ds(e,0),ls(e,r),as(e,Ze()),n;switch(e.finishedWork=a,e.finishedLanes=r,t){case 0:case 1:throw Error(i(345));case 2:case 5:xs(e,$l,Vl);break;case 3:if(ls(e,r),(130023424&r)===r&&10<(t=Bl+500-Ze())){if(0!==pt(e,0))break;if(((a=e.suspendedLanes)&r)!==r){ts(),e.pingedLanes|=e.suspendedLanes&a;break}e.timeoutHandle=ra(xs.bind(null,e,$l,Vl),t);break}xs(e,$l,Vl);break;case 4:if(ls(e,r),(4194240&r)===r)break;for(t=e.eventTimes,a=-1;0<r;){var u=31-ot(r);o=1<<u,(u=t[u])>a&&(a=u),r&=~o}if(r=a,10<(r=(120>(r=Ze()-r)?120:480>r?480:1080>r?1080:1920>r?1920:3e3>r?3e3:4320>r?4320:1960*El(r/1960))-r)){e.timeoutHandle=ra(xs.bind(null,e,$l,Vl),r);break}xs(e,$l,Vl);break;default:throw Error(i(329))}}}return as(e,Ze()),e.callbackNode===n?is.bind(null,e):null}function os(e,t){var n=Il;return e.current.memoizedState.isDehydrated&&(ds(e,t).flags|=256),2!==(e=ys(e,t))&&(t=$l,$l=n,null!==t&&us(t)),e}function us(e){null===$l?$l=e:$l.push.apply($l,e)}function ls(e,t){for(t&=~Ul,t&=~jl,e.suspendedLanes|=t,e.pingedLanes&=~t,e=e.expirationTimes;0<t;){var n=31-ot(t),r=1<<n;e[n]=-1,t&=~r}}function ss(e){if(0!=(6&Nl))throw Error(i(327));ks();var t=pt(e,0);if(0==(1&t))return as(e,Ze()),null;var n=ys(e,t);if(0!==e.tag&&2===n){var r=ht(e);0!==r&&(t=r,n=os(e,r))}if(1===n)throw n=Dl,ds(e,0),ls(e,t),as(e,Ze()),n;if(6===n)throw Error(i(345));return e.finishedWork=e.current.alternate,e.finishedLanes=t,xs(e,$l,Vl),as(e,Ze()),null}function cs(e,t){var n=Nl;Nl|=1;try{return e(t)}finally{0===(Nl=n)&&(Wl=Ze()+500,Ua&&Ba())}}function fs(e){null!==Gl&&0===Gl.tag&&0==(6&Nl)&&ks();var t=Nl;Nl|=1;var n=Ml.transition,r=bt;try{if(Ml.transition=null,bt=1,e)return e()}finally{bt=r,Ml.transition=n,0==(6&(Nl=t))&&Ba()}}function ps(){Ol=Al.current,Ea(Al)}function ds(e,t){e.finishedWork=null,e.finishedLanes=0;var n=e.timeoutHandle;if(-1!==n&&(e.timeoutHandle=-1,aa(n)),null!==zl)for(n=zl.return;null!==n;){var r=n;switch(ti(r),r.tag){case 1:null!=(r=r.type.childContextTypes)&&Oa();break;case 3:ao(),Ea(Na),Ea(Ma),co();break;case 5:oo(r);break;case 4:ao();break;case 13:case 19:Ea(uo);break;case 10:xi(r.type._context);break;case 22:case 23:ps()}n=n.return}if(Pl=e,zl=e=As(e.current,null),Ll=Ol=t,Fl=0,Dl=null,Ul=jl=Rl=0,$l=Il=null,null!==Ci){for(t=0;t<Ci.length;t++)if(null!==(r=(n=Ci[t]).interleaved)){n.interleaved=null;var a=r.next,i=n.pending;if(null!==i){var o=i.next;i.next=a,r.next=o}n.pending=r}Ci=null}return e}function hs(e,t){for(;;){var n=zl;try{if(wi(),fo.current=ou,mo){for(var r=vo.memoizedState;null!==r;){var a=r.queue;null!==a&&(a.pending=null),r=r.next}mo=!1}if(ho=0,yo=go=vo=null,bo=!1,_o=0,Tl.current=null,null===n||null===n.return){Fl=1,Dl=t,zl=null;break}e:{var o=e,u=n.return,l=n,s=t;if(t=Ll,l.flags|=32768,null!==s&&\"object\"==typeof s&&\"function\"==typeof s.then){var c=s,f=l,p=f.tag;if(0==(1&f.mode)&&(0===p||11===p||15===p)){var d=f.alternate;d?(f.updateQueue=d.updateQueue,f.memoizedState=d.memoizedState,f.lanes=d.lanes):(f.updateQueue=null,f.memoizedState=null)}var h=yu(u);if(null!==h){h.flags&=-257,mu(h,u,l,0,t),1&h.mode&&gu(o,c,t),s=c;var v=(t=h).updateQueue;if(null===v){var g=new Set;g.add(s),t.updateQueue=g}else v.add(s);break e}if(0==(1&t)){gu(o,c,t),gs();break e}s=Error(i(426))}else if(ai&&1&l.mode){var y=yu(u);if(null!==y){0==(65536&y.flags)&&(y.flags|=256),mu(y,u,l,0,t),hi(cu(s,l));break e}}o=s=cu(s,l),4!==Fl&&(Fl=2),null===Il?Il=[o]:Il.push(o),o=u;do{switch(o.tag){case 3:o.flags|=65536,t&=-t,o.lanes|=t,Di(o,hu(0,s,t));break e;case 1:l=s;var m=o.type,b=o.stateNode;if(0==(128&o.flags)&&(\"function\"==typeof m.getDerivedStateFromError||null!==b&&\"function\"==typeof b.componentDidCatch&&(null===Ql||!Ql.has(b)))){o.flags|=65536,t&=-t,o.lanes|=t,Di(o,vu(o,l,t));break e}}o=o.return}while(null!==o)}ws(n)}catch(e){t=e,zl===n&&null!==n&&(zl=n=n.return);continue}break}}function vs(){var e=Cl.current;return Cl.current=ou,null===e?ou:e}function gs(){0!==Fl&&3!==Fl&&2!==Fl||(Fl=4),null===Pl||0==(268435455&Rl)&&0==(268435455&jl)||ls(Pl,Ll)}function ys(e,t){var n=Nl;Nl|=2;var r=vs();for(Pl===e&&Ll===t||(Vl=null,ds(e,t));;)try{ms();break}catch(t){hs(e,t)}if(wi(),Nl=n,Cl.current=r,null!==zl)throw Error(i(261));return Pl=null,Ll=0,Fl}function ms(){for(;null!==zl;)_s(zl)}function bs(){for(;null!==zl&&!Ge();)_s(zl)}function _s(e){var t=Sl(e.alternate,e,Ol);e.memoizedProps=e.pendingProps,null===t?ws(e):zl=t,Tl.current=null}function ws(e){var t=e;do{var n=t.alternate;if(e=t.return,0==(32768&t.flags)){if(null!==(n=Yu(n,t,Ol)))return void(zl=n)}else{if(null!==(n=Gu(n,t)))return n.flags&=32767,void(zl=n);if(null===e)return Fl=6,void(zl=null);e.flags|=32768,e.subtreeFlags=0,e.deletions=null}if(null!==(t=t.sibling))return void(zl=t);zl=t=e}while(null!==t);0===Fl&&(Fl=5)}function xs(e,t,n){var r=bt,a=Ml.transition;try{Ml.transition=null,bt=1,function(e,t,n,r){do{ks()}while(null!==Gl);if(0!=(6&Nl))throw Error(i(327));n=e.finishedWork;var a=e.finishedLanes;if(null===n)return null;if(e.finishedWork=null,e.finishedLanes=0,n===e.current)throw Error(i(177));e.callbackNode=null,e.callbackPriority=0;var o=n.lanes|n.childLanes;if(function(e,t){var n=e.pendingLanes&~t;e.pendingLanes=t,e.suspendedLanes=0,e.pingedLanes=0,e.expiredLanes&=t,e.mutableReadLanes&=t,e.entangledLanes&=t,t=e.entanglements;var r=e.eventTimes;for(e=e.expirationTimes;0<n;){var a=31-ot(n),i=1<<a;t[a]=0,r[a]=-1,e[a]=-1,n&=~i}}(e,o),e===Pl&&(zl=Pl=null,Ll=0),0==(2064&n.subtreeFlags)&&0==(2064&n.flags)||Yl||(Yl=!0,Ps(tt,(function(){return ks(),null}))),o=0!=(15990&n.flags),0!=(15990&n.subtreeFlags)||o){o=Ml.transition,Ml.transition=null;var u=bt;bt=1;var l=Nl;Nl|=4,Tl.current=null,function(e,t){if(ea=Vt,dr(e=pr())){if(\"selectionStart\"in e)var n={start:e.selectionStart,end:e.selectionEnd};else e:{var r=(n=(n=e.ownerDocument)&&n.defaultView||window).getSelection&&n.getSelection();if(r&&0!==r.rangeCount){n=r.anchorNode;var a=r.anchorOffset,o=r.focusNode;r=r.focusOffset;try{n.nodeType,o.nodeType}catch(e){n=null;break e}var u=0,l=-1,s=-1,c=0,f=0,p=e,d=null;t:for(;;){for(var h;p!==n||0!==a&&3!==p.nodeType||(l=u+a),p!==o||0!==r&&3!==p.nodeType||(s=u+r),3===p.nodeType&&(u+=p.nodeValue.length),null!==(h=p.firstChild);)d=p,p=h;for(;;){if(p===e)break t;if(d===n&&++c===a&&(l=u),d===o&&++f===r&&(s=u),null!==(h=p.nextSibling))break;d=(p=d).parentNode}p=h}n=-1===l||-1===s?null:{start:l,end:s}}else n=null}n=n||{start:0,end:0}}else n=null;for(ta={focusedElem:e,selectionRange:n},Vt=!1,Ju=t;null!==Ju;)if(e=(t=Ju).child,0!=(1028&t.subtreeFlags)&&null!==e)e.return=t,Ju=e;else for(;null!==Ju;){t=Ju;try{var v=t.alternate;if(0!=(1024&t.flags))switch(t.tag){case 0:case 11:case 15:case 5:case 6:case 4:case 17:break;case 1:if(null!==v){var g=v.memoizedProps,y=v.memoizedState,m=t.stateNode,b=m.getSnapshotBeforeUpdate(t.elementType===t.type?g:gi(t.type,g),y);m.__reactInternalSnapshotBeforeUpdate=b}break;case 3:var _=t.stateNode.containerInfo;1===_.nodeType?_.textContent=\"\":9===_.nodeType&&_.documentElement&&_.removeChild(_.documentElement);break;default:throw Error(i(163))}}catch(e){Es(t,t.return,e)}if(null!==(e=t.sibling)){e.return=t.return,Ju=e;break}Ju=t.return}v=nl,nl=!1}(e,n),yl(n,e),hr(ta),Vt=!!ea,ta=ea=null,e.current=n,bl(n,e,a),Ke(),Nl=l,bt=u,Ml.transition=o}else e.current=n;if(Yl&&(Yl=!1,Gl=e,Kl=a),0===(o=e.pendingLanes)&&(Ql=null),function(e){if(it&&\"function\"==typeof it.onCommitFiberRoot)try{it.onCommitFiberRoot(at,e,void 0,128==(128&e.current.flags))}catch(e){}}(n.stateNode),as(e,Ze()),null!==t)for(r=e.onRecoverableError,n=0;n<t.length;n++)r((a=t[n]).value,{componentStack:a.stack,digest:a.digest});if(Hl)throw Hl=!1,e=ql,ql=null,e;0!=(1&Kl)&&0!==e.tag&&ks(),0!=(1&(o=e.pendingLanes))?e===Xl?Zl++:(Zl=0,Xl=e):Zl=0,Ba()}(e,t,n,r)}finally{Ml.transition=a,bt=r}return null}function ks(){if(null!==Gl){var e=_t(Kl),t=Ml.transition,n=bt;try{if(Ml.transition=null,bt=16>e?16:e,null===Gl)var r=!1;else{if(e=Gl,Gl=null,Kl=0,0!=(6&Nl))throw Error(i(331));var a=Nl;for(Nl|=4,Ju=e.current;null!==Ju;){var o=Ju,u=o.child;if(0!=(16&Ju.flags)){var l=o.deletions;if(null!==l){for(var s=0;s<l.length;s++){var c=l[s];for(Ju=c;null!==Ju;){var f=Ju;switch(f.tag){case 0:case 11:case 15:rl(8,f,o)}var p=f.child;if(null!==p)p.return=f,Ju=p;else for(;null!==Ju;){var d=(f=Ju).sibling,h=f.return;if(ol(f),f===c){Ju=null;break}if(null!==d){d.return=h,Ju=d;break}Ju=h}}}var v=o.alternate;if(null!==v){var g=v.child;if(null!==g){v.child=null;do{var y=g.sibling;g.sibling=null,g=y}while(null!==g)}}Ju=o}}if(0!=(2064&o.subtreeFlags)&&null!==u)u.return=o,Ju=u;else e:for(;null!==Ju;){if(0!=(2048&(o=Ju).flags))switch(o.tag){case 0:case 11:case 15:rl(9,o,o.return)}var m=o.sibling;if(null!==m){m.return=o.return,Ju=m;break e}Ju=o.return}}var b=e.current;for(Ju=b;null!==Ju;){var _=(u=Ju).child;if(0!=(2064&u.subtreeFlags)&&null!==_)_.return=u,Ju=_;else e:for(u=b;null!==Ju;){if(0!=(2048&(l=Ju).flags))try{switch(l.tag){case 0:case 11:case 15:al(9,l)}}catch(e){Es(l,l.return,e)}if(l===u){Ju=null;break e}var w=l.sibling;if(null!==w){w.return=l.return,Ju=w;break e}Ju=l.return}}if(Nl=a,Ba(),it&&\"function\"==typeof it.onPostCommitFiberRoot)try{it.onPostCommitFiberRoot(at,e)}catch(e){}r=!0}return r}finally{bt=n,Ml.transition=t}}return!1}function Ss(e,t,n){e=Ai(e,t=hu(0,t=cu(n,t),1),1),t=ts(),null!==e&&(yt(e,1,t),as(e,t))}function Es(e,t,n){if(3===e.tag)Ss(e,e,n);else for(;null!==t;){if(3===t.tag){Ss(t,e,n);break}if(1===t.tag){var r=t.stateNode;if(\"function\"==typeof t.type.getDerivedStateFromError||\"function\"==typeof r.componentDidCatch&&(null===Ql||!Ql.has(r))){t=Ai(t,e=vu(t,e=cu(n,e),1),1),e=ts(),null!==t&&(yt(t,1,e),as(t,e));break}}t=t.return}}function Cs(e,t,n){var r=e.pingCache;null!==r&&r.delete(t),t=ts(),e.pingedLanes|=e.suspendedLanes&n,Pl===e&&(Ll&n)===n&&(4===Fl||3===Fl&&(130023424&Ll)===Ll&&500>Ze()-Bl?ds(e,0):Ul|=n),as(e,t)}function Ts(e,t){0===t&&(0==(1&e.mode)?t=1:(t=ct,0==(130023424&(ct<<=1))&&(ct=4194304)));var n=ts();null!==(e=Ni(e,t))&&(yt(e,t,n),as(e,n))}function Ms(e){var t=e.memoizedState,n=0;null!==t&&(n=t.retryLane),Ts(e,n)}function Ns(e,t){var n=0;switch(e.tag){case 13:var r=e.stateNode,a=e.memoizedState;null!==a&&(n=a.retryLane);break;case 19:r=e.stateNode;break;default:throw Error(i(314))}null!==r&&r.delete(t),Ts(e,n)}function Ps(e,t){return Qe(e,t)}function zs(e,t,n,r){this.tag=e,this.key=n,this.sibling=this.child=this.return=this.stateNode=this.type=this.elementType=null,this.index=0,this.ref=null,this.pendingProps=t,this.dependencies=this.memoizedState=this.updateQueue=this.memoizedProps=null,this.mode=r,this.subtreeFlags=this.flags=0,this.deletions=null,this.childLanes=this.lanes=0,this.alternate=null}function Ls(e,t,n,r){return new zs(e,t,n,r)}function Os(e){return!(!(e=e.prototype)||!e.isReactComponent)}function As(e,t){var n=e.alternate;return null===n?((n=Ls(e.tag,t,e.key,e.mode)).elementType=e.elementType,n.type=e.type,n.stateNode=e.stateNode,n.alternate=e,e.alternate=n):(n.pendingProps=t,n.type=e.type,n.flags=0,n.subtreeFlags=0,n.deletions=null),n.flags=14680064&e.flags,n.childLanes=e.childLanes,n.lanes=e.lanes,n.child=e.child,n.memoizedProps=e.memoizedProps,n.memoizedState=e.memoizedState,n.updateQueue=e.updateQueue,t=e.dependencies,n.dependencies=null===t?null:{lanes:t.lanes,firstContext:t.firstContext},n.sibling=e.sibling,n.index=e.index,n.ref=e.ref,n}function Fs(e,t,n,r,a,o){var u=2;if(r=e,\"function\"==typeof e)Os(e)&&(u=1);else if(\"string\"==typeof e)u=5;else e:switch(e){case k:return Ds(n.children,a,o,t);case S:u=8,a|=8;break;case E:return(e=Ls(12,n,t,2|a)).elementType=E,e.lanes=o,e;case N:return(e=Ls(13,n,t,a)).elementType=N,e.lanes=o,e;case P:return(e=Ls(19,n,t,a)).elementType=P,e.lanes=o,e;case O:return Rs(n,a,o,t);default:if(\"object\"==typeof e&&null!==e)switch(e.$$typeof){case C:u=10;break e;case T:u=9;break e;case M:u=11;break e;case z:u=14;break e;case L:u=16,r=null;break e}throw Error(i(130,null==e?e:typeof e,\"\"))}return(t=Ls(u,n,t,a)).elementType=e,t.type=r,t.lanes=o,t}function Ds(e,t,n,r){return(e=Ls(7,e,r,t)).lanes=n,e}function Rs(e,t,n,r){return(e=Ls(22,e,r,t)).elementType=O,e.lanes=n,e.stateNode={isHidden:!1},e}function js(e,t,n){return(e=Ls(6,e,null,t)).lanes=n,e}function Us(e,t,n){return(t=Ls(4,null!==e.children?e.children:[],e.key,t)).lanes=n,t.stateNode={containerInfo:e.containerInfo,pendingChildren:null,implementation:e.implementation},t}function Is(e,t,n,r,a){this.tag=t,this.containerInfo=e,this.finishedWork=this.pingCache=this.current=this.pendingChildren=null,this.timeoutHandle=-1,this.callbackNode=this.pendingContext=this.context=null,this.callbackPriority=0,this.eventTimes=gt(0),this.expirationTimes=gt(-1),this.entangledLanes=this.finishedLanes=this.mutableReadLanes=this.expiredLanes=this.pingedLanes=this.suspendedLanes=this.pendingLanes=0,this.entanglements=gt(0),this.identifierPrefix=r,this.onRecoverableError=a,this.mutableSourceEagerHydrationData=null}function $s(e,t,n,r,a,i,o,u,l){return e=new Is(e,t,n,u,l),1===t?(t=1,!0===i&&(t|=8)):t=0,i=Ls(3,null,null,t),e.current=i,i.stateNode=e,i.memoizedState={element:r,isDehydrated:n,cache:null,transitions:null,pendingSuspenseBoundaries:null},zi(i),e}function Bs(e){if(!e)return Ta;e:{if(Be(e=e._reactInternals)!==e||1!==e.tag)throw Error(i(170));var t=e;do{switch(t.tag){case 3:t=t.stateNode.context;break e;case 1:if(La(t.type)){t=t.stateNode.__reactInternalMemoizedMergedChildContext;break e}}t=t.return}while(null!==t);throw Error(i(171))}if(1===e.tag){var n=e.type;if(La(n))return Fa(e,n,t)}return t}function Ws(e,t,n,r,a,i,o,u,l){return(e=$s(n,r,!0,e,0,i,0,u,l)).context=Bs(null),n=e.current,(i=Oi(r=ts(),a=ns(n))).callback=null!=t?t:null,Ai(n,i,a),e.current.lanes=a,yt(e,a,r),as(e,r),e}function Vs(e,t,n,r){var a=t.current,i=ts(),o=ns(a);return n=Bs(n),null===t.context?t.context=n:t.pendingContext=n,(t=Oi(i,o)).payload={element:e},null!==(r=void 0===r?null:r)&&(t.callback=r),null!==(e=Ai(a,t,o))&&(rs(e,a,o,i),Fi(e,a,o)),o}function Hs(e){return(e=e.current).child?(e.child.tag,e.child.stateNode):null}function qs(e,t){if(null!==(e=e.memoizedState)&&null!==e.dehydrated){var n=e.retryLane;e.retryLane=0!==n&&n<t?n:t}}function Qs(e,t){qs(e,t),(e=e.alternate)&&qs(e,t)}Sl=function(e,t,n){if(null!==e)if(e.memoizedProps!==t.pendingProps||Na.current)_u=!0;else{if(0==(e.lanes&n)&&0==(128&t.flags))return _u=!1,function(e,t,n){switch(t.tag){case 3:Pu(t),di();break;case 5:io(t);break;case 1:La(t.type)&&Da(t);break;case 4:ro(t,t.stateNode.containerInfo);break;case 10:var r=t.type._context,a=t.memoizedProps.value;Ca(yi,r._currentValue),r._currentValue=a;break;case 13:if(null!==(r=t.memoizedState))return null!==r.dehydrated?(Ca(uo,1&uo.current),t.flags|=128,null):0!=(n&t.child.childLanes)?ju(e,t,n):(Ca(uo,1&uo.current),null!==(e=Hu(e,t,n))?e.sibling:null);Ca(uo,1&uo.current);break;case 19:if(r=0!=(n&t.childLanes),0!=(128&e.flags)){if(r)return Wu(e,t,n);t.flags|=128}if(null!==(a=t.memoizedState)&&(a.rendering=null,a.tail=null,a.lastEffect=null),Ca(uo,uo.current),r)break;return null;case 22:case 23:return t.lanes=0,Eu(e,t,n)}return Hu(e,t,n)}(e,t,n);_u=0!=(131072&e.flags)}else _u=!1,ai&&0!=(1048576&t.flags)&&Ja(t,qa,t.index);switch(t.lanes=0,t.tag){case 2:var r=t.type;Vu(e,t),e=t.pendingProps;var a=za(t,Ma.current);Si(t,n),a=So(null,t,r,e,a,n);var o=Eo();return t.flags|=1,\"object\"==typeof a&&null!==a&&\"function\"==typeof a.render&&void 0===a.$$typeof?(t.tag=1,t.memoizedState=null,t.updateQueue=null,La(r)?(o=!0,Da(t)):o=!1,t.memoizedState=null!==a.state&&void 0!==a.state?a.state:null,zi(t),a.updater=$i,t.stateNode=a,a._reactInternals=t,Hi(t,r,e,n),t=Nu(null,t,r,!0,o,n)):(t.tag=0,ai&&o&&ei(t),wu(null,t,a,n),t=t.child),t;case 16:r=t.elementType;e:{switch(Vu(e,t),e=t.pendingProps,r=(a=r._init)(r._payload),t.type=r,a=t.tag=function(e){if(\"function\"==typeof e)return Os(e)?1:0;if(null!=e){if((e=e.$$typeof)===M)return 11;if(e===z)return 14}return 2}(r),e=gi(r,e),a){case 0:t=Tu(null,t,r,e,n);break e;case 1:t=Mu(null,t,r,e,n);break e;case 11:t=xu(null,t,r,e,n);break e;case 14:t=ku(null,t,r,gi(r.type,e),n);break e}throw Error(i(306,r,\"\"))}return t;case 0:return r=t.type,a=t.pendingProps,Tu(e,t,r,a=t.elementType===r?a:gi(r,a),n);case 1:return r=t.type,a=t.pendingProps,Mu(e,t,r,a=t.elementType===r?a:gi(r,a),n);case 3:e:{if(Pu(t),null===e)throw Error(i(387));r=t.pendingProps,a=(o=t.memoizedState).element,Li(e,t),Ri(t,r,null,n);var u=t.memoizedState;if(r=u.element,o.isDehydrated){if(o={element:r,isDehydrated:!1,cache:u.cache,pendingSuspenseBoundaries:u.pendingSuspenseBoundaries,transitions:u.transitions},t.updateQueue.baseState=o,t.memoizedState=o,256&t.flags){t=zu(e,t,r,n,a=cu(Error(i(423)),t));break e}if(r!==a){t=zu(e,t,r,n,a=cu(Error(i(424)),t));break e}for(ri=sa(t.stateNode.containerInfo.firstChild),ni=t,ai=!0,ii=null,n=Zi(t,null,r,n),t.child=n;n;)n.flags=-3&n.flags|4096,n=n.sibling}else{if(di(),r===a){t=Hu(e,t,n);break e}wu(e,t,r,n)}t=t.child}return t;case 5:return io(t),null===e&&si(t),r=t.type,a=t.pendingProps,o=null!==e?e.memoizedProps:null,u=a.children,na(r,a)?u=null:null!==o&&na(r,o)&&(t.flags|=32),Cu(e,t),wu(e,t,u,n),t.child;case 6:return null===e&&si(t),null;case 13:return ju(e,t,n);case 4:return ro(t,t.stateNode.containerInfo),r=t.pendingProps,null===e?t.child=Ki(t,null,r,n):wu(e,t,r,n),t.child;case 11:return r=t.type,a=t.pendingProps,xu(e,t,r,a=t.elementType===r?a:gi(r,a),n);case 7:return wu(e,t,t.pendingProps,n),t.child;case 8:case 12:return wu(e,t,t.pendingProps.children,n),t.child;case 10:e:{if(r=t.type._context,a=t.pendingProps,o=t.memoizedProps,u=a.value,Ca(yi,r._currentValue),r._currentValue=u,null!==o)if(ur(o.value,u)){if(o.children===a.children&&!Na.current){t=Hu(e,t,n);break e}}else for(null!==(o=t.child)&&(o.return=t);null!==o;){var l=o.dependencies;if(null!==l){u=o.child;for(var s=l.firstContext;null!==s;){if(s.context===r){if(1===o.tag){(s=Oi(-1,n&-n)).tag=2;var c=o.updateQueue;if(null!==c){var f=(c=c.shared).pending;null===f?s.next=s:(s.next=f.next,f.next=s),c.pending=s}}o.lanes|=n,null!==(s=o.alternate)&&(s.lanes|=n),ki(o.return,n,t),l.lanes|=n;break}s=s.next}}else if(10===o.tag)u=o.type===t.type?null:o.child;else if(18===o.tag){if(null===(u=o.return))throw Error(i(341));u.lanes|=n,null!==(l=u.alternate)&&(l.lanes|=n),ki(u,n,t),u=o.sibling}else u=o.child;if(null!==u)u.return=o;else for(u=o;null!==u;){if(u===t){u=null;break}if(null!==(o=u.sibling)){o.return=u.return,u=o;break}u=u.return}o=u}wu(e,t,a.children,n),t=t.child}return t;case 9:return a=t.type,r=t.pendingProps.children,Si(t,n),r=r(a=Ei(a)),t.flags|=1,wu(e,t,r,n),t.child;case 14:return a=gi(r=t.type,t.pendingProps),ku(e,t,r,a=gi(r.type,a),n);case 15:return Su(e,t,t.type,t.pendingProps,n);case 17:return r=t.type,a=t.pendingProps,a=t.elementType===r?a:gi(r,a),Vu(e,t),t.tag=1,La(r)?(e=!0,Da(t)):e=!1,Si(t,n),Wi(t,r,a),Hi(t,r,a,n),Nu(null,t,r,!0,e,n);case 19:return Wu(e,t,n);case 22:return Eu(e,t,n)}throw Error(i(156,t.tag))};var Ys=\"function\"==typeof reportError?reportError:function(e){console.error(e)};function Gs(e){this._internalRoot=e}function Ks(e){this._internalRoot=e}function Zs(e){return!(!e||1!==e.nodeType&&9!==e.nodeType&&11!==e.nodeType)}function Xs(e){return!(!e||1!==e.nodeType&&9!==e.nodeType&&11!==e.nodeType&&(8!==e.nodeType||\" react-mount-point-unstable \"!==e.nodeValue))}function Js(){}function ec(e,t,n,r,a){var i=n._reactRootContainer;if(i){var o=i;if(\"function\"==typeof a){var u=a;a=function(){var e=Hs(o);u.call(e)}}Vs(t,o,e,a)}else o=function(e,t,n,r,a){if(a){if(\"function\"==typeof r){var i=r;r=function(){var e=Hs(o);i.call(e)}}var o=Ws(t,r,e,0,null,!1,0,\"\",Js);return e._reactRootContainer=o,e[ha]=o.current,Br(8===e.nodeType?e.parentNode:e),fs(),o}for(;a=e.lastChild;)e.removeChild(a);if(\"function\"==typeof r){var u=r;r=function(){var e=Hs(l);u.call(e)}}var l=$s(e,0,!1,null,0,!1,0,\"\",Js);return e._reactRootContainer=l,e[ha]=l.current,Br(8===e.nodeType?e.parentNode:e),fs((function(){Vs(t,l,n,r)})),l}(n,t,e,a,r);return Hs(o)}Ks.prototype.render=Gs.prototype.render=function(e){var t=this._internalRoot;if(null===t)throw Error(i(409));Vs(e,t,null,null)},Ks.prototype.unmount=Gs.prototype.unmount=function(){var e=this._internalRoot;if(null!==e){this._internalRoot=null;var t=e.containerInfo;fs((function(){Vs(null,e,null,null)})),t[ha]=null}},Ks.prototype.unstable_scheduleHydration=function(e){if(e){var t=St();e={blockedOn:null,target:e,priority:t};for(var n=0;n<Ot.length&&0!==t&&t<Ot[n].priority;n++);Ot.splice(n,0,e),0===n&&Rt(e)}},wt=function(e){switch(e.tag){case 3:var t=e.stateNode;if(t.current.memoizedState.isDehydrated){var n=ft(t.pendingLanes);0!==n&&(mt(t,1|n),as(t,Ze()),0==(6&Nl)&&(Wl=Ze()+500,Ba()))}break;case 13:fs((function(){var t=Ni(e,1);if(null!==t){var n=ts();rs(t,e,1,n)}})),Qs(e,1)}},xt=function(e){if(13===e.tag){var t=Ni(e,134217728);null!==t&&rs(t,e,134217728,ts()),Qs(e,134217728)}},kt=function(e){if(13===e.tag){var t=ns(e),n=Ni(e,t);null!==n&&rs(n,e,t,ts()),Qs(e,t)}},St=function(){return bt},Et=function(e,t){var n=bt;try{return bt=e,t()}finally{bt=n}},xe=function(e,t,n){switch(t){case\"input\":if(X(e,n),t=n.name,\"radio\"===n.type&&null!=t){for(n=e;n.parentNode;)n=n.parentNode;for(n=n.querySelectorAll(\"input[name=\"+JSON.stringify(\"\"+t)+'][type=\"radio\"]'),t=0;t<n.length;t++){var r=n[t];if(r!==e&&r.form===e.form){var a=wa(r);if(!a)throw Error(i(90));Q(r),X(r,a)}}}break;case\"textarea\":ie(e,n);break;case\"select\":null!=(t=n.value)&&ne(e,!!n.multiple,t,!1)}},Me=cs,Ne=fs;var tc={usingClientEntryPoint:!1,Events:[ba,_a,wa,Ce,Te,cs]},nc={findFiberByHostInstance:ma,bundleType:0,version:\"18.2.0\",rendererPackageName:\"react-dom\"},rc={bundleType:nc.bundleType,version:nc.version,rendererPackageName:nc.rendererPackageName,rendererConfig:nc.rendererConfig,overrideHookState:null,overrideHookStateDeletePath:null,overrideHookStateRenamePath:null,overrideProps:null,overridePropsDeletePath:null,overridePropsRenamePath:null,setErrorHandler:null,setSuspenseHandler:null,scheduleUpdate:null,currentDispatcherRef:_.ReactCurrentDispatcher,findHostInstanceByFiber:function(e){return null===(e=He(e))?null:e.stateNode},findFiberByHostInstance:nc.findFiberByHostInstance||function(){return null},findHostInstancesForRefresh:null,scheduleRefresh:null,scheduleRoot:null,setRefreshHandler:null,getCurrentFiber:null,reconcilerVersion:\"18.2.0-next-9e3b772b8-20220608\"};if(\"undefined\"!=typeof __REACT_DEVTOOLS_GLOBAL_HOOK__){var ac=__REACT_DEVTOOLS_GLOBAL_HOOK__;if(!ac.isDisabled&&ac.supportsFiber)try{at=ac.inject(rc),it=ac}catch(ce){}}t.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED=tc,t.createPortal=function(e,t){var n=2<arguments.length&&void 0!==arguments[2]?arguments[2]:null;if(!Zs(t))throw Error(i(200));return function(e,t,n){var r=3<arguments.length&&void 0!==arguments[3]?arguments[3]:null;return{$$typeof:x,key:null==r?null:\"\"+r,children:e,containerInfo:t,implementation:n}}(e,t,null,n)},t.createRoot=function(e,t){if(!Zs(e))throw Error(i(299));var n=!1,r=\"\",a=Ys;return null!=t&&(!0===t.unstable_strictMode&&(n=!0),void 0!==t.identifierPrefix&&(r=t.identifierPrefix),void 0!==t.onRecoverableError&&(a=t.onRecoverableError)),t=$s(e,1,!1,null,0,n,0,r,a),e[ha]=t.current,Br(8===e.nodeType?e.parentNode:e),new Gs(t)},t.findDOMNode=function(e){if(null==e)return null;if(1===e.nodeType)return e;var t=e._reactInternals;if(void 0===t){if(\"function\"==typeof e.render)throw Error(i(188));throw e=Object.keys(e).join(\",\"),Error(i(268,e))}return null===(e=He(t))?null:e.stateNode},t.flushSync=function(e){return fs(e)},t.hydrate=function(e,t,n){if(!Xs(t))throw Error(i(200));return ec(null,e,t,!0,n)},t.hydrateRoot=function(e,t,n){if(!Zs(e))throw Error(i(405));var r=null!=n&&n.hydratedSources||null,a=!1,o=\"\",u=Ys;if(null!=n&&(!0===n.unstable_strictMode&&(a=!0),void 0!==n.identifierPrefix&&(o=n.identifierPrefix),void 0!==n.onRecoverableError&&(u=n.onRecoverableError)),t=Ws(t,null,e,1,null!=n?n:null,a,0,o,u),e[ha]=t.current,Br(e),r)for(e=0;e<r.length;e++)a=(a=(n=r[e])._getVersion)(n._source),null==t.mutableSourceEagerHydrationData?t.mutableSourceEagerHydrationData=[n,a]:t.mutableSourceEagerHydrationData.push(n,a);return new Ks(t)},t.render=function(e,t,n){if(!Xs(t))throw Error(i(200));return ec(null,e,t,!1,n)},t.unmountComponentAtNode=function(e){if(!Xs(e))throw Error(i(40));return!!e._reactRootContainer&&(fs((function(){ec(null,null,e,!1,(function(){e._reactRootContainer=null,e[ha]=null}))})),!0)},t.unstable_batchedUpdates=cs,t.unstable_renderSubtreeIntoContainer=function(e,t,n,r){if(!Xs(n))throw Error(i(200));if(null==e||void 0===e._reactInternals)throw Error(i(38));return ec(e,t,n,!1,r)},t.version=\"18.2.0-next-9e3b772b8-20220608\"},935:(e,t,n)=>{\"use strict\";!function e(){if(\"undefined\"!=typeof __REACT_DEVTOOLS_GLOBAL_HOOK__&&\"function\"==typeof __REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE)try{__REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE(e)}catch(e){console.error(e)}}(),e.exports=n(448)},408:(e,t)=>{\"use strict\";var n=Symbol.for(\"react.element\"),r=Symbol.for(\"react.portal\"),a=Symbol.for(\"react.fragment\"),i=Symbol.for(\"react.strict_mode\"),o=Symbol.for(\"react.profiler\"),u=Symbol.for(\"react.provider\"),l=Symbol.for(\"react.context\"),s=Symbol.for(\"react.forward_ref\"),c=Symbol.for(\"react.suspense\"),f=Symbol.for(\"react.memo\"),p=Symbol.for(\"react.lazy\"),d=Symbol.iterator,h={isMounted:function(){return!1},enqueueForceUpdate:function(){},enqueueReplaceState:function(){},enqueueSetState:function(){}},v=Object.assign,g={};function y(e,t,n){this.props=e,this.context=t,this.refs=g,this.updater=n||h}function m(){}function b(e,t,n){this.props=e,this.context=t,this.refs=g,this.updater=n||h}y.prototype.isReactComponent={},y.prototype.setState=function(e,t){if(\"object\"!=typeof e&&\"function\"!=typeof e&&null!=e)throw Error(\"setState(...): takes an object of state variables to update or a function which returns an object of state variables.\");this.updater.enqueueSetState(this,e,t,\"setState\")},y.prototype.forceUpdate=function(e){this.updater.enqueueForceUpdate(this,e,\"forceUpdate\")},m.prototype=y.prototype;var _=b.prototype=new m;_.constructor=b,v(_,y.prototype),_.isPureReactComponent=!0;var w=Array.isArray,x=Object.prototype.hasOwnProperty,k={current:null},S={key:!0,ref:!0,__self:!0,__source:!0};function E(e,t,r){var a,i={},o=null,u=null;if(null!=t)for(a in void 0!==t.ref&&(u=t.ref),void 0!==t.key&&(o=\"\"+t.key),t)x.call(t,a)&&!S.hasOwnProperty(a)&&(i[a]=t[a]);var l=arguments.length-2;if(1===l)i.children=r;else if(1<l){for(var s=Array(l),c=0;c<l;c++)s[c]=arguments[c+2];i.children=s}if(e&&e.defaultProps)for(a in l=e.defaultProps)void 0===i[a]&&(i[a]=l[a]);return{$$typeof:n,type:e,key:o,ref:u,props:i,_owner:k.current}}function C(e){return\"object\"==typeof e&&null!==e&&e.$$typeof===n}var T=/\\/+/g;function M(e,t){return\"object\"==typeof e&&null!==e&&null!=e.key?function(e){var t={\"=\":\"=0\",\":\":\"=2\"};return\"$\"+e.replace(/[=:]/g,(function(e){return t[e]}))}(\"\"+e.key):t.toString(36)}function N(e,t,a,i,o){var u=typeof e;\"undefined\"!==u&&\"boolean\"!==u||(e=null);var l=!1;if(null===e)l=!0;else switch(u){case\"string\":case\"number\":l=!0;break;case\"object\":switch(e.$$typeof){case n:case r:l=!0}}if(l)return o=o(l=e),e=\"\"===i?\".\"+M(l,0):i,w(o)?(a=\"\",null!=e&&(a=e.replace(T,\"$&/\")+\"/\"),N(o,t,a,\"\",(function(e){return e}))):null!=o&&(C(o)&&(o=function(e,t){return{$$typeof:n,type:e.type,key:t,ref:e.ref,props:e.props,_owner:e._owner}}(o,a+(!o.key||l&&l.key===o.key?\"\":(\"\"+o.key).replace(T,\"$&/\")+\"/\")+e)),t.push(o)),1;if(l=0,i=\"\"===i?\".\":i+\":\",w(e))for(var s=0;s<e.length;s++){var c=i+M(u=e[s],s);l+=N(u,t,a,c,o)}else if(c=function(e){return null===e||\"object\"!=typeof e?null:\"function\"==typeof(e=d&&e[d]||e[\"@@iterator\"])?e:null}(e),\"function\"==typeof c)for(e=c.call(e),s=0;!(u=e.next()).done;)l+=N(u=u.value,t,a,c=i+M(u,s++),o);else if(\"object\"===u)throw t=String(e),Error(\"Objects are not valid as a React child (found: \"+(\"[object Object]\"===t?\"object with keys {\"+Object.keys(e).join(\", \")+\"}\":t)+\"). If you meant to render a collection of children, use an array instead.\");return l}function P(e,t,n){if(null==e)return e;var r=[],a=0;return N(e,r,\"\",\"\",(function(e){return t.call(n,e,a++)})),r}function z(e){if(-1===e._status){var t=e._result;(t=t()).then((function(t){0!==e._status&&-1!==e._status||(e._status=1,e._result=t)}),(function(t){0!==e._status&&-1!==e._status||(e._status=2,e._result=t)})),-1===e._status&&(e._status=0,e._result=t)}if(1===e._status)return e._result.default;throw e._result}var L={current:null},O={transition:null},A={ReactCurrentDispatcher:L,ReactCurrentBatchConfig:O,ReactCurrentOwner:k};t.Children={map:P,forEach:function(e,t,n){P(e,(function(){t.apply(this,arguments)}),n)},count:function(e){var t=0;return P(e,(function(){t++})),t},toArray:function(e){return P(e,(function(e){return e}))||[]},only:function(e){if(!C(e))throw Error(\"React.Children.only expected to receive a single React element child.\");return e}},t.Component=y,t.Fragment=a,t.Profiler=o,t.PureComponent=b,t.StrictMode=i,t.Suspense=c,t.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED=A,t.cloneElement=function(e,t,r){if(null==e)throw Error(\"React.cloneElement(...): The argument must be a React element, but you passed \"+e+\".\");var a=v({},e.props),i=e.key,o=e.ref,u=e._owner;if(null!=t){if(void 0!==t.ref&&(o=t.ref,u=k.current),void 0!==t.key&&(i=\"\"+t.key),e.type&&e.type.defaultProps)var l=e.type.defaultProps;for(s in t)x.call(t,s)&&!S.hasOwnProperty(s)&&(a[s]=void 0===t[s]&&void 0!==l?l[s]:t[s])}var s=arguments.length-2;if(1===s)a.children=r;else if(1<s){l=Array(s);for(var c=0;c<s;c++)l[c]=arguments[c+2];a.children=l}return{$$typeof:n,type:e.type,key:i,ref:o,props:a,_owner:u}},t.createContext=function(e){return(e={$$typeof:l,_currentValue:e,_currentValue2:e,_threadCount:0,Provider:null,Consumer:null,_defaultValue:null,_globalName:null}).Provider={$$typeof:u,_context:e},e.Consumer=e},t.createElement=E,t.createFactory=function(e){var t=E.bind(null,e);return t.type=e,t},t.createRef=function(){return{current:null}},t.forwardRef=function(e){return{$$typeof:s,render:e}},t.isValidElement=C,t.lazy=function(e){return{$$typeof:p,_payload:{_status:-1,_result:e},_init:z}},t.memo=function(e,t){return{$$typeof:f,type:e,compare:void 0===t?null:t}},t.startTransition=function(e){var t=O.transition;O.transition={};try{e()}finally{O.transition=t}},t.unstable_act=function(){throw Error(\"act(...) is not supported in production builds of React.\")},t.useCallback=function(e,t){return L.current.useCallback(e,t)},t.useContext=function(e){return L.current.useContext(e)},t.useDebugValue=function(){},t.useDeferredValue=function(e){return L.current.useDeferredValue(e)},t.useEffect=function(e,t){return L.current.useEffect(e,t)},t.useId=function(){return L.current.useId()},t.useImperativeHandle=function(e,t,n){return L.current.useImperativeHandle(e,t,n)},t.useInsertionEffect=function(e,t){return L.current.useInsertionEffect(e,t)},t.useLayoutEffect=function(e,t){return L.current.useLayoutEffect(e,t)},t.useMemo=function(e,t){return L.current.useMemo(e,t)},t.useReducer=function(e,t,n){return L.current.useReducer(e,t,n)},t.useRef=function(e){return L.current.useRef(e)},t.useState=function(e){return L.current.useState(e)},t.useSyncExternalStore=function(e,t,n){return L.current.useSyncExternalStore(e,t,n)},t.useTransition=function(){return L.current.useTransition()},t.version=\"18.2.0\"},294:(e,t,n)=>{\"use strict\";e.exports=n(408)},53:(e,t)=>{\"use strict\";function n(e,t){var n=e.length;e.push(t);e:for(;0<n;){var r=n-1>>>1,a=e[r];if(!(0<i(a,t)))break e;e[r]=t,e[n]=a,n=r}}function r(e){return 0===e.length?null:e[0]}function a(e){if(0===e.length)return null;var t=e[0],n=e.pop();if(n!==t){e[0]=n;e:for(var r=0,a=e.length,o=a>>>1;r<o;){var u=2*(r+1)-1,l=e[u],s=u+1,c=e[s];if(0>i(l,n))s<a&&0>i(c,l)?(e[r]=c,e[s]=n,r=s):(e[r]=l,e[u]=n,r=u);else{if(!(s<a&&0>i(c,n)))break e;e[r]=c,e[s]=n,r=s}}}return t}function i(e,t){var n=e.sortIndex-t.sortIndex;return 0!==n?n:e.id-t.id}if(\"object\"==typeof performance&&\"function\"==typeof performance.now){var o=performance;t.unstable_now=function(){return o.now()}}else{var u=Date,l=u.now();t.unstable_now=function(){return u.now()-l}}var s=[],c=[],f=1,p=null,d=3,h=!1,v=!1,g=!1,y=\"function\"==typeof setTimeout?setTimeout:null,m=\"function\"==typeof clearTimeout?clearTimeout:null,b=\"undefined\"!=typeof setImmediate?setImmediate:null;function _(e){for(var t=r(c);null!==t;){if(null===t.callback)a(c);else{if(!(t.startTime<=e))break;a(c),t.sortIndex=t.expirationTime,n(s,t)}t=r(c)}}function w(e){if(g=!1,_(e),!v)if(null!==r(s))v=!0,O(x);else{var t=r(c);null!==t&&A(w,t.startTime-e)}}function x(e,n){v=!1,g&&(g=!1,m(C),C=-1),h=!0;var i=d;try{for(_(n),p=r(s);null!==p&&(!(p.expirationTime>n)||e&&!N());){var o=p.callback;if(\"function\"==typeof o){p.callback=null,d=p.priorityLevel;var u=o(p.expirationTime<=n);n=t.unstable_now(),\"function\"==typeof u?p.callback=u:p===r(s)&&a(s),_(n)}else a(s);p=r(s)}if(null!==p)var l=!0;else{var f=r(c);null!==f&&A(w,f.startTime-n),l=!1}return l}finally{p=null,d=i,h=!1}}\"undefined\"!=typeof navigator&&void 0!==navigator.scheduling&&void 0!==navigator.scheduling.isInputPending&&navigator.scheduling.isInputPending.bind(navigator.scheduling);var k,S=!1,E=null,C=-1,T=5,M=-1;function N(){return!(t.unstable_now()-M<T)}function P(){if(null!==E){var e=t.unstable_now();M=e;var n=!0;try{n=E(!0,e)}finally{n?k():(S=!1,E=null)}}else S=!1}if(\"function\"==typeof b)k=function(){b(P)};else if(\"undefined\"!=typeof MessageChannel){var z=new MessageChannel,L=z.port2;z.port1.onmessage=P,k=function(){L.postMessage(null)}}else k=function(){y(P,0)};function O(e){E=e,S||(S=!0,k())}function A(e,n){C=y((function(){e(t.unstable_now())}),n)}t.unstable_IdlePriority=5,t.unstable_ImmediatePriority=1,t.unstable_LowPriority=4,t.unstable_NormalPriority=3,t.unstable_Profiling=null,t.unstable_UserBlockingPriority=2,t.unstable_cancelCallback=function(e){e.callback=null},t.unstable_continueExecution=function(){v||h||(v=!0,O(x))},t.unstable_forceFrameRate=function(e){0>e||125<e?console.error(\"forceFrameRate takes a positive int between 0 and 125, forcing frame rates higher than 125 fps is not supported\"):T=0<e?Math.floor(1e3/e):5},t.unstable_getCurrentPriorityLevel=function(){return d},t.unstable_getFirstCallbackNode=function(){return r(s)},t.unstable_next=function(e){switch(d){case 1:case 2:case 3:var t=3;break;default:t=d}var n=d;d=t;try{return e()}finally{d=n}},t.unstable_pauseExecution=function(){},t.unstable_requestPaint=function(){},t.unstable_runWithPriority=function(e,t){switch(e){case 1:case 2:case 3:case 4:case 5:break;default:e=3}var n=d;d=e;try{return t()}finally{d=n}},t.unstable_scheduleCallback=function(e,a,i){var o=t.unstable_now();switch(i=\"object\"==typeof i&&null!==i&&\"number\"==typeof(i=i.delay)&&0<i?o+i:o,e){case 1:var u=-1;break;case 2:u=250;break;case 5:u=1073741823;break;case 4:u=1e4;break;default:u=5e3}return e={id:f++,callback:a,priorityLevel:e,startTime:i,expirationTime:u=i+u,sortIndex:-1},i>o?(e.sortIndex=i,n(c,e),null===r(s)&&e===r(c)&&(g?(m(C),C=-1):g=!0,A(w,i-o))):(e.sortIndex=u,n(s,e),v||h||(v=!0,O(x))),e},t.unstable_shouldYield=N,t.unstable_wrapCallback=function(e){var t=d;return function(){var n=d;d=t;try{return e.apply(this,arguments)}finally{d=n}}}},840:(e,t,n)=>{\"use strict\";e.exports=n(53)}},t={};function n(r){var a=t[r];if(void 0!==a)return a.exports;var i=t[r]={id:r,loaded:!1,exports:{}};return e[r].call(i.exports,i,i.exports,n),i.loaded=!0,i.exports}n.g=function(){if(\"object\"==typeof globalThis)return globalThis;try{return this||new Function(\"return this\")()}catch(e){if(\"object\"==typeof window)return window}}(),n.nmd=e=>(e.paths=[],e.children||(e.children=[]),e),(()=>{\"use strict\";var e=n(294),t=n(935);const r=Math.sqrt(50),a=Math.sqrt(10),i=Math.sqrt(2);function o(e,t,n){const u=(t-e)/Math.max(0,n),l=Math.floor(Math.log10(u)),s=u/Math.pow(10,l),c=s>=r?10:s>=a?5:s>=i?2:1;let f,p,d;return l<0?(d=Math.pow(10,-l)/c,f=Math.round(e*d),p=Math.round(t*d),f/d<e&&++f,p/d>t&&--p,d=-d):(d=Math.pow(10,l)*c,f=Math.round(e/d),p=Math.round(t/d),f*d<e&&++f,p*d>t&&--p),p<f&&.5<=n&&n<2?o(e,t,2*n):[f,p,d]}function u(e,t,n){return o(e=+e,t=+t,n=+n)[2]}function l(e,t,n){n=+n;const r=(t=+t)<(e=+e),a=r?u(t,e,n):u(e,t,n);return(r?-1:1)*(a<0?1/-a:a)}function s(e,t){return null==e||null==t?NaN:e<t?-1:e>t?1:e>=t?0:NaN}function c(e,t){return null==e||null==t?NaN:t<e?-1:t>e?1:t>=e?0:NaN}function f(e){let t,n,r;function a(e,r,a=0,i=e.length){if(a<i){if(0!==t(r,r))return i;do{const t=a+i>>>1;n(e[t],r)<0?a=t+1:i=t}while(a<i)}return a}return 2!==e.length?(t=s,n=(t,n)=>s(e(t),n),r=(t,n)=>e(t)-n):(t=e===s||e===c?e:p,n=e,r=e),{left:a,center:function(e,t,n=0,i=e.length){const o=a(e,t,n,i-1);return o>n&&r(e[o-1],t)>-r(e[o],t)?o-1:o},right:function(e,r,a=0,i=e.length){if(a<i){if(0!==t(r,r))return i;do{const t=a+i>>>1;n(e[t],r)<=0?a=t+1:i=t}while(a<i)}return a}}}function p(){return 0}const d=f(s),h=d.right,v=(d.left,f((function(e){return null===e?NaN:+e})).center,h);function g(e,t,n){e.prototype=t.prototype=n,n.constructor=e}function y(e,t){var n=Object.create(e.prototype);for(var r in t)n[r]=t[r];return n}function m(){}var b=.7,_=1/b,w=\"\\\\s*([+-]?\\\\d+)\\\\s*\",x=\"\\\\s*([+-]?(?:\\\\d*\\\\.)?\\\\d+(?:[eE][+-]?\\\\d+)?)\\\\s*\",k=\"\\\\s*([+-]?(?:\\\\d*\\\\.)?\\\\d+(?:[eE][+-]?\\\\d+)?)%\\\\s*\",S=/^#([0-9a-f]{3,8})$/,E=new RegExp(`^rgb\\\\(${w},${w},${w}\\\\)$`),C=new RegExp(`^rgb\\\\(${k},${k},${k}\\\\)$`),T=new RegExp(`^rgba\\\\(${w},${w},${w},${x}\\\\)$`),M=new RegExp(`^rgba\\\\(${k},${k},${k},${x}\\\\)$`),N=new RegExp(`^hsl\\\\(${x},${k},${k}\\\\)$`),P=new RegExp(`^hsla\\\\(${x},${k},${k},${x}\\\\)$`),z={aliceblue:15792383,antiquewhite:16444375,aqua:65535,aquamarine:8388564,azure:15794175,beige:16119260,bisque:16770244,black:0,blanchedalmond:16772045,blue:255,blueviolet:9055202,brown:10824234,burlywood:14596231,cadetblue:6266528,chartreuse:8388352,chocolate:13789470,coral:16744272,cornflowerblue:6591981,cornsilk:16775388,crimson:14423100,cyan:65535,darkblue:139,darkcyan:35723,darkgoldenrod:12092939,darkgray:11119017,darkgreen:25600,darkgrey:11119017,darkkhaki:12433259,darkmagenta:9109643,darkolivegreen:5597999,darkorange:16747520,darkorchid:10040012,darkred:9109504,darksalmon:15308410,darkseagreen:9419919,darkslateblue:4734347,darkslategray:3100495,darkslategrey:3100495,darkturquoise:52945,darkviolet:9699539,deeppink:16716947,deepskyblue:49151,dimgray:6908265,dimgrey:6908265,dodgerblue:2003199,firebrick:11674146,floralwhite:16775920,forestgreen:2263842,fuchsia:16711935,gainsboro:14474460,ghostwhite:16316671,gold:16766720,goldenrod:14329120,gray:8421504,green:32768,greenyellow:11403055,grey:8421504,honeydew:15794160,hotpink:16738740,indianred:13458524,indigo:4915330,ivory:16777200,khaki:15787660,lavender:15132410,lavenderblush:16773365,lawngreen:8190976,lemonchiffon:16775885,lightblue:11393254,lightcoral:15761536,lightcyan:14745599,lightgoldenrodyellow:16448210,lightgray:13882323,lightgreen:9498256,lightgrey:13882323,lightpink:16758465,lightsalmon:16752762,lightseagreen:2142890,lightskyblue:8900346,lightslategray:7833753,lightslategrey:7833753,lightsteelblue:11584734,lightyellow:16777184,lime:65280,limegreen:3329330,linen:16445670,magenta:16711935,maroon:8388608,mediumaquamarine:6737322,mediumblue:205,mediumorchid:12211667,mediumpurple:9662683,mediumseagreen:3978097,mediumslateblue:8087790,mediumspringgreen:64154,mediumturquoise:4772300,mediumvioletred:13047173,midnightblue:1644912,mintcream:16121850,mistyrose:16770273,moccasin:16770229,navajowhite:16768685,navy:128,oldlace:16643558,olive:8421376,olivedrab:7048739,orange:16753920,orangered:16729344,orchid:14315734,palegoldenrod:15657130,palegreen:10025880,paleturquoise:11529966,palevioletred:14381203,papayawhip:16773077,peachpuff:16767673,peru:13468991,pink:16761035,plum:14524637,powderblue:11591910,purple:8388736,rebeccapurple:6697881,red:16711680,rosybrown:12357519,royalblue:4286945,saddlebrown:9127187,salmon:16416882,sandybrown:16032864,seagreen:3050327,seashell:16774638,sienna:10506797,silver:12632256,skyblue:8900331,slateblue:6970061,slategray:7372944,slategrey:7372944,snow:16775930,springgreen:65407,steelblue:4620980,tan:13808780,teal:32896,thistle:14204888,tomato:16737095,turquoise:4251856,violet:15631086,wheat:16113331,white:16777215,whitesmoke:16119285,yellow:16776960,yellowgreen:10145074};function L(){return this.rgb().formatHex()}function O(){return this.rgb().formatRgb()}function A(e){var t,n;return e=(e+\"\").trim().toLowerCase(),(t=S.exec(e))?(n=t[1].length,t=parseInt(t[1],16),6===n?F(t):3===n?new j(t>>8&15|t>>4&240,t>>4&15|240&t,(15&t)<<4|15&t,1):8===n?D(t>>24&255,t>>16&255,t>>8&255,(255&t)/255):4===n?D(t>>12&15|t>>8&240,t>>8&15|t>>4&240,t>>4&15|240&t,((15&t)<<4|15&t)/255):null):(t=E.exec(e))?new j(t[1],t[2],t[3],1):(t=C.exec(e))?new j(255*t[1]/100,255*t[2]/100,255*t[3]/100,1):(t=T.exec(e))?D(t[1],t[2],t[3],t[4]):(t=M.exec(e))?D(255*t[1]/100,255*t[2]/100,255*t[3]/100,t[4]):(t=N.exec(e))?V(t[1],t[2]/100,t[3]/100,1):(t=P.exec(e))?V(t[1],t[2]/100,t[3]/100,t[4]):z.hasOwnProperty(e)?F(z[e]):\"transparent\"===e?new j(NaN,NaN,NaN,0):null}function F(e){return new j(e>>16&255,e>>8&255,255&e,1)}function D(e,t,n,r){return r<=0&&(e=t=n=NaN),new j(e,t,n,r)}function R(e,t,n,r){return 1===arguments.length?((a=e)instanceof m||(a=A(a)),a?new j((a=a.rgb()).r,a.g,a.b,a.opacity):new j):new j(e,t,n,null==r?1:r);var a}function j(e,t,n,r){this.r=+e,this.g=+t,this.b=+n,this.opacity=+r}function U(){return`#${W(this.r)}${W(this.g)}${W(this.b)}`}function I(){const e=$(this.opacity);return`${1===e?\"rgb(\":\"rgba(\"}${B(this.r)}, ${B(this.g)}, ${B(this.b)}${1===e?\")\":`, ${e})`}`}function $(e){return isNaN(e)?1:Math.max(0,Math.min(1,e))}function B(e){return Math.max(0,Math.min(255,Math.round(e)||0))}function W(e){return((e=B(e))<16?\"0\":\"\")+e.toString(16)}function V(e,t,n,r){return r<=0?e=t=n=NaN:n<=0||n>=1?e=t=NaN:t<=0&&(e=NaN),new Q(e,t,n,r)}function H(e){if(e instanceof Q)return new Q(e.h,e.s,e.l,e.opacity);if(e instanceof m||(e=A(e)),!e)return new Q;if(e instanceof Q)return e;var t=(e=e.rgb()).r/255,n=e.g/255,r=e.b/255,a=Math.min(t,n,r),i=Math.max(t,n,r),o=NaN,u=i-a,l=(i+a)/2;return u?(o=t===i?(n-r)/u+6*(n<r):n===i?(r-t)/u+2:(t-n)/u+4,u/=l<.5?i+a:2-i-a,o*=60):u=l>0&&l<1?0:o,new Q(o,u,l,e.opacity)}function q(e,t,n,r){return 1===arguments.length?H(e):new Q(e,t,n,null==r?1:r)}function Q(e,t,n,r){this.h=+e,this.s=+t,this.l=+n,this.opacity=+r}function Y(e){return(e=(e||0)%360)<0?e+360:e}function G(e){return Math.max(0,Math.min(1,e||0))}function K(e,t,n){return 255*(e<60?t+(n-t)*e/60:e<180?n:e<240?t+(n-t)*(240-e)/60:t)}function Z(e,t,n,r,a){var i=e*e,o=i*e;return((1-3*e+3*i-o)*t+(4-6*i+3*o)*n+(1+3*e+3*i-3*o)*r+o*a)/6}g(m,A,{copy(e){return Object.assign(new this.constructor,this,e)},displayable(){return this.rgb().displayable()},hex:L,formatHex:L,formatHex8:function(){return this.rgb().formatHex8()},formatHsl:function(){return H(this).formatHsl()},formatRgb:O,toString:O}),g(j,R,y(m,{brighter(e){return e=null==e?_:Math.pow(_,e),new j(this.r*e,this.g*e,this.b*e,this.opacity)},darker(e){return e=null==e?b:Math.pow(b,e),new j(this.r*e,this.g*e,this.b*e,this.opacity)},rgb(){return this},clamp(){return new j(B(this.r),B(this.g),B(this.b),$(this.opacity))},displayable(){return-.5<=this.r&&this.r<255.5&&-.5<=this.g&&this.g<255.5&&-.5<=this.b&&this.b<255.5&&0<=this.opacity&&this.opacity<=1},hex:U,formatHex:U,formatHex8:function(){return`#${W(this.r)}${W(this.g)}${W(this.b)}${W(255*(isNaN(this.opacity)?1:this.opacity))}`},formatRgb:I,toString:I})),g(Q,q,y(m,{brighter(e){return e=null==e?_:Math.pow(_,e),new Q(this.h,this.s,this.l*e,this.opacity)},darker(e){return e=null==e?b:Math.pow(b,e),new Q(this.h,this.s,this.l*e,this.opacity)},rgb(){var e=this.h%360+360*(this.h<0),t=isNaN(e)||isNaN(this.s)?0:this.s,n=this.l,r=n+(n<.5?n:1-n)*t,a=2*n-r;return new j(K(e>=240?e-240:e+120,a,r),K(e,a,r),K(e<120?e+240:e-120,a,r),this.opacity)},clamp(){return new Q(Y(this.h),G(this.s),G(this.l),$(this.opacity))},displayable(){return(0<=this.s&&this.s<=1||isNaN(this.s))&&0<=this.l&&this.l<=1&&0<=this.opacity&&this.opacity<=1},formatHsl(){const e=$(this.opacity);return`${1===e?\"hsl(\":\"hsla(\"}${Y(this.h)}, ${100*G(this.s)}%, ${100*G(this.l)}%${1===e?\")\":`, ${e})`}`}}));const X=e=>()=>e;function J(e,t){var n=t-e;return n?function(e,t){return function(n){return e+n*t}}(e,n):X(isNaN(e)?t:e)}const ee=function e(t){var n=function(e){return 1==(e=+e)?J:function(t,n){return n-t?function(e,t,n){return e=Math.pow(e,n),t=Math.pow(t,n)-e,n=1/n,function(r){return Math.pow(e+r*t,n)}}(t,n,e):X(isNaN(t)?n:t)}}(t);function r(e,t){var r=n((e=R(e)).r,(t=R(t)).r),a=n(e.g,t.g),i=n(e.b,t.b),o=J(e.opacity,t.opacity);return function(t){return e.r=r(t),e.g=a(t),e.b=i(t),e.opacity=o(t),e+\"\"}}return r.gamma=e,r}(1);function te(e){return function(t){var n,r,a=t.length,i=new Array(a),o=new Array(a),u=new Array(a);for(n=0;n<a;++n)r=R(t[n]),i[n]=r.r||0,o[n]=r.g||0,u[n]=r.b||0;return i=e(i),o=e(o),u=e(u),r.opacity=1,function(e){return r.r=i(e),r.g=o(e),r.b=u(e),r+\"\"}}}function ne(e,t){var n,r=t?t.length:0,a=e?Math.min(r,e.length):0,i=new Array(a),o=new Array(r);for(n=0;n<a;++n)i[n]=ce(e[n],t[n]);for(;n<r;++n)o[n]=t[n];return function(e){for(n=0;n<a;++n)o[n]=i[n](e);return o}}function re(e,t){var n=new Date;return e=+e,t=+t,function(r){return n.setTime(e*(1-r)+t*r),n}}function ae(e,t){return e=+e,t=+t,function(n){return e*(1-n)+t*n}}function ie(e,t){var n,r={},a={};for(n in null!==e&&\"object\"==typeof e||(e={}),null!==t&&\"object\"==typeof t||(t={}),t)n in e?r[n]=ce(e[n],t[n]):a[n]=t[n];return function(e){for(n in r)a[n]=r[n](e);return a}}te((function(e){var t=e.length-1;return function(n){var r=n<=0?n=0:n>=1?(n=1,t-1):Math.floor(n*t),a=e[r],i=e[r+1],o=r>0?e[r-1]:2*a-i,u=r<t-1?e[r+2]:2*i-a;return Z((n-r/t)*t,o,a,i,u)}})),te((function(e){var t=e.length;return function(n){var r=Math.floor(((n%=1)<0?++n:n)*t),a=e[(r+t-1)%t],i=e[r%t],o=e[(r+1)%t],u=e[(r+2)%t];return Z((n-r/t)*t,a,i,o,u)}}));var oe=/[-+]?(?:\\d+\\.?\\d*|\\.?\\d+)(?:[eE][-+]?\\d+)?/g,ue=new RegExp(oe.source,\"g\");function le(e,t){var n,r,a,i=oe.lastIndex=ue.lastIndex=0,o=-1,u=[],l=[];for(e+=\"\",t+=\"\";(n=oe.exec(e))&&(r=ue.exec(t));)(a=r.index)>i&&(a=t.slice(i,a),u[o]?u[o]+=a:u[++o]=a),(n=n[0])===(r=r[0])?u[o]?u[o]+=r:u[++o]=r:(u[++o]=null,l.push({i:o,x:ae(n,r)})),i=ue.lastIndex;return i<t.length&&(a=t.slice(i),u[o]?u[o]+=a:u[++o]=a),u.length<2?l[0]?function(e){return function(t){return e(t)+\"\"}}(l[0].x):function(e){return function(){return e}}(t):(t=l.length,function(e){for(var n,r=0;r<t;++r)u[(n=l[r]).i]=n.x(e);return u.join(\"\")})}function se(e,t){t||(t=[]);var n,r=e?Math.min(t.length,e.length):0,a=t.slice();return function(i){for(n=0;n<r;++n)a[n]=e[n]*(1-i)+t[n]*i;return a}}function ce(e,t){var n,r,a=typeof t;return null==t||\"boolean\"===a?X(t):(\"number\"===a?ae:\"string\"===a?(n=A(t))?(t=n,ee):le:t instanceof A?ee:t instanceof Date?re:(r=t,!ArrayBuffer.isView(r)||r instanceof DataView?Array.isArray(t)?ne:\"function\"!=typeof t.valueOf&&\"function\"!=typeof t.toString||isNaN(t)?ie:ae:se))(e,t)}function fe(e,t){return e=+e,t=+t,function(n){return Math.round(e*(1-n)+t*n)}}function pe(e){return+e}var de=[0,1];function he(e){return e}function ve(e,t){return(t-=e=+e)?function(n){return(n-e)/t}:(n=isNaN(t)?NaN:.5,function(){return n});var n}function ge(e,t,n){var r=e[0],a=e[1],i=t[0],o=t[1];return a<r?(r=ve(a,r),i=n(o,i)):(r=ve(r,a),i=n(i,o)),function(e){return i(r(e))}}function ye(e,t,n){var r=Math.min(e.length,t.length)-1,a=new Array(r),i=new Array(r),o=-1;for(e[r]<e[0]&&(e=e.slice().reverse(),t=t.slice().reverse());++o<r;)a[o]=ve(e[o],e[o+1]),i[o]=n(t[o],t[o+1]);return function(t){var n=v(e,t,1,r)-1;return i[n](a[n](t))}}function me(e,t){return t.domain(e.domain()).range(e.range()).interpolate(e.interpolate()).clamp(e.clamp()).unknown(e.unknown())}function be(){return function(){var e,t,n,r,a,i,o=de,u=de,l=ce,s=he;function c(){var e,t,n,l=Math.min(o.length,u.length);return s!==he&&(e=o[0],t=o[l-1],e>t&&(n=e,e=t,t=n),s=function(n){return Math.max(e,Math.min(t,n))}),r=l>2?ye:ge,a=i=null,f}function f(t){return null==t||isNaN(t=+t)?n:(a||(a=r(o.map(e),u,l)))(e(s(t)))}return f.invert=function(n){return s(t((i||(i=r(u,o.map(e),ae)))(n)))},f.domain=function(e){return arguments.length?(o=Array.from(e,pe),c()):o.slice()},f.range=function(e){return arguments.length?(u=Array.from(e),c()):u.slice()},f.rangeRound=function(e){return u=Array.from(e),l=fe,c()},f.clamp=function(e){return arguments.length?(s=!!e||he,c()):s!==he},f.interpolate=function(e){return arguments.length?(l=e,c()):l},f.unknown=function(e){return arguments.length?(n=e,f):n},function(n,r){return e=n,t=r,c()}}()(he,he)}function _e(e,t){switch(arguments.length){case 0:break;case 1:this.range(e);break;default:this.range(t).domain(e)}return this}var we,xe=/^(?:(.)?([<>=^]))?([+\\-( ])?([$#])?(0)?(\\d+)?(,)?(\\.\\d+)?(~)?([a-z%])?$/i;function ke(e){if(!(t=xe.exec(e)))throw new Error(\"invalid format: \"+e);var t;return new Se({fill:t[1],align:t[2],sign:t[3],symbol:t[4],zero:t[5],width:t[6],comma:t[7],precision:t[8]&&t[8].slice(1),trim:t[9],type:t[10]})}function Se(e){this.fill=void 0===e.fill?\" \":e.fill+\"\",this.align=void 0===e.align?\">\":e.align+\"\",this.sign=void 0===e.sign?\"-\":e.sign+\"\",this.symbol=void 0===e.symbol?\"\":e.symbol+\"\",this.zero=!!e.zero,this.width=void 0===e.width?void 0:+e.width,this.comma=!!e.comma,this.precision=void 0===e.precision?void 0:+e.precision,this.trim=!!e.trim,this.type=void 0===e.type?\"\":e.type+\"\"}function Ee(e,t){if((n=(e=t?e.toExponential(t-1):e.toExponential()).indexOf(\"e\"))<0)return null;var n,r=e.slice(0,n);return[r.length>1?r[0]+r.slice(2):r,+e.slice(n+1)]}function Ce(e){return(e=Ee(Math.abs(e)))?e[1]:NaN}function Te(e,t){var n=Ee(e,t);if(!n)return e+\"\";var r=n[0],a=n[1];return a<0?\"0.\"+new Array(-a).join(\"0\")+r:r.length>a+1?r.slice(0,a+1)+\".\"+r.slice(a+1):r+new Array(a-r.length+2).join(\"0\")}ke.prototype=Se.prototype,Se.prototype.toString=function(){return this.fill+this.align+this.sign+this.symbol+(this.zero?\"0\":\"\")+(void 0===this.width?\"\":Math.max(1,0|this.width))+(this.comma?\",\":\"\")+(void 0===this.precision?\"\":\".\"+Math.max(0,0|this.precision))+(this.trim?\"~\":\"\")+this.type};const Me={\"%\":(e,t)=>(100*e).toFixed(t),b:e=>Math.round(e).toString(2),c:e=>e+\"\",d:function(e){return Math.abs(e=Math.round(e))>=1e21?e.toLocaleString(\"en\").replace(/,/g,\"\"):e.toString(10)},e:(e,t)=>e.toExponential(t),f:(e,t)=>e.toFixed(t),g:(e,t)=>e.toPrecision(t),o:e=>Math.round(e).toString(8),p:(e,t)=>Te(100*e,t),r:Te,s:function(e,t){var n=Ee(e,t);if(!n)return e+\"\";var r=n[0],a=n[1],i=a-(we=3*Math.max(-8,Math.min(8,Math.floor(a/3))))+1,o=r.length;return i===o?r:i>o?r+new Array(i-o+1).join(\"0\"):i>0?r.slice(0,i)+\".\"+r.slice(i):\"0.\"+new Array(1-i).join(\"0\")+Ee(e,Math.max(0,t+i-1))[0]},X:e=>Math.round(e).toString(16).toUpperCase(),x:e=>Math.round(e).toString(16)};function Ne(e){return e}var Pe,ze,Le,Oe=Array.prototype.map,Ae=[\"y\",\"z\",\"a\",\"f\",\"p\",\"n\",\"µ\",\"m\",\"\",\"k\",\"M\",\"G\",\"T\",\"P\",\"E\",\"Z\",\"Y\"];function Fe(e){var t=e.domain;return e.ticks=function(e){var n=t();return function(e,t,n){if(!((n=+n)>0))return[];if((e=+e)==(t=+t))return[e];const r=t<e,[a,i,u]=r?o(t,e,n):o(e,t,n);if(!(i>=a))return[];const l=i-a+1,s=new Array(l);if(r)if(u<0)for(let e=0;e<l;++e)s[e]=(i-e)/-u;else for(let e=0;e<l;++e)s[e]=(i-e)*u;else if(u<0)for(let e=0;e<l;++e)s[e]=(a+e)/-u;else for(let e=0;e<l;++e)s[e]=(a+e)*u;return s}(n[0],n[n.length-1],null==e?10:e)},e.tickFormat=function(e,n){var r=t();return function(e,t,n,r){var a,i=l(e,t,n);switch((r=ke(null==r?\",f\":r)).type){case\"s\":var o=Math.max(Math.abs(e),Math.abs(t));return null!=r.precision||isNaN(a=function(e,t){return Math.max(0,3*Math.max(-8,Math.min(8,Math.floor(Ce(t)/3)))-Ce(Math.abs(e)))}(i,o))||(r.precision=a),Le(r,o);case\"\":case\"e\":case\"g\":case\"p\":case\"r\":null!=r.precision||isNaN(a=function(e,t){return e=Math.abs(e),t=Math.abs(t)-e,Math.max(0,Ce(t)-Ce(e))+1}(i,Math.max(Math.abs(e),Math.abs(t))))||(r.precision=a-(\"e\"===r.type));break;case\"f\":case\"%\":null!=r.precision||isNaN(a=function(e){return Math.max(0,-Ce(Math.abs(e)))}(i))||(r.precision=a-2*(\"%\"===r.type))}return ze(r)}(r[0],r[r.length-1],null==e?10:e,n)},e.nice=function(n){null==n&&(n=10);var r,a,i=t(),o=0,l=i.length-1,s=i[o],c=i[l],f=10;for(c<s&&(a=s,s=c,c=a,a=o,o=l,l=a);f-- >0;){if((a=u(s,c,n))===r)return i[o]=s,i[l]=c,t(i);if(a>0)s=Math.floor(s/a)*a,c=Math.ceil(c/a)*a;else{if(!(a<0))break;s=Math.ceil(s*a)/a,c=Math.floor(c*a)/a}r=a}return e},e}function De(){var e=be();return e.copy=function(){return me(e,De())},_e.apply(e,arguments),Fe(e)}Pe=function(e){var t,n,r=void 0===e.grouping||void 0===e.thousands?Ne:(t=Oe.call(e.grouping,Number),n=e.thousands+\"\",function(e,r){for(var a=e.length,i=[],o=0,u=t[0],l=0;a>0&&u>0&&(l+u+1>r&&(u=Math.max(1,r-l)),i.push(e.substring(a-=u,a+u)),!((l+=u+1)>r));)u=t[o=(o+1)%t.length];return i.reverse().join(n)}),a=void 0===e.currency?\"\":e.currency[0]+\"\",i=void 0===e.currency?\"\":e.currency[1]+\"\",o=void 0===e.decimal?\".\":e.decimal+\"\",u=void 0===e.numerals?Ne:function(e){return function(t){return t.replace(/[0-9]/g,(function(t){return e[+t]}))}}(Oe.call(e.numerals,String)),l=void 0===e.percent?\"%\":e.percent+\"\",s=void 0===e.minus?\"−\":e.minus+\"\",c=void 0===e.nan?\"NaN\":e.nan+\"\";function f(e){var t=(e=ke(e)).fill,n=e.align,f=e.sign,p=e.symbol,d=e.zero,h=e.width,v=e.comma,g=e.precision,y=e.trim,m=e.type;\"n\"===m?(v=!0,m=\"g\"):Me[m]||(void 0===g&&(g=12),y=!0,m=\"g\"),(d||\"0\"===t&&\"=\"===n)&&(d=!0,t=\"0\",n=\"=\");var b=\"$\"===p?a:\"#\"===p&&/[boxX]/.test(m)?\"0\"+m.toLowerCase():\"\",_=\"$\"===p?i:/[%p]/.test(m)?l:\"\",w=Me[m],x=/[defgprs%]/.test(m);function k(e){var a,i,l,p=b,k=_;if(\"c\"===m)k=w(e)+k,e=\"\";else{var S=(e=+e)<0||1/e<0;if(e=isNaN(e)?c:w(Math.abs(e),g),y&&(e=function(e){e:for(var t,n=e.length,r=1,a=-1;r<n;++r)switch(e[r]){case\".\":a=t=r;break;case\"0\":0===a&&(a=r),t=r;break;default:if(!+e[r])break e;a>0&&(a=0)}return a>0?e.slice(0,a)+e.slice(t+1):e}(e)),S&&0==+e&&\"+\"!==f&&(S=!1),p=(S?\"(\"===f?f:s:\"-\"===f||\"(\"===f?\"\":f)+p,k=(\"s\"===m?Ae[8+we/3]:\"\")+k+(S&&\"(\"===f?\")\":\"\"),x)for(a=-1,i=e.length;++a<i;)if(48>(l=e.charCodeAt(a))||l>57){k=(46===l?o+e.slice(a+1):e.slice(a))+k,e=e.slice(0,a);break}}v&&!d&&(e=r(e,1/0));var E=p.length+e.length+k.length,C=E<h?new Array(h-E+1).join(t):\"\";switch(v&&d&&(e=r(C+e,C.length?h-k.length:1/0),C=\"\"),n){case\"<\":e=p+e+k+C;break;case\"=\":e=p+C+e+k;break;case\"^\":e=C.slice(0,E=C.length>>1)+p+e+k+C.slice(E);break;default:e=C+p+e+k}return u(e)}return g=void 0===g?6:/[gprs]/.test(m)?Math.max(1,Math.min(21,g)):Math.max(0,Math.min(20,g)),k.toString=function(){return e+\"\"},k}return{format:f,formatPrefix:function(e,t){var n=f(((e=ke(e)).type=\"f\",e)),r=3*Math.max(-8,Math.min(8,Math.floor(Ce(t)/3))),a=Math.pow(10,-r),i=Ae[8+r/3];return function(e){return n(a*e)+i}}}}({thousands:\",\",grouping:[3],currency:[\"$\",\"\"]}),ze=Pe.format,Le=Pe.formatPrefix;var Re=n(486);const je={colors:{RdBu:[\"rgb(255, 13, 87)\",\"rgb(30, 136, 229)\"],GnPR:[\"rgb(24, 196, 93)\",\"rgb(124, 82, 255)\"],CyPU:[\"#0099C6\",\"#990099\"],PkYg:[\"#DD4477\",\"#66AA00\"],DrDb:[\"#B82E2E\",\"#316395\"],LpLb:[\"#994499\",\"#22AA99\"],YlDp:[\"#AAAA11\",\"#6633CC\"],OrId:[\"#E67300\",\"#3E0099\"]},gray:\"#777\"};function Ue(e){return Ue=\"function\"==typeof Symbol&&\"symbol\"==typeof Symbol.iterator?function(e){return typeof e}:function(e){return e&&\"function\"==typeof Symbol&&e.constructor===Symbol&&e!==Symbol.prototype?\"symbol\":typeof e},Ue(e)}function Ie(e,t){for(var n=0;n<t.length;n++){var r=t[n];r.enumerable=r.enumerable||!1,r.configurable=!0,\"value\"in r&&(r.writable=!0),Object.defineProperty(e,(void 0,a=function(e,t){if(\"object\"!==Ue(e)||null===e)return e;var n=e[Symbol.toPrimitive];if(void 0!==n){var r=n.call(e,\"string\");if(\"object\"!==Ue(r))return r;throw new TypeError(\"@@toPrimitive must return a primitive value.\")}return String(e)}(r.key),\"symbol\"===Ue(a)?a:String(a)),r)}var a}function $e(e,t){return $e=Object.setPrototypeOf?Object.setPrototypeOf.bind():function(e,t){return e.__proto__=t,e},$e(e,t)}function Be(e){if(void 0===e)throw new ReferenceError(\"this hasn't been initialised - super() hasn't been called\");return e}function We(e){return We=Object.setPrototypeOf?Object.getPrototypeOf.bind():function(e){return e.__proto__||Object.getPrototypeOf(e)},We(e)}var Ve=function(t){!function(e,t){if(\"function\"!=typeof t&&null!==t)throw new TypeError(\"Super expression must either be null or a function\");e.prototype=Object.create(t&&t.prototype,{constructor:{value:e,writable:!0,configurable:!0}}),Object.defineProperty(e,\"prototype\",{writable:!1}),t&&$e(e,t)}(u,t);var n,r,a,i,o=(a=u,i=function(){if(\"undefined\"==typeof Reflect||!Reflect.construct)return!1;if(Reflect.construct.sham)return!1;if(\"function\"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Reflect.construct(Boolean,[],(function(){}))),!0}catch(e){return!1}}(),function(){var e,t=We(a);if(i){var n=We(this).constructor;e=Reflect.construct(t,arguments,n)}else e=t.apply(this,arguments);return function(e,t){if(t&&(\"object\"===Ue(t)||\"function\"==typeof t))return t;if(void 0!==t)throw new TypeError(\"Derived constructors may only return object or undefined\");return Be(e)}(this,e)});function u(){var e;return function(e,t){if(!(e instanceof t))throw new TypeError(\"Cannot call a class as a function\")}(this,u),(e=o.call(this)).width=100,window.lastSimpleListInstance=Be(e),e.effectFormat=ze(\".2\"),e}return n=u,(r=[{key:\"render\",value:function(){var t=this,n=void 0;\"string\"==typeof this.props.plot_cmap?this.props.plot_cmap in je.colors?n=je.colors[this.props.plot_cmap]:(console.log(\"Invalid color map name, reverting to default.\"),n=je.colors.RdBu):Array.isArray(this.props.plot_cmap)&&(n=this.props.plot_cmap),console.log(this.props.features,this.props.features),this.scale=De().domain([0,(0,Re.max)((0,Re.map)(this.props.features,(function(e){return Math.abs(e.effect)})))]).range([0,this.width]);var r=(0,Re.reverse)((0,Re.sortBy)(Object.keys(this.props.features),(function(e){return Math.abs(t.props.features[e].effect)}))).map((function(r){var a,i,o=t.props.features[r],u=t.props.featureNames[r],l={width:t.scale(Math.abs(o.effect)),height:\"20px\",background:o.effect<0?n[0]:n[1],display:\"inline-block\"},s={lineHeight:\"20px\",display:\"inline-block\",width:t.width+40,verticalAlign:\"top\",marginRight:\"5px\",textAlign:\"right\"},c={lineHeight:\"20px\",display:\"inline-block\",width:t.width+40,verticalAlign:\"top\",marginLeft:\"5px\"};return o.effect<0?(i=e.createElement(\"span\",{style:c},u),s.width=40+t.width-t.scale(Math.abs(o.effect)),s.textAlign=\"right\",s.color=\"#999\",s.fontSize=\"13px\",a=e.createElement(\"span\",{style:s},t.effectFormat(o.effect))):(s.textAlign=\"right\",a=e.createElement(\"span\",{style:s},u),c.width=40,c.textAlign=\"left\",c.color=\"#999\",c.fontSize=\"13px\",i=e.createElement(\"span\",{style:c},t.effectFormat(o.effect))),e.createElement(\"div\",{key:r,style:{marginTop:\"2px\"}},a,e.createElement(\"div\",{style:l}),i)}));return e.createElement(\"span\",null,r)}}])&&Ie(n.prototype,r),Object.defineProperty(n,\"prototype\",{writable:!1}),u}(e.Component);Ve.defaultProps={plot_cmap:\"RdBu\"};const He=Ve;function qe(){}function Qe(e){return null==e?qe:function(){return this.querySelector(e)}}function Ye(){return[]}function Ge(e){return function(t){return t.matches(e)}}var Ke=Array.prototype.find;function Ze(){return this.firstElementChild}var Xe=Array.prototype.filter;function Je(){return Array.from(this.children)}function et(e){return new Array(e.length)}function tt(e,t){this.ownerDocument=e.ownerDocument,this.namespaceURI=e.namespaceURI,this._next=null,this._parent=e,this.__data__=t}function nt(e,t,n,r,a,i){for(var o,u=0,l=t.length,s=i.length;u<s;++u)(o=t[u])?(o.__data__=i[u],r[u]=o):n[u]=new tt(e,i[u]);for(;u<l;++u)(o=t[u])&&(a[u]=o)}function rt(e,t,n,r,a,i,o){var u,l,s,c=new Map,f=t.length,p=i.length,d=new Array(f);for(u=0;u<f;++u)(l=t[u])&&(d[u]=s=o.call(l,l.__data__,u,t)+\"\",c.has(s)?a[u]=l:c.set(s,l));for(u=0;u<p;++u)s=o.call(e,i[u],u,i)+\"\",(l=c.get(s))?(r[u]=l,l.__data__=i[u],c.delete(s)):n[u]=new tt(e,i[u]);for(u=0;u<f;++u)(l=t[u])&&c.get(d[u])===l&&(a[u]=l)}function at(e){return e.__data__}function it(e){return\"object\"==typeof e&&\"length\"in e?e:Array.from(e)}function ot(e,t){return e<t?-1:e>t?1:e>=t?0:NaN}tt.prototype={constructor:tt,appendChild:function(e){return this._parent.insertBefore(e,this._next)},insertBefore:function(e,t){return this._parent.insertBefore(e,t)},querySelector:function(e){return this._parent.querySelector(e)},querySelectorAll:function(e){return this._parent.querySelectorAll(e)}};var ut=\"http://www.w3.org/1999/xhtml\";const lt={svg:\"http://www.w3.org/2000/svg\",xhtml:ut,xlink:\"http://www.w3.org/1999/xlink\",xml:\"http://www.w3.org/XML/1998/namespace\",xmlns:\"http://www.w3.org/2000/xmlns/\"};function st(e){var t=e+=\"\",n=t.indexOf(\":\");return n>=0&&\"xmlns\"!==(t=e.slice(0,n))&&(e=e.slice(n+1)),lt.hasOwnProperty(t)?{space:lt[t],local:e}:e}function ct(e){return function(){this.removeAttribute(e)}}function ft(e){return function(){this.removeAttributeNS(e.space,e.local)}}function pt(e,t){return function(){this.setAttribute(e,t)}}function dt(e,t){return function(){this.setAttributeNS(e.space,e.local,t)}}function ht(e,t){return function(){var n=t.apply(this,arguments);null==n?this.removeAttribute(e):this.setAttribute(e,n)}}function vt(e,t){return function(){var n=t.apply(this,arguments);null==n?this.removeAttributeNS(e.space,e.local):this.setAttributeNS(e.space,e.local,n)}}function gt(e){return e.ownerDocument&&e.ownerDocument.defaultView||e.document&&e||e.defaultView}function yt(e){return function(){this.style.removeProperty(e)}}function mt(e,t,n){return function(){this.style.setProperty(e,t,n)}}function bt(e,t,n){return function(){var r=t.apply(this,arguments);null==r?this.style.removeProperty(e):this.style.setProperty(e,r,n)}}function _t(e){return function(){delete this[e]}}function wt(e,t){return function(){this[e]=t}}function xt(e,t){return function(){var n=t.apply(this,arguments);null==n?delete this[e]:this[e]=n}}function kt(e){return e.trim().split(/^|\\s+/)}function St(e){return e.classList||new Et(e)}function Et(e){this._node=e,this._names=kt(e.getAttribute(\"class\")||\"\")}function Ct(e,t){for(var n=St(e),r=-1,a=t.length;++r<a;)n.add(t[r])}function Tt(e,t){for(var n=St(e),r=-1,a=t.length;++r<a;)n.remove(t[r])}function Mt(e){return function(){Ct(this,e)}}function Nt(e){return function(){Tt(this,e)}}function Pt(e,t){return function(){(t.apply(this,arguments)?Ct:Tt)(this,e)}}function zt(){this.textContent=\"\"}function Lt(e){return function(){this.textContent=e}}function Ot(e){return function(){var t=e.apply(this,arguments);this.textContent=null==t?\"\":t}}function At(){this.innerHTML=\"\"}function Ft(e){return function(){this.innerHTML=e}}function Dt(e){return function(){var t=e.apply(this,arguments);this.innerHTML=null==t?\"\":t}}function Rt(){this.nextSibling&&this.parentNode.appendChild(this)}function jt(){this.previousSibling&&this.parentNode.insertBefore(this,this.parentNode.firstChild)}function Ut(e){return function(){var t=this.ownerDocument,n=this.namespaceURI;return n===ut&&t.documentElement.namespaceURI===ut?t.createElement(e):t.createElementNS(n,e)}}function It(e){return function(){return this.ownerDocument.createElementNS(e.space,e.local)}}function $t(e){var t=st(e);return(t.local?It:Ut)(t)}function Bt(){return null}function Wt(){var e=this.parentNode;e&&e.removeChild(this)}function Vt(){var e=this.cloneNode(!1),t=this.parentNode;return t?t.insertBefore(e,this.nextSibling):e}function Ht(){var e=this.cloneNode(!0),t=this.parentNode;return t?t.insertBefore(e,this.nextSibling):e}function qt(e){return function(){var t=this.__on;if(t){for(var n,r=0,a=-1,i=t.length;r<i;++r)n=t[r],e.type&&n.type!==e.type||n.name!==e.name?t[++a]=n:this.removeEventListener(n.type,n.listener,n.options);++a?t.length=a:delete this.__on}}}function Qt(e,t,n){return function(){var r,a=this.__on,i=function(e){return function(t){e.call(this,t,this.__data__)}}(t);if(a)for(var o=0,u=a.length;o<u;++o)if((r=a[o]).type===e.type&&r.name===e.name)return this.removeEventListener(r.type,r.listener,r.options),this.addEventListener(r.type,r.listener=i,r.options=n),void(r.value=t);this.addEventListener(e.type,i,n),r={type:e.type,name:e.name,value:t,listener:i,options:n},a?a.push(r):this.__on=[r]}}function Yt(e,t,n){var r=gt(e),a=r.CustomEvent;\"function\"==typeof a?a=new a(t,n):(a=r.document.createEvent(\"Event\"),n?(a.initEvent(t,n.bubbles,n.cancelable),a.detail=n.detail):a.initEvent(t,!1,!1)),e.dispatchEvent(a)}function Gt(e,t){return function(){return Yt(this,e,t)}}function Kt(e,t){return function(){return Yt(this,e,t.apply(this,arguments))}}Et.prototype={add:function(e){this._names.indexOf(e)<0&&(this._names.push(e),this._node.setAttribute(\"class\",this._names.join(\" \")))},remove:function(e){var t=this._names.indexOf(e);t>=0&&(this._names.splice(t,1),this._node.setAttribute(\"class\",this._names.join(\" \")))},contains:function(e){return this._names.indexOf(e)>=0}};var Zt=[null];function Xt(e,t){this._groups=e,this._parents=t}function Jt(e){return\"string\"==typeof e?new Xt([[document.querySelector(e)]],[document.documentElement]):new Xt([[e]],Zt)}function en(e){return e}Xt.prototype=function(){return new Xt([[document.documentElement]],Zt)}.prototype={constructor:Xt,select:function(e){\"function\"!=typeof e&&(e=Qe(e));for(var t=this._groups,n=t.length,r=new Array(n),a=0;a<n;++a)for(var i,o,u=t[a],l=u.length,s=r[a]=new Array(l),c=0;c<l;++c)(i=u[c])&&(o=e.call(i,i.__data__,c,u))&&(\"__data__\"in i&&(o.__data__=i.__data__),s[c]=o);return new Xt(r,this._parents)},selectAll:function(e){e=\"function\"==typeof e?function(e){return function(){return null==(t=e.apply(this,arguments))?[]:Array.isArray(t)?t:Array.from(t);var t}}(e):function(e){return null==e?Ye:function(){return this.querySelectorAll(e)}}(e);for(var t=this._groups,n=t.length,r=[],a=[],i=0;i<n;++i)for(var o,u=t[i],l=u.length,s=0;s<l;++s)(o=u[s])&&(r.push(e.call(o,o.__data__,s,u)),a.push(o));return new Xt(r,a)},selectChild:function(e){return this.select(null==e?Ze:function(e){return function(){return Ke.call(this.children,e)}}(\"function\"==typeof e?e:Ge(e)))},selectChildren:function(e){return this.selectAll(null==e?Je:function(e){return function(){return Xe.call(this.children,e)}}(\"function\"==typeof e?e:Ge(e)))},filter:function(e){\"function\"!=typeof e&&(e=function(e){return function(){return this.matches(e)}}(e));for(var t=this._groups,n=t.length,r=new Array(n),a=0;a<n;++a)for(var i,o=t[a],u=o.length,l=r[a]=[],s=0;s<u;++s)(i=o[s])&&e.call(i,i.__data__,s,o)&&l.push(i);return new Xt(r,this._parents)},data:function(e,t){if(!arguments.length)return Array.from(this,at);var n,r=t?rt:nt,a=this._parents,i=this._groups;\"function\"!=typeof e&&(n=e,e=function(){return n});for(var o=i.length,u=new Array(o),l=new Array(o),s=new Array(o),c=0;c<o;++c){var f=a[c],p=i[c],d=p.length,h=it(e.call(f,f&&f.__data__,c,a)),v=h.length,g=l[c]=new Array(v),y=u[c]=new Array(v);r(f,p,g,y,s[c]=new Array(d),h,t);for(var m,b,_=0,w=0;_<v;++_)if(m=g[_]){for(_>=w&&(w=_+1);!(b=y[w])&&++w<v;);m._next=b||null}}return(u=new Xt(u,a))._enter=l,u._exit=s,u},enter:function(){return new Xt(this._enter||this._groups.map(et),this._parents)},exit:function(){return new Xt(this._exit||this._groups.map(et),this._parents)},join:function(e,t,n){var r=this.enter(),a=this,i=this.exit();return\"function\"==typeof e?(r=e(r))&&(r=r.selection()):r=r.append(e+\"\"),null!=t&&(a=t(a))&&(a=a.selection()),null==n?i.remove():n(i),r&&a?r.merge(a).order():a},merge:function(e){for(var t=e.selection?e.selection():e,n=this._groups,r=t._groups,a=n.length,i=r.length,o=Math.min(a,i),u=new Array(a),l=0;l<o;++l)for(var s,c=n[l],f=r[l],p=c.length,d=u[l]=new Array(p),h=0;h<p;++h)(s=c[h]||f[h])&&(d[h]=s);for(;l<a;++l)u[l]=n[l];return new Xt(u,this._parents)},selection:function(){return this},order:function(){for(var e=this._groups,t=-1,n=e.length;++t<n;)for(var r,a=e[t],i=a.length-1,o=a[i];--i>=0;)(r=a[i])&&(o&&4^r.compareDocumentPosition(o)&&o.parentNode.insertBefore(r,o),o=r);return this},sort:function(e){function t(t,n){return t&&n?e(t.__data__,n.__data__):!t-!n}e||(e=ot);for(var n=this._groups,r=n.length,a=new Array(r),i=0;i<r;++i){for(var o,u=n[i],l=u.length,s=a[i]=new Array(l),c=0;c<l;++c)(o=u[c])&&(s[c]=o);s.sort(t)}return new Xt(a,this._parents).order()},call:function(){var e=arguments[0];return arguments[0]=this,e.apply(null,arguments),this},nodes:function(){return Array.from(this)},node:function(){for(var e=this._groups,t=0,n=e.length;t<n;++t)for(var r=e[t],a=0,i=r.length;a<i;++a){var o=r[a];if(o)return o}return null},size:function(){let e=0;for(const t of this)++e;return e},empty:function(){return!this.node()},each:function(e){for(var t=this._groups,n=0,r=t.length;n<r;++n)for(var a,i=t[n],o=0,u=i.length;o<u;++o)(a=i[o])&&e.call(a,a.__data__,o,i);return this},attr:function(e,t){var n=st(e);if(arguments.length<2){var r=this.node();return n.local?r.getAttributeNS(n.space,n.local):r.getAttribute(n)}return this.each((null==t?n.local?ft:ct:\"function\"==typeof t?n.local?vt:ht:n.local?dt:pt)(n,t))},style:function(e,t,n){return arguments.length>1?this.each((null==t?yt:\"function\"==typeof t?bt:mt)(e,t,null==n?\"\":n)):function(e,t){return e.style.getPropertyValue(t)||gt(e).getComputedStyle(e,null).getPropertyValue(t)}(this.node(),e)},property:function(e,t){return arguments.length>1?this.each((null==t?_t:\"function\"==typeof t?xt:wt)(e,t)):this.node()[e]},classed:function(e,t){var n=kt(e+\"\");if(arguments.length<2){for(var r=St(this.node()),a=-1,i=n.length;++a<i;)if(!r.contains(n[a]))return!1;return!0}return this.each((\"function\"==typeof t?Pt:t?Mt:Nt)(n,t))},text:function(e){return arguments.length?this.each(null==e?zt:(\"function\"==typeof e?Ot:Lt)(e)):this.node().textContent},html:function(e){return arguments.length?this.each(null==e?At:(\"function\"==typeof e?Dt:Ft)(e)):this.node().innerHTML},raise:function(){return this.each(Rt)},lower:function(){return this.each(jt)},append:function(e){var t=\"function\"==typeof e?e:$t(e);return this.select((function(){return this.appendChild(t.apply(this,arguments))}))},insert:function(e,t){var n=\"function\"==typeof e?e:$t(e),r=null==t?Bt:\"function\"==typeof t?t:Qe(t);return this.select((function(){return this.insertBefore(n.apply(this,arguments),r.apply(this,arguments)||null)}))},remove:function(){return this.each(Wt)},clone:function(e){return this.select(e?Ht:Vt)},datum:function(e){return arguments.length?this.property(\"__data__\",e):this.node().__data__},on:function(e,t,n){var r,a,i=function(e){return e.trim().split(/^|\\s+/).map((function(e){var t=\"\",n=e.indexOf(\".\");return n>=0&&(t=e.slice(n+1),e=e.slice(0,n)),{type:e,name:t}}))}(e+\"\"),o=i.length;if(!(arguments.length<2)){for(u=t?Qt:qt,r=0;r<o;++r)this.each(u(i[r],t,n));return this}var u=this.node().__on;if(u)for(var l,s=0,c=u.length;s<c;++s)for(r=0,l=u[s];r<o;++r)if((a=i[r]).type===l.type&&a.name===l.name)return l.value},dispatch:function(e,t){return this.each((\"function\"==typeof t?Kt:Gt)(e,t))},[Symbol.iterator]:function*(){for(var e=this._groups,t=0,n=e.length;t<n;++t)for(var r,a=e[t],i=0,o=a.length;i<o;++i)(r=a[i])&&(yield r)}};var tn=1,nn=2,rn=3,an=4,on=1e-6;function un(e){return\"translate(\"+e+\",0)\"}function ln(e){return\"translate(0,\"+e+\")\"}function sn(e){return t=>+e(t)}function cn(e,t){return t=Math.max(0,e.bandwidth()-2*t)/2,e.round()&&(t=Math.round(t)),n=>+e(n)+t}function fn(){return!this.__axis}function pn(e,t){var n=[],r=null,a=null,i=6,o=6,u=3,l=\"undefined\"!=typeof window&&window.devicePixelRatio>1?0:.5,s=e===tn||e===an?-1:1,c=e===an||e===nn?\"x\":\"y\",f=e===tn||e===rn?un:ln;function p(p){var d=null==r?t.ticks?t.ticks.apply(t,n):t.domain():r,h=null==a?t.tickFormat?t.tickFormat.apply(t,n):en:a,v=Math.max(i,0)+u,g=t.range(),y=+g[0]+l,m=+g[g.length-1]+l,b=(t.bandwidth?cn:sn)(t.copy(),l),_=p.selection?p.selection():p,w=_.selectAll(\".domain\").data([null]),x=_.selectAll(\".tick\").data(d,t).order(),k=x.exit(),S=x.enter().append(\"g\").attr(\"class\",\"tick\"),E=x.select(\"line\"),C=x.select(\"text\");w=w.merge(w.enter().insert(\"path\",\".tick\").attr(\"class\",\"domain\").attr(\"stroke\",\"currentColor\")),x=x.merge(S),E=E.merge(S.append(\"line\").attr(\"stroke\",\"currentColor\").attr(c+\"2\",s*i)),C=C.merge(S.append(\"text\").attr(\"fill\",\"currentColor\").attr(c,s*v).attr(\"dy\",e===tn?\"0em\":e===rn?\"0.71em\":\"0.32em\")),p!==_&&(w=w.transition(p),x=x.transition(p),E=E.transition(p),C=C.transition(p),k=k.transition(p).attr(\"opacity\",on).attr(\"transform\",(function(e){return isFinite(e=b(e))?f(e+l):this.getAttribute(\"transform\")})),S.attr(\"opacity\",on).attr(\"transform\",(function(e){var t=this.parentNode.__axis;return f((t&&isFinite(t=t(e))?t:b(e))+l)}))),k.remove(),w.attr(\"d\",e===an||e===nn?o?\"M\"+s*o+\",\"+y+\"H\"+l+\"V\"+m+\"H\"+s*o:\"M\"+l+\",\"+y+\"V\"+m:o?\"M\"+y+\",\"+s*o+\"V\"+l+\"H\"+m+\"V\"+s*o:\"M\"+y+\",\"+l+\"H\"+m),x.attr(\"opacity\",1).attr(\"transform\",(function(e){return f(b(e)+l)})),E.attr(c+\"2\",s*i),C.attr(c,s*v).text(h),_.filter(fn).attr(\"fill\",\"none\").attr(\"font-size\",10).attr(\"font-family\",\"sans-serif\").attr(\"text-anchor\",e===nn?\"start\":e===an?\"end\":\"middle\"),_.each((function(){this.__axis=b}))}return p.scale=function(e){return arguments.length?(t=e,p):t},p.ticks=function(){return n=Array.from(arguments),p},p.tickArguments=function(e){return arguments.length?(n=null==e?[]:Array.from(e),p):n.slice()},p.tickValues=function(e){return arguments.length?(r=null==e?null:Array.from(e),p):r&&r.slice()},p.tickFormat=function(e){return arguments.length?(a=e,p):a},p.tickSize=function(e){return arguments.length?(i=o=+e,p):i},p.tickSizeInner=function(e){return arguments.length?(i=+e,p):i},p.tickSizeOuter=function(e){return arguments.length?(o=+e,p):o},p.tickPadding=function(e){return arguments.length?(u=+e,p):u},p.offset=function(e){return arguments.length?(l=+e,p):l},p}function dn(e){return pn(rn,e)}function hn(e){return function(){return e}}function vn(e){this._context=e}function gn(e){return new vn(e)}Array.prototype.slice,vn.prototype={areaStart:function(){this._line=0},areaEnd:function(){this._line=NaN},lineStart:function(){this._point=0},lineEnd:function(){(this._line||0!==this._line&&1===this._point)&&this._context.closePath(),this._line=1-this._line},point:function(e,t){switch(e=+e,t=+t,this._point){case 0:this._point=1,this._line?this._context.lineTo(e,t):this._context.moveTo(e,t);break;case 1:this._point=2;default:this._context.lineTo(e,t)}}};const yn=Math.PI,mn=2*yn,bn=1e-6,_n=mn-bn;function wn(e){this._+=e[0];for(let t=1,n=e.length;t<n;++t)this._+=arguments[t]+e[t]}class xn{constructor(e){this._x0=this._y0=this._x1=this._y1=null,this._=\"\",this._append=null==e?wn:function(e){let t=Math.floor(e);if(!(t>=0))throw new Error(`invalid digits: ${e}`);if(t>15)return wn;const n=10**t;return function(e){this._+=e[0];for(let t=1,r=e.length;t<r;++t)this._+=Math.round(arguments[t]*n)/n+e[t]}}(e)}moveTo(e,t){this._append`M${this._x0=this._x1=+e},${this._y0=this._y1=+t}`}closePath(){null!==this._x1&&(this._x1=this._x0,this._y1=this._y0,this._append`Z`)}lineTo(e,t){this._append`L${this._x1=+e},${this._y1=+t}`}quadraticCurveTo(e,t,n,r){this._append`Q${+e},${+t},${this._x1=+n},${this._y1=+r}`}bezierCurveTo(e,t,n,r,a,i){this._append`C${+e},${+t},${+n},${+r},${this._x1=+a},${this._y1=+i}`}arcTo(e,t,n,r,a){if(e=+e,t=+t,n=+n,r=+r,(a=+a)<0)throw new Error(`negative radius: ${a}`);let i=this._x1,o=this._y1,u=n-e,l=r-t,s=i-e,c=o-t,f=s*s+c*c;if(null===this._x1)this._append`M${this._x1=e},${this._y1=t}`;else if(f>bn)if(Math.abs(c*u-l*s)>bn&&a){let p=n-i,d=r-o,h=u*u+l*l,v=p*p+d*d,g=Math.sqrt(h),y=Math.sqrt(f),m=a*Math.tan((yn-Math.acos((h+f-v)/(2*g*y)))/2),b=m/y,_=m/g;Math.abs(b-1)>bn&&this._append`L${e+b*s},${t+b*c}`,this._append`A${a},${a},0,0,${+(c*p>s*d)},${this._x1=e+_*u},${this._y1=t+_*l}`}else this._append`L${this._x1=e},${this._y1=t}`}arc(e,t,n,r,a,i){if(e=+e,t=+t,i=!!i,(n=+n)<0)throw new Error(`negative radius: ${n}`);let o=n*Math.cos(r),u=n*Math.sin(r),l=e+o,s=t+u,c=1^i,f=i?r-a:a-r;null===this._x1?this._append`M${l},${s}`:(Math.abs(this._x1-l)>bn||Math.abs(this._y1-s)>bn)&&this._append`L${l},${s}`,n&&(f<0&&(f=f%mn+mn),f>_n?this._append`A${n},${n},0,1,${c},${e-o},${t-u}A${n},${n},0,1,${c},${this._x1=l},${this._y1=s}`:f>bn&&this._append`A${n},${n},0,${+(f>=yn)},${c},${this._x1=e+n*Math.cos(a)},${this._y1=t+n*Math.sin(a)}`)}rect(e,t,n,r){this._append`M${this._x0=this._x1=+e},${this._y0=this._y1=+t}h${n=+n}v${+r}h${-n}Z`}toString(){return this._}}function kn(e){return e[0]}function Sn(e){return e[1]}function En(e,t){var n=hn(!0),r=null,a=gn,i=null,o=function(e){let t=3;return e.digits=function(n){if(!arguments.length)return t;if(null==n)t=null;else{const e=Math.floor(n);if(!(e>=0))throw new RangeError(`invalid digits: ${n}`);t=e}return e},()=>new xn(t)}(u);function u(u){var l,s,c,f=(u=function(e){return\"object\"==typeof e&&\"length\"in e?e:Array.from(e)}(u)).length,p=!1;for(null==r&&(i=a(c=o())),l=0;l<=f;++l)!(l<f&&n(s=u[l],l,u))===p&&((p=!p)?i.lineStart():i.lineEnd()),p&&i.point(+e(s,l,u),+t(s,l,u));if(c)return i=null,c+\"\"||null}return e=\"function\"==typeof e?e:void 0===e?kn:hn(e),t=\"function\"==typeof t?t:void 0===t?Sn:hn(t),u.x=function(t){return arguments.length?(e=\"function\"==typeof t?t:hn(+t),u):e},u.y=function(e){return arguments.length?(t=\"function\"==typeof e?e:hn(+e),u):t},u.defined=function(e){return arguments.length?(n=\"function\"==typeof e?e:hn(!!e),u):n},u.curve=function(e){return arguments.length?(a=e,null!=r&&(i=a(r)),u):a},u.context=function(e){return arguments.length?(null==e?r=i=null:i=a(r=e),u):r},u}function Cn(e){return Cn=\"function\"==typeof Symbol&&\"symbol\"==typeof Symbol.iterator?function(e){return typeof e}:function(e){return e&&\"function\"==typeof Symbol&&e.constructor===Symbol&&e!==Symbol.prototype?\"symbol\":typeof e},Cn(e)}function Tn(e,t){for(var n=0;n<t.length;n++){var r=t[n];r.enumerable=r.enumerable||!1,r.configurable=!0,\"value\"in r&&(r.writable=!0),Object.defineProperty(e,(void 0,a=function(e,t){if(\"object\"!==Cn(e)||null===e)return e;var n=e[Symbol.toPrimitive];if(void 0!==n){var r=n.call(e,\"string\");if(\"object\"!==Cn(r))return r;throw new TypeError(\"@@toPrimitive must return a primitive value.\")}return String(e)}(r.key),\"symbol\"===Cn(a)?a:String(a)),r)}var a}function Mn(e,t){return Mn=Object.setPrototypeOf?Object.setPrototypeOf.bind():function(e,t){return e.__proto__=t,e},Mn(e,t)}function Nn(e){if(void 0===e)throw new ReferenceError(\"this hasn't been initialised - super() hasn't been called\");return e}function Pn(e){return Pn=Object.setPrototypeOf?Object.getPrototypeOf.bind():function(e){return e.__proto__||Object.getPrototypeOf(e)},Pn(e)}var zn=function(t){!function(e,t){if(\"function\"!=typeof t&&null!==t)throw new TypeError(\"Super expression must either be null or a function\");e.prototype=Object.create(t&&t.prototype,{constructor:{value:e,writable:!0,configurable:!0}}),Object.defineProperty(e,\"prototype\",{writable:!1}),t&&Mn(e,t)}(u,t);var n,r,a,i,o=(a=u,i=function(){if(\"undefined\"==typeof Reflect||!Reflect.construct)return!1;if(Reflect.construct.sham)return!1;if(\"function\"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Reflect.construct(Boolean,[],(function(){}))),!0}catch(e){return!1}}(),function(){var e,t=Pn(a);if(i){var n=Pn(this).constructor;e=Reflect.construct(t,arguments,n)}else e=t.apply(this,arguments);return function(e,t){if(t&&(\"object\"===Cn(t)||\"function\"==typeof t))return t;if(void 0!==t)throw new TypeError(\"Derived constructors may only return object or undefined\");return Nn(e)}(this,e)});function u(){var e;return function(e,t){if(!(e instanceof t))throw new TypeError(\"Cannot call a class as a function\")}(this,u),e=o.call(this),window.lastAdditiveForceVisualizer=Nn(e),e.effectFormat=ze(\".2\"),e.redraw=(0,Re.debounce)((function(){return e.draw()}),200),e}return n=u,(r=[{key:\"componentDidMount\",value:function(){var e=this;this.mainGroup=this.svg.append(\"g\"),this.axisElement=this.mainGroup.append(\"g\").attr(\"transform\",\"translate(0,35)\").attr(\"class\",\"force-bar-axis\"),this.onTopGroup=this.svg.append(\"g\"),this.baseValueTitle=this.svg.append(\"text\"),this.joinPointLine=this.svg.append(\"line\"),this.joinPointLabelOutline=this.svg.append(\"text\"),this.joinPointLabel=this.svg.append(\"text\"),this.joinPointTitleLeft=this.svg.append(\"text\"),this.joinPointTitleLeftArrow=this.svg.append(\"text\"),this.joinPointTitle=this.svg.append(\"text\"),this.joinPointTitleRightArrow=this.svg.append(\"text\"),this.joinPointTitleRight=this.svg.append(\"text\"),this.hoverLabelBacking=this.svg.append(\"text\").attr(\"x\",10).attr(\"y\",20).attr(\"text-anchor\",\"middle\").attr(\"font-size\",12).attr(\"stroke\",\"#fff\").attr(\"fill\",\"#fff\").attr(\"stroke-width\",\"4\").attr(\"stroke-linejoin\",\"round\").text(\"\").on(\"mouseover\",(function(){e.hoverLabel.attr(\"opacity\",1),e.hoverLabelBacking.attr(\"opacity\",1)})).on(\"mouseout\",(function(){e.hoverLabel.attr(\"opacity\",0),e.hoverLabelBacking.attr(\"opacity\",0)})),this.hoverLabel=this.svg.append(\"text\").attr(\"x\",10).attr(\"y\",20).attr(\"text-anchor\",\"middle\").attr(\"font-size\",12).attr(\"fill\",\"#0f0\").text(\"\").on(\"mouseover\",(function(){e.hoverLabel.attr(\"opacity\",1),e.hoverLabelBacking.attr(\"opacity\",1)})).on(\"mouseout\",(function(){e.hoverLabel.attr(\"opacity\",0),e.hoverLabelBacking.attr(\"opacity\",0)}));var t=void 0;\"string\"==typeof this.props.plot_cmap?this.props.plot_cmap in je.colors?t=je.colors[this.props.plot_cmap]:(console.log(\"Invalid color map name, reverting to default.\"),t=je.colors.RdBu):Array.isArray(this.props.plot_cmap)&&(t=this.props.plot_cmap),this.colors=t.map((function(e){return q(e)})),this.brighterColors=[1.45,1.6].map((function(t,n){return e.colors[n].brighter(t)})),this.colors.map((function(t,n){var r=e.svg.append(\"linearGradient\").attr(\"id\",\"linear-grad-\"+n).attr(\"x1\",\"0%\").attr(\"y1\",\"0%\").attr(\"x2\",\"0%\").attr(\"y2\",\"100%\");r.append(\"stop\").attr(\"offset\",\"0%\").attr(\"stop-color\",t).attr(\"stop-opacity\",.6),r.append(\"stop\").attr(\"offset\",\"100%\").attr(\"stop-color\",t).attr(\"stop-opacity\",0);var a=e.svg.append(\"linearGradient\").attr(\"id\",\"linear-backgrad-\"+n).attr(\"x1\",\"0%\").attr(\"y1\",\"0%\").attr(\"x2\",\"0%\").attr(\"y2\",\"100%\");a.append(\"stop\").attr(\"offset\",\"0%\").attr(\"stop-color\",t).attr(\"stop-opacity\",.5),a.append(\"stop\").attr(\"offset\",\"100%\").attr(\"stop-color\",t).attr(\"stop-opacity\",0)})),this.tickFormat=ze(\",.4\"),this.scaleCentered=De(),this.axis=dn().scale(this.scaleCentered).tickSizeInner(4).tickSizeOuter(0).tickFormat((function(t){return e.tickFormat(e.invLinkFunction(t))})).tickPadding(-18),window.addEventListener(\"resize\",this.redraw),window.setTimeout(this.redraw,50)}},{key:\"componentDidUpdate\",value:function(){this.draw()}},{key:\"draw\",value:function(){var e=this;(0,Re.each)(this.props.featureNames,(function(t,n){e.props.features[n]&&(e.props.features[n].name=t)})),\"identity\"===this.props.link?this.invLinkFunction=function(t){return e.props.baseValue+t}:\"logit\"===this.props.link?this.invLinkFunction=function(t){return 1/(1+Math.exp(-(e.props.baseValue+t)))}:console.log(\"ERROR: Unrecognized link function: \",this.props.link);var t=this.svg.node().parentNode.offsetWidth;if(0==t)return setTimeout((function(){return e.draw(e.props)}),500);this.svg.style(\"height\",\"150px\"),this.svg.style(\"width\",t+\"px\");var n=(0,Re.sortBy)(this.props.features,(function(e){return-1/(e.effect+1e-10)})),r=(0,Re.sum)((0,Re.map)(n,(function(e){return Math.abs(e.effect)}))),a=(0,Re.sum)((0,Re.map)((0,Re.filter)(n,(function(e){return e.effect>0})),(function(e){return e.effect})))||0,i=(0,Re.sum)((0,Re.map)((0,Re.filter)(n,(function(e){return e.effect<0})),(function(e){return-e.effect})))||0;this.domainSize=3*Math.max(a,i);var o=De().domain([0,this.domainSize]).range([0,t]),u=t/2-o(i);this.scaleCentered.domain([-this.domainSize/2,this.domainSize/2]).range([0,t]).clamp(!0),this.axisElement.attr(\"transform\",\"translate(0,50)\").call(this.axis);var l,s,c,f=0;for(l=0;l<n.length;++l)n[l].x=f,n[l].effect<0&&void 0===s&&(s=f,c=l),f+=Math.abs(n[l].effect);void 0===s&&(s=f,c=l);var p=En().x((function(e){return e[0]})).y((function(e){return e[1]})),d=function(t){return void 0!==t.value&&null!==t.value&&\"\"!==t.value?t.name+\" = \"+(isNaN(t.value)?t.value:e.tickFormat(t.value)):t.name};n=this.props.hideBars?[]:n;var h=this.mainGroup.selectAll(\".force-bar-blocks\").data(n);h.enter().append(\"path\").attr(\"class\",\"force-bar-blocks\").merge(h).attr(\"d\",(function(e,t){var n=o(e.x)+u,r=o(Math.abs(e.effect)),a=e.effect<0?-4:4,i=a;return t===c&&(a=0),t===c-1&&(i=0),p([[n,56],[n+r,56],[n+r+i,64.5],[n+r,73],[n,73],[n+a,64.5]])})).attr(\"fill\",(function(t){return t.effect>0?e.colors[0]:e.colors[1]})).on(\"mouseover\",(function(t){if(o(Math.abs(t.effect))<o(r)/50||o(Math.abs(t.effect))<10){var n=o(t.x)+u,a=o(Math.abs(t.effect));e.hoverLabel.attr(\"opacity\",1).attr(\"x\",n+a/2).attr(\"y\",50.5).attr(\"fill\",t.effect>0?e.colors[0]:e.colors[1]).text(d(t)),e.hoverLabelBacking.attr(\"opacity\",1).attr(\"x\",n+a/2).attr(\"y\",50.5).text(d(t))}})).on(\"mouseout\",(function(){e.hoverLabel.attr(\"opacity\",0),e.hoverLabelBacking.attr(\"opacity\",0)})),h.exit().remove();var v=(0,Re.filter)(n,(function(e){return o(Math.abs(e.effect))>o(r)/50&&o(Math.abs(e.effect))>10})),g=this.onTopGroup.selectAll(\".force-bar-labels\").data(v);if(g.exit().remove(),g=g.enter().append(\"text\").attr(\"class\",\"force-bar-labels\").attr(\"font-size\",\"12px\").attr(\"y\",98).merge(g).text((function(t){return void 0!==t.value&&null!==t.value&&\"\"!==t.value?t.name+\" = \"+(isNaN(t.value)?t.value:e.tickFormat(t.value)):t.name})).attr(\"fill\",(function(t){return t.effect>0?e.colors[0]:e.colors[1]})).attr(\"stroke\",(function(e){return e.textWidth=Math.max(this.getComputedTextLength(),o(Math.abs(e.effect))-10),e.innerTextWidth=this.getComputedTextLength(),\"none\"})),this.filteredData=v,n.length>0){f=s+o.invert(5);for(var y=c;y<n.length;++y)n[y].textx=f,f+=o.invert(n[y].textWidth+10);f=s-o.invert(5);for(var m=c-1;m>=0;--m)n[m].textx=f,f-=o.invert(n[m].textWidth+10)}g.attr(\"x\",(function(e){return o(e.textx)+u+(e.effect>0?-e.textWidth/2:e.textWidth/2)})).attr(\"text-anchor\",\"middle\"),v=(0,Re.filter)(v,(function(n){return o(n.textx)+u>e.props.labelMargin&&o(n.textx)+u<t-e.props.labelMargin})),this.filteredData2=v;var b=v.slice(),_=(0,Re.findIndex)(n,v[0])-1;_>=0&&b.unshift(n[_]);var w=this.mainGroup.selectAll(\".force-bar-labelBacking\").data(v);w.enter().append(\"path\").attr(\"class\",\"force-bar-labelBacking\").attr(\"stroke\",\"none\").attr(\"opacity\",.2).merge(w).attr(\"d\",(function(e){return p([[o(e.x)+o(Math.abs(e.effect))+u,73],[(e.effect>0?o(e.textx):o(e.textx)+e.textWidth)+u+5,83],[(e.effect>0?o(e.textx):o(e.textx)+e.textWidth)+u+5,104],[(e.effect>0?o(e.textx)-e.textWidth:o(e.textx))+u-5,104],[(e.effect>0?o(e.textx)-e.textWidth:o(e.textx))+u-5,83],[o(e.x)+u,73]])})).attr(\"fill\",(function(e){return\"url(#linear-backgrad-\".concat(e.effect>0?0:1,\")\")})),w.exit().remove();var x=this.mainGroup.selectAll(\".force-bar-labelDividers\").data(v.slice(0,-1));x.enter().append(\"rect\").attr(\"class\",\"force-bar-labelDividers\").attr(\"height\",\"21px\").attr(\"width\",\"1px\").attr(\"y\",83).merge(x).attr(\"x\",(function(e){return(e.effect>0?o(e.textx):o(e.textx)+e.textWidth)+u+4.5})).attr(\"fill\",(function(e){return\"url(#linear-grad-\".concat(e.effect>0?0:1,\")\")})),x.exit().remove();var k=this.mainGroup.selectAll(\".force-bar-labelLinks\").data(v.slice(0,-1));k.enter().append(\"line\").attr(\"class\",\"force-bar-labelLinks\").attr(\"y1\",73).attr(\"y2\",83).attr(\"stroke-opacity\",.5).attr(\"stroke-width\",1).merge(k).attr(\"x1\",(function(e){return o(e.x)+o(Math.abs(e.effect))+u})).attr(\"x2\",(function(e){return(e.effect>0?o(e.textx):o(e.textx)+e.textWidth)+u+5})).attr(\"stroke\",(function(t){return t.effect>0?e.colors[0]:e.colors[1]})),k.exit().remove();var S=this.mainGroup.selectAll(\".force-bar-blockDividers\").data(n.slice(0,-1));S.enter().append(\"path\").attr(\"class\",\"force-bar-blockDividers\").attr(\"stroke-width\",2).attr(\"fill\",\"none\").merge(S).attr(\"d\",(function(e){var t=o(e.x)+o(Math.abs(e.effect))+u;return p([[t,56],[t+(e.effect<0?-4:4),64.5],[t,73]])})).attr(\"stroke\",(function(t,n){return c===n+1||Math.abs(t.effect)<1e-8?\"#rgba(0,0,0,0)\":t.effect>0?e.brighterColors[0]:e.brighterColors[1]})),S.exit().remove(),this.joinPointLine.attr(\"x1\",o(s)+u).attr(\"x2\",o(s)+u).attr(\"y1\",50).attr(\"y2\",56).attr(\"stroke\",\"#F2F2F2\").attr(\"stroke-width\",1).attr(\"opacity\",1),this.joinPointLabelOutline.attr(\"x\",o(s)+u).attr(\"y\",45).attr(\"color\",\"#fff\").attr(\"text-anchor\",\"middle\").attr(\"font-weight\",\"bold\").attr(\"stroke\",\"#fff\").attr(\"stroke-width\",6).text(ze(\",.2f\")(this.invLinkFunction(s-i))).attr(\"opacity\",1),console.log(\"joinPoint\",s,u,50,i),this.joinPointLabel.attr(\"x\",o(s)+u).attr(\"y\",45).attr(\"text-anchor\",\"middle\").attr(\"font-weight\",\"bold\").attr(\"fill\",\"#000\").text(ze(\",.2f\")(this.invLinkFunction(s-i))).attr(\"opacity\",1),this.joinPointTitle.attr(\"x\",o(s)+u).attr(\"y\",28).attr(\"text-anchor\",\"middle\").attr(\"font-size\",\"12\").attr(\"fill\",\"#000\").text(this.props.outNames[0]).attr(\"opacity\",.5),this.props.hideBars||(this.joinPointTitleLeft.attr(\"x\",o(s)+u-16).attr(\"y\",12).attr(\"text-anchor\",\"end\").attr(\"font-size\",\"13\").attr(\"fill\",this.colors[0]).text(\"higher\").attr(\"opacity\",1),this.joinPointTitleRight.attr(\"x\",o(s)+u+16).attr(\"y\",12).attr(\"text-anchor\",\"start\").attr(\"font-size\",\"13\").attr(\"fill\",this.colors[1]).text(\"lower\").attr(\"opacity\",1),this.joinPointTitleLeftArrow.attr(\"x\",o(s)+u+7).attr(\"y\",8).attr(\"text-anchor\",\"end\").attr(\"font-size\",\"13\").attr(\"fill\",this.colors[0]).text(\"→\").attr(\"opacity\",1),this.joinPointTitleRightArrow.attr(\"x\",o(s)+u-7).attr(\"y\",14).attr(\"text-anchor\",\"start\").attr(\"font-size\",\"13\").attr(\"fill\",this.colors[1]).text(\"←\").attr(\"opacity\",1)),this.props.hideBaseValueLabel||this.baseValueTitle.attr(\"x\",this.scaleCentered(0)).attr(\"y\",28).attr(\"text-anchor\",\"middle\").attr(\"font-size\",\"12\").attr(\"fill\",\"#000\").text(\"base value\").attr(\"opacity\",.5)}},{key:\"componentWillUnmount\",value:function(){window.removeEventListener(\"resize\",this.redraw)}},{key:\"render\",value:function(){var t=this;return e.createElement(\"svg\",{ref:function(e){return t.svg=Jt(e)},style:{userSelect:\"none\",display:\"block\",fontFamily:\"arial\",sansSerif:!0}},e.createElement(\"style\",{dangerouslySetInnerHTML:{__html:\"\\n          .force-bar-axis path {\\n            fill: none;\\n            opacity: 0.4;\\n          }\\n          .force-bar-axis paths {\\n            display: none;\\n          }\\n          .tick line {\\n            stroke: #000;\\n            stroke-width: 1px;\\n            opacity: 0.4;\\n          }\\n          .tick text {\\n            fill: #000;\\n            opacity: 0.5;\\n            font-size: 12px;\\n            padding: 0px;\\n          }\"}}))}}])&&Tn(n.prototype,r),Object.defineProperty(n,\"prototype\",{writable:!1}),u}(e.Component);zn.defaultProps={plot_cmap:\"RdBu\"};const Ln=zn,On=1e3,An=6e4,Fn=36e5,Dn=864e5,Rn=6048e5,jn=31536e6,Un=new Date,In=new Date;function $n(e,t,n,r){function a(t){return e(t=0===arguments.length?new Date:new Date(+t)),t}return a.floor=t=>(e(t=new Date(+t)),t),a.ceil=n=>(e(n=new Date(n-1)),t(n,1),e(n),n),a.round=e=>{const t=a(e),n=a.ceil(e);return e-t<n-e?t:n},a.offset=(e,n)=>(t(e=new Date(+e),null==n?1:Math.floor(n)),e),a.range=(n,r,i)=>{const o=[];if(n=a.ceil(n),i=null==i?1:Math.floor(i),!(n<r&&i>0))return o;let u;do{o.push(u=new Date(+n)),t(n,i),e(n)}while(u<n&&n<r);return o},a.filter=n=>$n((t=>{if(t>=t)for(;e(t),!n(t);)t.setTime(t-1)}),((e,r)=>{if(e>=e)if(r<0)for(;++r<=0;)for(;t(e,-1),!n(e););else for(;--r>=0;)for(;t(e,1),!n(e););})),n&&(a.count=(t,r)=>(Un.setTime(+t),In.setTime(+r),e(Un),e(In),Math.floor(n(Un,In))),a.every=e=>(e=Math.floor(e),isFinite(e)&&e>0?e>1?a.filter(r?t=>r(t)%e==0:t=>a.count(0,t)%e==0):a:null)),a}const Bn=$n((()=>{}),((e,t)=>{e.setTime(+e+t)}),((e,t)=>t-e));Bn.every=e=>(e=Math.floor(e),isFinite(e)&&e>0?e>1?$n((t=>{t.setTime(Math.floor(t/e)*e)}),((t,n)=>{t.setTime(+t+n*e)}),((t,n)=>(n-t)/e)):Bn:null),Bn.range;const Wn=$n((e=>{e.setTime(e-e.getMilliseconds())}),((e,t)=>{e.setTime(+e+t*On)}),((e,t)=>(t-e)/On),(e=>e.getUTCSeconds())),Vn=(Wn.range,$n((e=>{e.setTime(e-e.getMilliseconds()-e.getSeconds()*On)}),((e,t)=>{e.setTime(+e+t*An)}),((e,t)=>(t-e)/An),(e=>e.getMinutes()))),Hn=(Vn.range,$n((e=>{e.setUTCSeconds(0,0)}),((e,t)=>{e.setTime(+e+t*An)}),((e,t)=>(t-e)/An),(e=>e.getUTCMinutes()))),qn=(Hn.range,$n((e=>{e.setTime(e-e.getMilliseconds()-e.getSeconds()*On-e.getMinutes()*An)}),((e,t)=>{e.setTime(+e+t*Fn)}),((e,t)=>(t-e)/Fn),(e=>e.getHours()))),Qn=(qn.range,$n((e=>{e.setUTCMinutes(0,0,0)}),((e,t)=>{e.setTime(+e+t*Fn)}),((e,t)=>(t-e)/Fn),(e=>e.getUTCHours()))),Yn=(Qn.range,$n((e=>e.setHours(0,0,0,0)),((e,t)=>e.setDate(e.getDate()+t)),((e,t)=>(t-e-(t.getTimezoneOffset()-e.getTimezoneOffset())*An)/Dn),(e=>e.getDate()-1))),Gn=(Yn.range,$n((e=>{e.setUTCHours(0,0,0,0)}),((e,t)=>{e.setUTCDate(e.getUTCDate()+t)}),((e,t)=>(t-e)/Dn),(e=>e.getUTCDate()-1))),Kn=(Gn.range,$n((e=>{e.setUTCHours(0,0,0,0)}),((e,t)=>{e.setUTCDate(e.getUTCDate()+t)}),((e,t)=>(t-e)/Dn),(e=>Math.floor(e/Dn))));function Zn(e){return $n((t=>{t.setDate(t.getDate()-(t.getDay()+7-e)%7),t.setHours(0,0,0,0)}),((e,t)=>{e.setDate(e.getDate()+7*t)}),((e,t)=>(t-e-(t.getTimezoneOffset()-e.getTimezoneOffset())*An)/Rn))}Kn.range;const Xn=Zn(0),Jn=Zn(1),er=Zn(2),tr=Zn(3),nr=Zn(4),rr=Zn(5),ar=Zn(6);function ir(e){return $n((t=>{t.setUTCDate(t.getUTCDate()-(t.getUTCDay()+7-e)%7),t.setUTCHours(0,0,0,0)}),((e,t)=>{e.setUTCDate(e.getUTCDate()+7*t)}),((e,t)=>(t-e)/Rn))}Xn.range,Jn.range,er.range,tr.range,nr.range,rr.range,ar.range;const or=ir(0),ur=ir(1),lr=ir(2),sr=ir(3),cr=ir(4),fr=ir(5),pr=ir(6),dr=(or.range,ur.range,lr.range,sr.range,cr.range,fr.range,pr.range,$n((e=>{e.setDate(1),e.setHours(0,0,0,0)}),((e,t)=>{e.setMonth(e.getMonth()+t)}),((e,t)=>t.getMonth()-e.getMonth()+12*(t.getFullYear()-e.getFullYear())),(e=>e.getMonth()))),hr=(dr.range,$n((e=>{e.setUTCDate(1),e.setUTCHours(0,0,0,0)}),((e,t)=>{e.setUTCMonth(e.getUTCMonth()+t)}),((e,t)=>t.getUTCMonth()-e.getUTCMonth()+12*(t.getUTCFullYear()-e.getUTCFullYear())),(e=>e.getUTCMonth()))),vr=(hr.range,$n((e=>{e.setMonth(0,1),e.setHours(0,0,0,0)}),((e,t)=>{e.setFullYear(e.getFullYear()+t)}),((e,t)=>t.getFullYear()-e.getFullYear()),(e=>e.getFullYear())));vr.every=e=>isFinite(e=Math.floor(e))&&e>0?$n((t=>{t.setFullYear(Math.floor(t.getFullYear()/e)*e),t.setMonth(0,1),t.setHours(0,0,0,0)}),((t,n)=>{t.setFullYear(t.getFullYear()+n*e)})):null,vr.range;const gr=$n((e=>{e.setUTCMonth(0,1),e.setUTCHours(0,0,0,0)}),((e,t)=>{e.setUTCFullYear(e.getUTCFullYear()+t)}),((e,t)=>t.getUTCFullYear()-e.getUTCFullYear()),(e=>e.getUTCFullYear()));function yr(e,t,n,r,a,i){const o=[[Wn,1,On],[Wn,5,5e3],[Wn,15,15e3],[Wn,30,3e4],[i,1,An],[i,5,3e5],[i,15,9e5],[i,30,18e5],[a,1,Fn],[a,3,108e5],[a,6,216e5],[a,12,432e5],[r,1,Dn],[r,2,1728e5],[n,1,Rn],[t,1,2592e6],[t,3,7776e6],[e,1,jn]];function u(t,n,r){const a=Math.abs(n-t)/r,i=f((([,,e])=>e)).right(o,a);if(i===o.length)return e.every(l(t/jn,n/jn,r));if(0===i)return Bn.every(Math.max(l(t,n,r),1));const[u,s]=o[a/o[i-1][2]<o[i][2]/a?i-1:i];return u.every(s)}return[function(e,t,n){const r=t<e;r&&([e,t]=[t,e]);const a=n&&\"function\"==typeof n.range?n:u(e,t,n),i=a?a.range(e,+t+1):[];return r?i.reverse():i},u]}gr.every=e=>isFinite(e=Math.floor(e))&&e>0?$n((t=>{t.setUTCFullYear(Math.floor(t.getUTCFullYear()/e)*e),t.setUTCMonth(0,1),t.setUTCHours(0,0,0,0)}),((t,n)=>{t.setUTCFullYear(t.getUTCFullYear()+n*e)})):null,gr.range;const[mr,br]=yr(gr,hr,or,Kn,Qn,Hn),[_r,wr]=yr(vr,dr,Xn,Yn,qn,Vn);function xr(e){if(0<=e.y&&e.y<100){var t=new Date(-1,e.m,e.d,e.H,e.M,e.S,e.L);return t.setFullYear(e.y),t}return new Date(e.y,e.m,e.d,e.H,e.M,e.S,e.L)}function kr(e){if(0<=e.y&&e.y<100){var t=new Date(Date.UTC(-1,e.m,e.d,e.H,e.M,e.S,e.L));return t.setUTCFullYear(e.y),t}return new Date(Date.UTC(e.y,e.m,e.d,e.H,e.M,e.S,e.L))}function Sr(e,t,n){return{y:e,m:t,d:n,H:0,M:0,S:0,L:0}}var Er,Cr,Tr,Mr={\"-\":\"\",_:\" \",0:\"0\"},Nr=/^\\s*\\d+/,Pr=/^%/,zr=/[\\\\^$*+?|[\\]().{}]/g;function Lr(e,t,n){var r=e<0?\"-\":\"\",a=(r?-e:e)+\"\",i=a.length;return r+(i<n?new Array(n-i+1).join(t)+a:a)}function Or(e){return e.replace(zr,\"\\\\$&\")}function Ar(e){return new RegExp(\"^(?:\"+e.map(Or).join(\"|\")+\")\",\"i\")}function Fr(e){return new Map(e.map(((e,t)=>[e.toLowerCase(),t])))}function Dr(e,t,n){var r=Nr.exec(t.slice(n,n+1));return r?(e.w=+r[0],n+r[0].length):-1}function Rr(e,t,n){var r=Nr.exec(t.slice(n,n+1));return r?(e.u=+r[0],n+r[0].length):-1}function jr(e,t,n){var r=Nr.exec(t.slice(n,n+2));return r?(e.U=+r[0],n+r[0].length):-1}function Ur(e,t,n){var r=Nr.exec(t.slice(n,n+2));return r?(e.V=+r[0],n+r[0].length):-1}function Ir(e,t,n){var r=Nr.exec(t.slice(n,n+2));return r?(e.W=+r[0],n+r[0].length):-1}function $r(e,t,n){var r=Nr.exec(t.slice(n,n+4));return r?(e.y=+r[0],n+r[0].length):-1}function Br(e,t,n){var r=Nr.exec(t.slice(n,n+2));return r?(e.y=+r[0]+(+r[0]>68?1900:2e3),n+r[0].length):-1}function Wr(e,t,n){var r=/^(Z)|([+-]\\d\\d)(?::?(\\d\\d))?/.exec(t.slice(n,n+6));return r?(e.Z=r[1]?0:-(r[2]+(r[3]||\"00\")),n+r[0].length):-1}function Vr(e,t,n){var r=Nr.exec(t.slice(n,n+1));return r?(e.q=3*r[0]-3,n+r[0].length):-1}function Hr(e,t,n){var r=Nr.exec(t.slice(n,n+2));return r?(e.m=r[0]-1,n+r[0].length):-1}function qr(e,t,n){var r=Nr.exec(t.slice(n,n+2));return r?(e.d=+r[0],n+r[0].length):-1}function Qr(e,t,n){var r=Nr.exec(t.slice(n,n+3));return r?(e.m=0,e.d=+r[0],n+r[0].length):-1}function Yr(e,t,n){var r=Nr.exec(t.slice(n,n+2));return r?(e.H=+r[0],n+r[0].length):-1}function Gr(e,t,n){var r=Nr.exec(t.slice(n,n+2));return r?(e.M=+r[0],n+r[0].length):-1}function Kr(e,t,n){var r=Nr.exec(t.slice(n,n+2));return r?(e.S=+r[0],n+r[0].length):-1}function Zr(e,t,n){var r=Nr.exec(t.slice(n,n+3));return r?(e.L=+r[0],n+r[0].length):-1}function Xr(e,t,n){var r=Nr.exec(t.slice(n,n+6));return r?(e.L=Math.floor(r[0]/1e3),n+r[0].length):-1}function Jr(e,t,n){var r=Pr.exec(t.slice(n,n+1));return r?n+r[0].length:-1}function ea(e,t,n){var r=Nr.exec(t.slice(n));return r?(e.Q=+r[0],n+r[0].length):-1}function ta(e,t,n){var r=Nr.exec(t.slice(n));return r?(e.s=+r[0],n+r[0].length):-1}function na(e,t){return Lr(e.getDate(),t,2)}function ra(e,t){return Lr(e.getHours(),t,2)}function aa(e,t){return Lr(e.getHours()%12||12,t,2)}function ia(e,t){return Lr(1+Yn.count(vr(e),e),t,3)}function oa(e,t){return Lr(e.getMilliseconds(),t,3)}function ua(e,t){return oa(e,t)+\"000\"}function la(e,t){return Lr(e.getMonth()+1,t,2)}function sa(e,t){return Lr(e.getMinutes(),t,2)}function ca(e,t){return Lr(e.getSeconds(),t,2)}function fa(e){var t=e.getDay();return 0===t?7:t}function pa(e,t){return Lr(Xn.count(vr(e)-1,e),t,2)}function da(e){var t=e.getDay();return t>=4||0===t?nr(e):nr.ceil(e)}function ha(e,t){return e=da(e),Lr(nr.count(vr(e),e)+(4===vr(e).getDay()),t,2)}function va(e){return e.getDay()}function ga(e,t){return Lr(Jn.count(vr(e)-1,e),t,2)}function ya(e,t){return Lr(e.getFullYear()%100,t,2)}function ma(e,t){return Lr((e=da(e)).getFullYear()%100,t,2)}function ba(e,t){return Lr(e.getFullYear()%1e4,t,4)}function _a(e,t){var n=e.getDay();return Lr((e=n>=4||0===n?nr(e):nr.ceil(e)).getFullYear()%1e4,t,4)}function wa(e){var t=e.getTimezoneOffset();return(t>0?\"-\":(t*=-1,\"+\"))+Lr(t/60|0,\"0\",2)+Lr(t%60,\"0\",2)}function xa(e,t){return Lr(e.getUTCDate(),t,2)}function ka(e,t){return Lr(e.getUTCHours(),t,2)}function Sa(e,t){return Lr(e.getUTCHours()%12||12,t,2)}function Ea(e,t){return Lr(1+Gn.count(gr(e),e),t,3)}function Ca(e,t){return Lr(e.getUTCMilliseconds(),t,3)}function Ta(e,t){return Ca(e,t)+\"000\"}function Ma(e,t){return Lr(e.getUTCMonth()+1,t,2)}function Na(e,t){return Lr(e.getUTCMinutes(),t,2)}function Pa(e,t){return Lr(e.getUTCSeconds(),t,2)}function za(e){var t=e.getUTCDay();return 0===t?7:t}function La(e,t){return Lr(or.count(gr(e)-1,e),t,2)}function Oa(e){var t=e.getUTCDay();return t>=4||0===t?cr(e):cr.ceil(e)}function Aa(e,t){return e=Oa(e),Lr(cr.count(gr(e),e)+(4===gr(e).getUTCDay()),t,2)}function Fa(e){return e.getUTCDay()}function Da(e,t){return Lr(ur.count(gr(e)-1,e),t,2)}function Ra(e,t){return Lr(e.getUTCFullYear()%100,t,2)}function ja(e,t){return Lr((e=Oa(e)).getUTCFullYear()%100,t,2)}function Ua(e,t){return Lr(e.getUTCFullYear()%1e4,t,4)}function Ia(e,t){var n=e.getUTCDay();return Lr((e=n>=4||0===n?cr(e):cr.ceil(e)).getUTCFullYear()%1e4,t,4)}function $a(){return\"+0000\"}function Ba(){return\"%\"}function Wa(e){return+e}function Va(e){return Math.floor(+e/1e3)}function Ha(e){return new Date(e)}function qa(e){return e instanceof Date?+e:+new Date(+e)}function Qa(e,t,n,r,a,i,o,u,l,s){var c=be(),f=c.invert,p=c.domain,d=s(\".%L\"),h=s(\":%S\"),v=s(\"%I:%M\"),g=s(\"%I %p\"),y=s(\"%a %d\"),m=s(\"%b %d\"),b=s(\"%B\"),_=s(\"%Y\");function w(e){return(l(e)<e?d:u(e)<e?h:o(e)<e?v:i(e)<e?g:r(e)<e?a(e)<e?y:m:n(e)<e?b:_)(e)}return c.invert=function(e){return new Date(f(e))},c.domain=function(e){return arguments.length?p(Array.from(e,qa)):p().map(Ha)},c.ticks=function(t){var n=p();return e(n[0],n[n.length-1],null==t?10:t)},c.tickFormat=function(e,t){return null==t?w:s(t)},c.nice=function(e){var n=p();return e&&\"function\"==typeof e.range||(e=t(n[0],n[n.length-1],null==e?10:e)),e?p(function(e,t){var n,r=0,a=(e=e.slice()).length-1,i=e[r],o=e[a];return o<i&&(n=r,r=a,a=n,n=i,i=o,o=n),e[r]=t.floor(i),e[a]=t.ceil(o),e}(n,e)):c},c.copy=function(){return me(c,Qa(e,t,n,r,a,i,o,u,l,s))},c}function Ya(){return _e.apply(Qa(_r,wr,vr,dr,Xn,Yn,qn,Vn,Wn,Cr).domain([new Date(2e3,0,1),new Date(2e3,0,2)]),arguments)}function Ga(e,t){var n=\"undefined\"!=typeof Symbol&&e[Symbol.iterator]||e[\"@@iterator\"];if(!n){if(Array.isArray(e)||(n=function(e,t){if(e){if(\"string\"==typeof e)return Ka(e,t);var n=Object.prototype.toString.call(e).slice(8,-1);return\"Object\"===n&&e.constructor&&(n=e.constructor.name),\"Map\"===n||\"Set\"===n?Array.from(e):\"Arguments\"===n||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n)?Ka(e,t):void 0}}(e))||t&&e&&\"number\"==typeof e.length){n&&(e=n);var r=0,a=function(){};return{s:a,n:function(){return r>=e.length?{done:!0}:{done:!1,value:e[r++]}},e:function(e){throw e},f:a}}throw new TypeError(\"Invalid attempt to iterate non-iterable instance.\\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.\")}var i,o=!0,u=!1;return{s:function(){n=n.call(e)},n:function(){var e=n.next();return o=e.done,e},e:function(e){u=!0,i=e},f:function(){try{o||null==n.return||n.return()}finally{if(u)throw i}}}}function Ka(e,t){(null==t||t>e.length)&&(t=e.length);for(var n=0,r=new Array(t);n<t;n++)r[n]=e[n];return r}function Za(e){return Za=\"function\"==typeof Symbol&&\"symbol\"==typeof Symbol.iterator?function(e){return typeof e}:function(e){return e&&\"function\"==typeof Symbol&&e.constructor===Symbol&&e!==Symbol.prototype?\"symbol\":typeof e},Za(e)}function Xa(e,t){for(var n=0;n<t.length;n++){var r=t[n];r.enumerable=r.enumerable||!1,r.configurable=!0,\"value\"in r&&(r.writable=!0),Object.defineProperty(e,(void 0,a=function(e,t){if(\"object\"!==Za(e)||null===e)return e;var n=e[Symbol.toPrimitive];if(void 0!==n){var r=n.call(e,\"string\");if(\"object\"!==Za(r))return r;throw new TypeError(\"@@toPrimitive must return a primitive value.\")}return String(e)}(r.key),\"symbol\"===Za(a)?a:String(a)),r)}var a}function Ja(e,t){return Ja=Object.setPrototypeOf?Object.setPrototypeOf.bind():function(e,t){return e.__proto__=t,e},Ja(e,t)}function ei(e){if(void 0===e)throw new ReferenceError(\"this hasn't been initialised - super() hasn't been called\");return e}function ti(e){return ti=Object.setPrototypeOf?Object.getPrototypeOf.bind():function(e){return e.__proto__||Object.getPrototypeOf(e)},ti(e)}Er=function(e){var t=e.dateTime,n=e.date,r=e.time,a=e.periods,i=e.days,o=e.shortDays,u=e.months,l=e.shortMonths,s=Ar(a),c=Fr(a),f=Ar(i),p=Fr(i),d=Ar(o),h=Fr(o),v=Ar(u),g=Fr(u),y=Ar(l),m=Fr(l),b={a:function(e){return o[e.getDay()]},A:function(e){return i[e.getDay()]},b:function(e){return l[e.getMonth()]},B:function(e){return u[e.getMonth()]},c:null,d:na,e:na,f:ua,g:ma,G:_a,H:ra,I:aa,j:ia,L:oa,m:la,M:sa,p:function(e){return a[+(e.getHours()>=12)]},q:function(e){return 1+~~(e.getMonth()/3)},Q:Wa,s:Va,S:ca,u:fa,U:pa,V:ha,w:va,W:ga,x:null,X:null,y:ya,Y:ba,Z:wa,\"%\":Ba},_={a:function(e){return o[e.getUTCDay()]},A:function(e){return i[e.getUTCDay()]},b:function(e){return l[e.getUTCMonth()]},B:function(e){return u[e.getUTCMonth()]},c:null,d:xa,e:xa,f:Ta,g:ja,G:Ia,H:ka,I:Sa,j:Ea,L:Ca,m:Ma,M:Na,p:function(e){return a[+(e.getUTCHours()>=12)]},q:function(e){return 1+~~(e.getUTCMonth()/3)},Q:Wa,s:Va,S:Pa,u:za,U:La,V:Aa,w:Fa,W:Da,x:null,X:null,y:Ra,Y:Ua,Z:$a,\"%\":Ba},w={a:function(e,t,n){var r=d.exec(t.slice(n));return r?(e.w=h.get(r[0].toLowerCase()),n+r[0].length):-1},A:function(e,t,n){var r=f.exec(t.slice(n));return r?(e.w=p.get(r[0].toLowerCase()),n+r[0].length):-1},b:function(e,t,n){var r=y.exec(t.slice(n));return r?(e.m=m.get(r[0].toLowerCase()),n+r[0].length):-1},B:function(e,t,n){var r=v.exec(t.slice(n));return r?(e.m=g.get(r[0].toLowerCase()),n+r[0].length):-1},c:function(e,n,r){return S(e,t,n,r)},d:qr,e:qr,f:Xr,g:Br,G:$r,H:Yr,I:Yr,j:Qr,L:Zr,m:Hr,M:Gr,p:function(e,t,n){var r=s.exec(t.slice(n));return r?(e.p=c.get(r[0].toLowerCase()),n+r[0].length):-1},q:Vr,Q:ea,s:ta,S:Kr,u:Rr,U:jr,V:Ur,w:Dr,W:Ir,x:function(e,t,r){return S(e,n,t,r)},X:function(e,t,n){return S(e,r,t,n)},y:Br,Y:$r,Z:Wr,\"%\":Jr};function x(e,t){return function(n){var r,a,i,o=[],u=-1,l=0,s=e.length;for(n instanceof Date||(n=new Date(+n));++u<s;)37===e.charCodeAt(u)&&(o.push(e.slice(l,u)),null!=(a=Mr[r=e.charAt(++u)])?r=e.charAt(++u):a=\"e\"===r?\" \":\"0\",(i=t[r])&&(r=i(n,a)),o.push(r),l=u+1);return o.push(e.slice(l,u)),o.join(\"\")}}function k(e,t){return function(n){var r,a,i=Sr(1900,void 0,1);if(S(i,e,n+=\"\",0)!=n.length)return null;if(\"Q\"in i)return new Date(i.Q);if(\"s\"in i)return new Date(1e3*i.s+(\"L\"in i?i.L:0));if(t&&!(\"Z\"in i)&&(i.Z=0),\"p\"in i&&(i.H=i.H%12+12*i.p),void 0===i.m&&(i.m=\"q\"in i?i.q:0),\"V\"in i){if(i.V<1||i.V>53)return null;\"w\"in i||(i.w=1),\"Z\"in i?(a=(r=kr(Sr(i.y,0,1))).getUTCDay(),r=a>4||0===a?ur.ceil(r):ur(r),r=Gn.offset(r,7*(i.V-1)),i.y=r.getUTCFullYear(),i.m=r.getUTCMonth(),i.d=r.getUTCDate()+(i.w+6)%7):(a=(r=xr(Sr(i.y,0,1))).getDay(),r=a>4||0===a?Jn.ceil(r):Jn(r),r=Yn.offset(r,7*(i.V-1)),i.y=r.getFullYear(),i.m=r.getMonth(),i.d=r.getDate()+(i.w+6)%7)}else(\"W\"in i||\"U\"in i)&&(\"w\"in i||(i.w=\"u\"in i?i.u%7:\"W\"in i?1:0),a=\"Z\"in i?kr(Sr(i.y,0,1)).getUTCDay():xr(Sr(i.y,0,1)).getDay(),i.m=0,i.d=\"W\"in i?(i.w+6)%7+7*i.W-(a+5)%7:i.w+7*i.U-(a+6)%7);return\"Z\"in i?(i.H+=i.Z/100|0,i.M+=i.Z%100,kr(i)):xr(i)}}function S(e,t,n,r){for(var a,i,o=0,u=t.length,l=n.length;o<u;){if(r>=l)return-1;if(37===(a=t.charCodeAt(o++))){if(a=t.charAt(o++),!(i=w[a in Mr?t.charAt(o++):a])||(r=i(e,n,r))<0)return-1}else if(a!=n.charCodeAt(r++))return-1}return r}return b.x=x(n,b),b.X=x(r,b),b.c=x(t,b),_.x=x(n,_),_.X=x(r,_),_.c=x(t,_),{format:function(e){var t=x(e+=\"\",b);return t.toString=function(){return e},t},parse:function(e){var t=k(e+=\"\",!1);return t.toString=function(){return e},t},utcFormat:function(e){var t=x(e+=\"\",_);return t.toString=function(){return e},t},utcParse:function(e){var t=k(e+=\"\",!0);return t.toString=function(){return e},t}}}({dateTime:\"%x, %X\",date:\"%-m/%-d/%Y\",time:\"%-I:%M:%S %p\",periods:[\"AM\",\"PM\"],days:[\"Sunday\",\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\"],shortDays:[\"Sun\",\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\"],months:[\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"],shortMonths:[\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]}),Cr=Er.format,Tr=Er.parse,Er.utcFormat,Er.utcParse;var ni=function(t){!function(e,t){if(\"function\"!=typeof t&&null!==t)throw new TypeError(\"Super expression must either be null or a function\");e.prototype=Object.create(t&&t.prototype,{constructor:{value:e,writable:!0,configurable:!0}}),Object.defineProperty(e,\"prototype\",{writable:!1}),t&&Ja(e,t)}(u,t);var n,r,a,i,o=(a=u,i=function(){if(\"undefined\"==typeof Reflect||!Reflect.construct)return!1;if(Reflect.construct.sham)return!1;if(\"function\"==typeof Proxy)return!0;try{return Boolean.prototype.valueOf.call(Reflect.construct(Boolean,[],(function(){}))),!0}catch(e){return!1}}(),function(){var e,t=ti(a);if(i){var n=ti(this).constructor;e=Reflect.construct(t,arguments,n)}else e=t.apply(this,arguments);return function(e,t){if(t&&(\"object\"===Za(t)||\"function\"==typeof t))return t;if(void 0!==t)throw new TypeError(\"Derived constructors may only return object or undefined\");return ei(e)}(this,e)});function u(){var e;return function(e,t){if(!(e instanceof t))throw new TypeError(\"Cannot call a class as a function\")}(this,u),e=o.call(this),window.lastAdditiveForceArrayVisualizer=ei(e),e.topOffset=28,e.leftOffset=80,e.height=350,e.effectFormat=ze(\".2\"),e.redraw=(0,Re.debounce)((function(){return e.draw()}),200),e}return n=u,(r=[{key:\"componentDidMount\",value:function(){var e=this;this.mainGroup=this.svg.append(\"g\"),this.onTopGroup=this.svg.append(\"g\"),this.xaxisElement=this.onTopGroup.append(\"g\").attr(\"transform\",\"translate(0,35)\").attr(\"class\",\"force-bar-array-xaxis\"),this.yaxisElement=this.onTopGroup.append(\"g\").attr(\"transform\",\"translate(0,35)\").attr(\"class\",\"force-bar-array-yaxis\"),this.hoverGroup1=this.svg.append(\"g\"),this.hoverGroup2=this.svg.append(\"g\"),this.baseValueTitle=this.svg.append(\"text\"),this.hoverLine=this.svg.append(\"line\"),this.hoverxOutline=this.svg.append(\"text\").attr(\"text-anchor\",\"middle\").attr(\"font-weight\",\"bold\").attr(\"fill\",\"#fff\").attr(\"stroke\",\"#fff\").attr(\"stroke-width\",\"6\").attr(\"font-size\",\"12px\"),this.hoverx=this.svg.append(\"text\").attr(\"text-anchor\",\"middle\").attr(\"font-weight\",\"bold\").attr(\"fill\",\"#000\").attr(\"font-size\",\"12px\"),this.hoverxTitle=this.svg.append(\"text\").attr(\"text-anchor\",\"middle\").attr(\"opacity\",.6).attr(\"font-size\",\"12px\"),this.hoveryOutline=this.svg.append(\"text\").attr(\"text-anchor\",\"end\").attr(\"font-weight\",\"bold\").attr(\"fill\",\"#fff\").attr(\"stroke\",\"#fff\").attr(\"stroke-width\",\"6\").attr(\"font-size\",\"12px\"),this.hovery=this.svg.append(\"text\").attr(\"text-anchor\",\"end\").attr(\"font-weight\",\"bold\").attr(\"fill\",\"#000\").attr(\"font-size\",\"12px\"),this.xlabel=this.wrapper.select(\".additive-force-array-xlabel\"),this.ylabel=this.wrapper.select(\".additive-force-array-ylabel\");var t=void 0;\"string\"==typeof this.props.plot_cmap?this.props.plot_cmap in je.colors?t=je.colors[this.props.plot_cmap]:(console.log(\"Invalid color map name, reverting to default.\"),t=je.colors.RdBu):Array.isArray(this.props.plot_cmap)&&(t=this.props.plot_cmap),this.colors=t.map((function(e){return q(e)})),this.brighterColors=[1.45,1.6].map((function(t,n){return e.colors[n].brighter(t)}));var n=ze(\",.4\");null!=this.props.ordering_keys&&null!=this.props.ordering_keys_time_format?(this.parseTime=Tr(this.props.ordering_keys_time_format),this.formatTime=Cr(this.props.ordering_keys_time_format),this.xtickFormat=function(e){return\"object\"==Za(e)?this.formatTime(e):n(e)}):(this.parseTime=null,this.formatTime=null,this.xtickFormat=n),this.xscale=De(),this.xaxis=dn().scale(this.xscale).tickSizeInner(4).tickSizeOuter(0).tickFormat((function(t){return e.xtickFormat(t)})).tickPadding(-18),this.ytickFormat=n,this.yscale=De(),this.yaxis=pn(an,undefined).scale(this.yscale).tickSizeInner(4).tickSizeOuter(0).tickFormat((function(t){return e.ytickFormat(e.invLinkFunction(t))})).tickPadding(2),this.xlabel.node().onchange=function(){return e.internalDraw()},this.ylabel.node().onchange=function(){return e.internalDraw()},this.svg.on(\"mousemove\",(function(t){return e.mouseMoved(t)})),this.svg.on(\"click\",(function(){return alert(\"This original index of the sample you clicked is \"+e.nearestExpIndex)})),this.svg.on(\"mouseout\",(function(t){return e.mouseOut(t)})),window.addEventListener(\"resize\",this.redraw),window.setTimeout(this.redraw,50)}},{key:\"componentDidUpdate\",value:function(){this.draw()}},{key:\"mouseOut\",value:function(){this.hoverLine.attr(\"display\",\"none\"),this.hoverx.attr(\"display\",\"none\"),this.hoverxOutline.attr(\"display\",\"none\"),this.hoverxTitle.attr(\"display\",\"none\"),this.hovery.attr(\"display\",\"none\"),this.hoveryOutline.attr(\"display\",\"none\"),this.hoverGroup1.attr(\"display\",\"none\"),this.hoverGroup2.attr(\"display\",\"none\")}},{key:\"mouseMoved\",value:function(e){var t,n,r=this;this.hoverLine.attr(\"display\",\"\"),this.hoverx.attr(\"display\",\"\"),this.hoverxOutline.attr(\"display\",\"\"),this.hoverxTitle.attr(\"display\",\"\"),this.hovery.attr(\"display\",\"\"),this.hoveryOutline.attr(\"display\",\"\"),this.hoverGroup1.attr(\"display\",\"\"),this.hoverGroup2.attr(\"display\",\"\");var a=function(e,t){if(e=function(e){let t;for(;t=e.sourceEvent;)e=t;return e}(e),void 0===t&&(t=e.currentTarget),t){var n=t.ownerSVGElement||t;if(n.createSVGPoint){var r=n.createSVGPoint();return r.x=e.clientX,r.y=e.clientY,[(r=r.matrixTransform(t.getScreenCTM().inverse())).x,r.y]}if(t.getBoundingClientRect){var a=t.getBoundingClientRect();return[e.clientX-a.left-t.clientLeft,e.clientY-a.top-t.clientTop]}}return[e.pageX,e.pageY]}(e,this.svg.node())[0];if(this.props.explanations){for(t=0;t<this.currExplanations.length;++t)(!n||Math.abs(n.xmapScaled-a)>Math.abs(this.currExplanations[t].xmapScaled-a))&&(n=this.currExplanations[t]);this.nearestExpIndex=n.origInd,this.hoverLine.attr(\"x1\",n.xmapScaled).attr(\"x2\",n.xmapScaled).attr(\"y1\",0+this.topOffset).attr(\"y2\",this.height),this.hoverx.attr(\"x\",n.xmapScaled).attr(\"y\",this.topOffset-5).text(this.xtickFormat(n.xmap)),this.hoverxOutline.attr(\"x\",n.xmapScaled).attr(\"y\",this.topOffset-5).text(this.xtickFormat(n.xmap)),this.hoverxTitle.attr(\"x\",n.xmapScaled).attr(\"y\",this.topOffset-18).text(n.count>1?n.count+\" averaged samples\":\"\"),this.hovery.attr(\"x\",this.leftOffset-6).attr(\"y\",n.joinPointy).text(this.ytickFormat(this.invLinkFunction(n.joinPoint))),this.hoveryOutline.attr(\"x\",this.leftOffset-6).attr(\"y\",n.joinPointy).text(this.ytickFormat(this.invLinkFunction(n.joinPoint)));for(var i,o,u=[],l=this.currPosOrderedFeatures.length-1;l>=0;--l){var s=this.currPosOrderedFeatures[l],c=n.features[s];o=5+(c.posyTop+c.posyBottom)/2,(!i||o-i>=15)&&c.posyTop-c.posyBottom>=6&&(u.push(c),i=o)}var f=[];i=void 0;var p,d=Ga(this.currNegOrderedFeatures);try{for(d.s();!(p=d.n()).done;){var h=p.value,v=n.features[h];o=5+(v.negyTop+v.negyBottom)/2,(!i||i-o>=15)&&v.negyTop-v.negyBottom>=6&&(f.push(v),i=o)}}catch(e){d.e(e)}finally{d.f()}var g=function(e){var t=\"\";return null!==e.value&&void 0!==e.value&&(t=\" = \"+(isNaN(e.value)?e.value:r.ytickFormat(e.value))),n.count>1?\"mean(\"+r.props.featureNames[e.ind]+\")\"+t:r.props.featureNames[e.ind]+t},y=this.hoverGroup1.selectAll(\".pos-values\").data(u);y.enter().append(\"text\").attr(\"class\",\"pos-values\").merge(y).attr(\"x\",n.xmapScaled+5).attr(\"y\",(function(e){return 4+(e.posyTop+e.posyBottom)/2})).attr(\"text-anchor\",\"start\").attr(\"font-size\",12).attr(\"stroke\",\"#fff\").attr(\"fill\",\"#fff\").attr(\"stroke-width\",\"4\").attr(\"stroke-linejoin\",\"round\").attr(\"opacity\",1).text(g),y.exit().remove();var m=this.hoverGroup2.selectAll(\".pos-values\").data(u);m.enter().append(\"text\").attr(\"class\",\"pos-values\").merge(m).attr(\"x\",n.xmapScaled+5).attr(\"y\",(function(e){return 4+(e.posyTop+e.posyBottom)/2})).attr(\"text-anchor\",\"start\").attr(\"font-size\",12).attr(\"fill\",this.colors[0]).text(g),m.exit().remove();var b=this.hoverGroup1.selectAll(\".neg-values\").data(f);b.enter().append(\"text\").attr(\"class\",\"neg-values\").merge(b).attr(\"x\",n.xmapScaled+5).attr(\"y\",(function(e){return 4+(e.negyTop+e.negyBottom)/2})).attr(\"text-anchor\",\"start\").attr(\"font-size\",12).attr(\"stroke\",\"#fff\").attr(\"fill\",\"#fff\").attr(\"stroke-width\",\"4\").attr(\"stroke-linejoin\",\"round\").attr(\"opacity\",1).text(g),b.exit().remove();var _=this.hoverGroup2.selectAll(\".neg-values\").data(f);_.enter().append(\"text\").attr(\"class\",\"neg-values\").merge(_).attr(\"x\",n.xmapScaled+5).attr(\"y\",(function(e){return 4+(e.negyTop+e.negyBottom)/2})).attr(\"text-anchor\",\"start\").attr(\"font-size\",12).attr(\"fill\",this.colors[1]).text(g),_.exit().remove()}}},{key:\"draw\",value:function(){var e=this;if(this.props.explanations&&0!==this.props.explanations.length){(0,Re.each)(this.props.explanations,(function(e,t){return e.origInd=t}));var t,n={},r={},a={},i=Ga(this.props.explanations);try{for(i.s();!(t=i.n()).done;){var o=t.value;for(var u in o.features)void 0===n[u]&&(n[u]=0,r[u]=0,a[u]=0),o.features[u].effect>0?n[u]+=o.features[u].effect:r[u]-=o.features[u].effect,null!==o.features[u].value&&void 0!==o.features[u].value&&(a[u]+=1)}}catch(e){i.e(e)}finally{i.f()}this.usedFeatures=(0,Re.sortBy)((0,Re.keys)(n),(function(e){return-(n[e]+r[e])})),console.log(\"found \",this.usedFeatures.length,\" used features\"),this.posOrderedFeatures=(0,Re.sortBy)(this.usedFeatures,(function(e){return n[e]})),this.negOrderedFeatures=(0,Re.sortBy)(this.usedFeatures,(function(e){return-r[e]})),this.singleValueFeatures=(0,Re.filter)(this.usedFeatures,(function(e){return a[e]>0}));var l=[\"sample order by similarity\",\"sample order by output value\",\"original sample ordering\"].concat(this.singleValueFeatures.map((function(t){return e.props.featureNames[t]})));null!=this.props.ordering_keys&&l.unshift(\"sample order by key\");var s=this.xlabel.selectAll(\"option\").data(l);s.enter().append(\"option\").merge(s).attr(\"value\",(function(e){return e})).text((function(e){return e})),s.exit().remove();var c=this.props.outNames[0]?this.props.outNames[0]:\"model output value\";(l=(0,Re.map)(this.usedFeatures,(function(t){return[e.props.featureNames[t],e.props.featureNames[t]+\" effects\"]}))).unshift([\"model output value\",c]);var f=this.ylabel.selectAll(\"option\").data(l);f.enter().append(\"option\").merge(f).attr(\"value\",(function(e){return e[0]})).text((function(e){return e[1]})),f.exit().remove(),this.ylabel.style(\"top\",(this.height-10-this.topOffset)/2+this.topOffset+\"px\").style(\"left\",10-this.ylabel.node().offsetWidth/2+\"px\"),this.internalDraw()}}},{key:\"internalDraw\",value:function(){var e,t,n=this,r=Ga(this.props.explanations);try{for(r.s();!(e=r.n()).done;){var a,i=e.value,o=Ga(this.usedFeatures);try{for(o.s();!(a=o.n()).done;){var u=a.value;i.features.hasOwnProperty(u)||(i.features[u]={effect:0,value:0}),i.features[u].ind=u}}catch(e){o.e(e)}finally{o.f()}}}catch(e){r.e(e)}finally{r.f()}var l=this.xlabel.node().value,s=\"sample order by key\"===l&&null!=this.props.ordering_keys_time_format;if(this.xscale=s?Ya():De(),this.xaxis.scale(this.xscale),\"sample order by similarity\"===l)t=(0,Re.sortBy)(this.props.explanations,(function(e){return e.simIndex})),(0,Re.each)(t,(function(e,t){return e.xmap=t}));else if(\"sample order by output value\"===l)t=(0,Re.sortBy)(this.props.explanations,(function(e){return-e.outValue})),(0,Re.each)(t,(function(e,t){return e.xmap=t}));else if(\"original sample ordering\"===l)t=(0,Re.sortBy)(this.props.explanations,(function(e){return e.origInd})),(0,Re.each)(t,(function(e,t){return e.xmap=t}));else if(\"sample order by key\"===l)t=this.props.explanations,s?(0,Re.each)(t,(function(e,t){return e.xmap=n.parseTime(n.props.ordering_keys[t])})):(0,Re.each)(t,(function(e,t){return e.xmap=n.props.ordering_keys[t]})),t=(0,Re.sortBy)(t,(function(e){return e.xmap}));else{var c=(0,Re.findKey)(this.props.featureNames,(function(e){return e===l}));(0,Re.each)(this.props.explanations,(function(e,t){return e.xmap=e.features[c].value}));var f=(0,Re.sortBy)(this.props.explanations,(function(e){return e.xmap})),p=(0,Re.map)(f,(function(e){return e.xmap}));if(\"string\"==typeof p[0])return void alert(\"Ordering by category names is not yet supported.\");var d,h,v=(0,Re.min)(p),g=((0,Re.max)(p)-v)/100;t=[];for(var y=0;y<f.length;++y){var m=f[y];if(d&&!h&&m.xmap-d.xmap<=g||h&&m.xmap-h.xmap<=g){h||((h=(0,Re.cloneDeep)(d)).count=1);var b,_=Ga(this.usedFeatures);try{for(_.s();!(b=_.n()).done;){var w=b.value;h.features[w].effect+=m.features[w].effect,h.features[w].value+=m.features[w].value}}catch(e){_.e(e)}finally{_.f()}h.count+=1}else if(d)if(h){var x,k=Ga(this.usedFeatures);try{for(k.s();!(x=k.n()).done;){var S=x.value;h.features[S].effect/=h.count,h.features[S].value/=h.count}}catch(e){k.e(e)}finally{k.f()}t.push(h),h=void 0}else t.push(d);d=m}d.xmap-t[t.length-1].xmap>g&&t.push(d)}this.currUsedFeatures=this.usedFeatures,this.currPosOrderedFeatures=this.posOrderedFeatures,this.currNegOrderedFeatures=this.negOrderedFeatures;var E=this.ylabel.node().value;if(\"model output value\"!==E){var C=t;t=(0,Re.cloneDeep)(t);for(var T=(0,Re.findKey)(this.props.featureNames,(function(e){return e===E})),M=0;M<t.length;++M){var N=t[M].features[T];t[M].features={},t[M].features[T]=N,C[M].remapped_version=t[M]}this.currUsedFeatures=[T],this.currPosOrderedFeatures=[T],this.currNegOrderedFeatures=[T]}this.currExplanations=t,\"identity\"===this.props.link?this.invLinkFunction=function(e){return n.props.baseValue+e}:\"logit\"===this.props.link?this.invLinkFunction=function(e){return 1/(1+Math.exp(-(n.props.baseValue+e)))}:console.log(\"ERROR: Unrecognized link function: \",this.props.link),this.predValues=(0,Re.map)(t,(function(e){return(0,Re.sum)((0,Re.map)(e.features,(function(e){return e.effect})))}));var P=this.wrapper.node().offsetWidth;if(0==P)return setTimeout((function(){return n.draw(t)}),500);this.svg.style(\"height\",this.height+\"px\"),this.svg.style(\"width\",P+\"px\");var z=(0,Re.map)(t,(function(e){return e.xmap}));this.xscale.domain([(0,Re.min)(z),(0,Re.max)(z)]).range([this.leftOffset,P]).clamp(!0),this.xaxisElement.attr(\"transform\",\"translate(0,\"+this.topOffset+\")\").call(this.xaxis);for(var L=0;L<this.currExplanations.length;++L)this.currExplanations[L].xmapScaled=this.xscale(this.currExplanations[L].xmap);for(var O=t.length,A=0,F=0;F<O;++F){var D=t[F].features,R=(0,Re.sum)((0,Re.map)((0,Re.filter)(D,(function(e){return e.effect>0})),(function(e){return e.effect})))||0,j=(0,Re.sum)((0,Re.map)((0,Re.filter)(D,(function(e){return e.effect<0})),(function(e){return-e.effect})))||0;A=Math.max(A,2.2*Math.max(R,j))}this.yscale.domain([-A/2,A/2]).range([this.height-10,this.topOffset]),this.yaxisElement.attr(\"transform\",\"translate(\"+this.leftOffset+\",0)\").call(this.yaxis);for(var U=0;U<O;++U){var I,$=t[U].features,B=-((0,Re.sum)((0,Re.map)((0,Re.filter)($,(function(e){return e.effect<0})),(function(e){return-e.effect})))||0),W=void 0,V=Ga(this.currPosOrderedFeatures);try{for(V.s();!(I=V.n()).done;)$[W=I.value].posyTop=this.yscale(B),$[W].effect>0&&(B+=$[W].effect),$[W].posyBottom=this.yscale(B),$[W].ind=W}catch(e){V.e(e)}finally{V.f()}var H,q=B,Q=Ga(this.currNegOrderedFeatures);try{for(Q.s();!(H=Q.n()).done;)$[W=H.value].negyTop=this.yscale(B),$[W].effect<0&&(B-=$[W].effect),$[W].negyBottom=this.yscale(B)}catch(e){Q.e(e)}finally{Q.f()}t[U].joinPoint=q,t[U].joinPointy=this.yscale(q)}var Y=En().x((function(e){return e[0]})).y((function(e){return e[1]})),G=this.mainGroup.selectAll(\".force-bar-array-area-pos\").data(this.currUsedFeatures);G.enter().append(\"path\").attr(\"class\",\"force-bar-array-area-pos\").merge(G).attr(\"d\",(function(e){var n=(0,Re.map)((0,Re.range)(O),(function(n){return[t[n].xmapScaled,t[n].features[e].posyTop]})),r=(0,Re.map)((0,Re.rangeRight)(O),(function(n){return[t[n].xmapScaled,t[n].features[e].posyBottom]}));return Y(n.concat(r))})).attr(\"fill\",this.colors[0]),G.exit().remove();var K=this.mainGroup.selectAll(\".force-bar-array-area-neg\").data(this.currUsedFeatures);K.enter().append(\"path\").attr(\"class\",\"force-bar-array-area-neg\").merge(K).attr(\"d\",(function(e){var n=(0,Re.map)((0,Re.range)(O),(function(n){return[t[n].xmapScaled,t[n].features[e].negyTop]})),r=(0,Re.map)((0,Re.rangeRight)(O),(function(n){return[t[n].xmapScaled,t[n].features[e].negyBottom]}));return Y(n.concat(r))})).attr(\"fill\",this.colors[1]),K.exit().remove();var Z=this.mainGroup.selectAll(\".force-bar-array-divider-pos\").data(this.currUsedFeatures);Z.enter().append(\"path\").attr(\"class\",\"force-bar-array-divider-pos\").merge(Z).attr(\"d\",(function(e){var n=(0,Re.map)((0,Re.range)(O),(function(n){return[t[n].xmapScaled,t[n].features[e].posyBottom]}));return Y(n)})).attr(\"fill\",\"none\").attr(\"stroke-width\",1).attr(\"stroke\",(function(){return n.colors[0].brighter(1.2)})),Z.exit().remove();var X=this.mainGroup.selectAll(\".force-bar-array-divider-neg\").data(this.currUsedFeatures);X.enter().append(\"path\").attr(\"class\",\"force-bar-array-divider-neg\").merge(X).attr(\"d\",(function(e){var n=(0,Re.map)((0,Re.range)(O),(function(n){return[t[n].xmapScaled,t[n].features[e].negyTop]}));return Y(n)})).attr(\"fill\",\"none\").attr(\"stroke-width\",1).attr(\"stroke\",(function(){return n.colors[1].brighter(1.5)})),X.exit().remove();for(var J=function(e,t,n,r,a){var i,o,u,l;\"pos\"===a?(i=e[n].features[t].posyBottom,o=e[n].features[t].posyTop):(i=e[n].features[t].negyBottom,o=e[n].features[t].negyTop);for(var s=n+1;s<=r;++s)\"pos\"===a?(u=e[s].features[t].posyBottom,l=e[s].features[t].posyTop):(u=e[s].features[t].negyBottom,l=e[s].features[t].negyTop),u>i&&(i=u),l<o&&(o=l);return{top:i,bottom:o}},ee=[],te=0,ne=[\"pos\",\"neg\"];te<ne.length;te++){var re,ae=ne[te],ie=Ga(this.currUsedFeatures);try{for(ie.s();!(re=ie.n()).done;)for(var oe=re.value,ue=0,le=0,se=0,ce={top:0,bottom:0},fe=void 0;le<O-1;){for(;se<100&&le<O-1;)++le,se=t[le].xmapScaled-t[ue].xmapScaled;for(ce=J(t,oe,ue,le,ae);ce.bottom-ce.top<20&&ue<le;)++ue,ce=J(t,oe,ue,le,ae);if(se=t[le].xmapScaled-t[ue].xmapScaled,ce.bottom-ce.top>=20&&se>=100){for(;le<O-1;){if(++le,!((fe=J(t,oe,ue,le,ae)).bottom-fe.top>20)){--le;break}ce=fe}se=t[le].xmapScaled-t[ue].xmapScaled,ee.push([(t[le].xmapScaled+t[ue].xmapScaled)/2,(ce.top+ce.bottom)/2,this.props.featureNames[oe]]);var pe=t[le].xmapScaled;for(ue=le;pe+100>t[ue].xmapScaled&&ue<O-1;)++ue;le=ue}}}catch(e){ie.e(e)}finally{ie.f()}}var de=this.onTopGroup.selectAll(\".force-bar-array-flabels\").data(ee);de.enter().append(\"text\").attr(\"class\",\"force-bar-array-flabels\").merge(de).attr(\"x\",(function(e){return e[0]})).attr(\"y\",(function(e){return e[1]+4})).text((function(e){return e[2]})),de.exit().remove()}},{key:\"componentWillUnmount\",value:function(){window.removeEventListener(\"resize\",this.redraw)}},{key:\"render\",value:function(){var t=this;return e.createElement(\"div\",{ref:function(e){return t.wrapper=Jt(e)},style:{textAlign:\"center\"}},e.createElement(\"style\",{dangerouslySetInnerHTML:{__html:\"\\n          .force-bar-array-wrapper {\\n            text-align: center;\\n          }\\n          .force-bar-array-xaxis path {\\n            fill: none;\\n            opacity: 0.4;\\n          }\\n          .force-bar-array-xaxis .domain {\\n            opacity: 0;\\n          }\\n          .force-bar-array-xaxis paths {\\n            display: none;\\n          }\\n          .force-bar-array-yaxis path {\\n            fill: none;\\n            opacity: 0.4;\\n          }\\n          .force-bar-array-yaxis paths {\\n            display: none;\\n          }\\n          .tick line {\\n            stroke: #000;\\n            stroke-width: 1px;\\n            opacity: 0.4;\\n          }\\n          .tick text {\\n            fill: #000;\\n            opacity: 0.5;\\n            font-size: 12px;\\n            padding: 0px;\\n          }\\n          .force-bar-array-flabels {\\n            font-size: 12px;\\n            fill: #fff;\\n            text-anchor: middle;\\n          }\\n          .additive-force-array-xlabel {\\n            background: none;\\n            border: 1px solid #ccc;\\n            opacity: 0.5;\\n            margin-bottom: 0px;\\n            font-size: 12px;\\n            font-family: arial;\\n            margin-left: 80px;\\n            max-width: 300px;\\n          }\\n          .additive-force-array-xlabel:focus {\\n            outline: none;\\n          }\\n          .additive-force-array-ylabel {\\n            position: relative;\\n            top: 0px;\\n            left: 0px;\\n            transform: rotate(-90deg);\\n            background: none;\\n            border: 1px solid #ccc;\\n            opacity: 0.5;\\n            margin-bottom: 0px;\\n            font-size: 12px;\\n            font-family: arial;\\n            max-width: 150px;\\n          }\\n          .additive-force-array-ylabel:focus {\\n            outline: none;\\n          }\\n          .additive-force-array-hoverLine {\\n            stroke-width: 1px;\\n            stroke: #fff;\\n            opacity: 1;\\n          }\"}}),e.createElement(\"select\",{className:\"additive-force-array-xlabel\"}),e.createElement(\"div\",{style:{height:\"0px\",textAlign:\"left\"}},e.createElement(\"select\",{className:\"additive-force-array-ylabel\"})),e.createElement(\"svg\",{ref:function(e){return t.svg=Jt(e)},style:{userSelect:\"none\",display:\"block\",fontFamily:\"arial\",sansSerif:!0}}))}}])&&Xa(n.prototype,r),Object.defineProperty(n,\"prototype\",{writable:!1}),u}(e.Component);ni.defaultProps={plot_cmap:\"RdBu\",ordering_keys:null,ordering_keys_time_format:null};const ri=ni;window.SHAP={SimpleListVisualizer:He,AdditiveForceVisualizer:Ln,AdditiveForceArrayVisualizer:ri,React:e,ReactDom:t}})()})();\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ----Tratamiento de datos---\n",
    "import functions as func\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "# ----Modelado del dataset----\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# ----Modelo y entrenamiento----\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgbm\n",
    "\n",
    "\n",
    "# ----Metricas y visualización de gráficos----\n",
    "from sklearn.metrics import classification_report, confusion_matrix, auc, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "# ----Warnings----\n",
    "import warnings\n",
    "warnings.filterwarnings('once')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -> Optimizar hiperparametros y mejores variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable a modificar si se quiere realizar una busqueda de los mejores hiperparametros y de las mejores variables a utilizar o usar las ya existentes.\n",
    "realizar_tuneo_HP_y_RFECV=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Carga de datos\n",
    "+ Carga del dataset en memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SumeriO\\AppData\\Local\\Temp\\ipykernel_20816\\158176516.py:1: DtypeWarning: Columns (28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataset_df=pd.read_csv(\"../Datasets/MasterBI_Train.csv\")\n"
     ]
    }
   ],
   "source": [
    "dataset_df=pd.read_csv(\"../Datasets/MasterBI_Train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. EDA\n",
    "En esta sección se hará un analisis sobre los datos para ver que los componen, como tipos de datos, existencia de nulos y valores únicos, además de comprobar si se necesita Imbalance Learning.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Información sobre los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Train dataset--\n",
      "Rows:892148\n",
      "Columns:83\n"
     ]
    }
   ],
   "source": [
    "print(f\"--Train dataset--\\nRows:{dataset_df.shape[0]}\\nColumns:{dataset_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MachineIdentifier</th>\n",
       "      <th>ProductName</th>\n",
       "      <th>EngineVersion</th>\n",
       "      <th>AppVersion</th>\n",
       "      <th>AvSigVersion</th>\n",
       "      <th>IsBeta</th>\n",
       "      <th>RtpStateBitfield</th>\n",
       "      <th>IsSxsPassiveMode</th>\n",
       "      <th>DefaultBrowsersIdentifier</th>\n",
       "      <th>AVProductStatesIdentifier</th>\n",
       "      <th>...</th>\n",
       "      <th>Census_FirmwareVersionIdentifier</th>\n",
       "      <th>Census_IsSecureBootEnabled</th>\n",
       "      <th>Census_IsWIMBootEnabled</th>\n",
       "      <th>Census_IsVirtualDevice</th>\n",
       "      <th>Census_IsTouchEnabled</th>\n",
       "      <th>Census_IsPenCapable</th>\n",
       "      <th>Census_IsAlwaysOnAlwaysConnectedCapable</th>\n",
       "      <th>Wdft_IsGamer</th>\n",
       "      <th>Wdft_RegionIdentifier</th>\n",
       "      <th>HasDetections</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7bd132f88d90f8c628ae7204ce1e0038</td>\n",
       "      <td>win8defender</td>\n",
       "      <td>1.1.15100.1</td>\n",
       "      <td>4.18.1807.18075</td>\n",
       "      <td>1.273.1486.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53447.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12300.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12678cb674ee9f2c85e526dc9774845d</td>\n",
       "      <td>win8defender</td>\n",
       "      <td>1.1.15100.1</td>\n",
       "      <td>4.18.1807.18075</td>\n",
       "      <td>1.273.1234.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53447.0</td>\n",
       "      <td>...</td>\n",
       "      <td>70187.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>624400f847781fe7ccc3546e921fb46f</td>\n",
       "      <td>win8defender</td>\n",
       "      <td>1.1.15000.2</td>\n",
       "      <td>4.18.1806.18062</td>\n",
       "      <td>1.271.545.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53447.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13909.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5dd6c5c23e09214f43fbe89888bf98f7</td>\n",
       "      <td>win8defender</td>\n",
       "      <td>1.1.15100.1</td>\n",
       "      <td>4.11.15063.0</td>\n",
       "      <td>1.273.912.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53447.0</td>\n",
       "      <td>...</td>\n",
       "      <td>33319.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5d601bdcff9df13a981c468325429ba0</td>\n",
       "      <td>win8defender</td>\n",
       "      <td>1.1.15200.1</td>\n",
       "      <td>4.18.1807.18075</td>\n",
       "      <td>1.275.1318.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53447.0</td>\n",
       "      <td>...</td>\n",
       "      <td>70290.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>adc6e41cee2ae561c1942926a4b86f3f</td>\n",
       "      <td>win8defender</td>\n",
       "      <td>1.1.15200.1</td>\n",
       "      <td>4.18.1807.18075</td>\n",
       "      <td>1.275.697.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7945.0</td>\n",
       "      <td>...</td>\n",
       "      <td>33165.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>420f0de9b1af3f6a00c9cf7610d08f0e</td>\n",
       "      <td>win8defender</td>\n",
       "      <td>1.1.15100.1</td>\n",
       "      <td>4.14.17639.18041</td>\n",
       "      <td>1.273.692.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53447.0</td>\n",
       "      <td>...</td>\n",
       "      <td>70299.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>d741f09bbcdd041f1cfe72f069b83b3e</td>\n",
       "      <td>win8defender</td>\n",
       "      <td>1.1.14901.4</td>\n",
       "      <td>4.14.17639.18041</td>\n",
       "      <td>1.269.272.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62773.0</td>\n",
       "      <td>...</td>\n",
       "      <td>11778.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>d543a3ef8787c0222b456ac50e755582</td>\n",
       "      <td>win8defender</td>\n",
       "      <td>1.1.15100.1</td>\n",
       "      <td>4.16.17656.18052</td>\n",
       "      <td>1.273.810.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7945.0</td>\n",
       "      <td>...</td>\n",
       "      <td>69919.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8abea91ba204f59a67a95729bb1300dd</td>\n",
       "      <td>win8defender</td>\n",
       "      <td>1.1.15200.1</td>\n",
       "      <td>4.18.1807.18075</td>\n",
       "      <td>1.275.727.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53447.0</td>\n",
       "      <td>...</td>\n",
       "      <td>19970.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  MachineIdentifier   ProductName EngineVersion  \\\n",
       "0  7bd132f88d90f8c628ae7204ce1e0038  win8defender   1.1.15100.1   \n",
       "1  12678cb674ee9f2c85e526dc9774845d  win8defender   1.1.15100.1   \n",
       "2  624400f847781fe7ccc3546e921fb46f  win8defender   1.1.15000.2   \n",
       "3  5dd6c5c23e09214f43fbe89888bf98f7  win8defender   1.1.15100.1   \n",
       "4  5d601bdcff9df13a981c468325429ba0  win8defender   1.1.15200.1   \n",
       "5  adc6e41cee2ae561c1942926a4b86f3f  win8defender   1.1.15200.1   \n",
       "6  420f0de9b1af3f6a00c9cf7610d08f0e  win8defender   1.1.15100.1   \n",
       "7  d741f09bbcdd041f1cfe72f069b83b3e  win8defender   1.1.14901.4   \n",
       "8  d543a3ef8787c0222b456ac50e755582  win8defender   1.1.15100.1   \n",
       "9  8abea91ba204f59a67a95729bb1300dd  win8defender   1.1.15200.1   \n",
       "\n",
       "         AppVersion  AvSigVersion  IsBeta  RtpStateBitfield  IsSxsPassiveMode  \\\n",
       "0   4.18.1807.18075  1.273.1486.0       0               7.0                 0   \n",
       "1   4.18.1807.18075  1.273.1234.0       0               7.0                 0   \n",
       "2   4.18.1806.18062   1.271.545.0       0               7.0                 0   \n",
       "3      4.11.15063.0   1.273.912.0       0               7.0                 0   \n",
       "4   4.18.1807.18075  1.275.1318.0       0               7.0                 0   \n",
       "5   4.18.1807.18075   1.275.697.0       0               7.0                 0   \n",
       "6  4.14.17639.18041   1.273.692.0       0               7.0                 0   \n",
       "7  4.14.17639.18041   1.269.272.0       0               7.0                 0   \n",
       "8  4.16.17656.18052   1.273.810.0       0               7.0                 0   \n",
       "9   4.18.1807.18075   1.275.727.0       0               7.0                 0   \n",
       "\n",
       "   DefaultBrowsersIdentifier  AVProductStatesIdentifier  ...  \\\n",
       "0                        NaN                    53447.0  ...   \n",
       "1                        NaN                    53447.0  ...   \n",
       "2                        NaN                    53447.0  ...   \n",
       "3                        NaN                    53447.0  ...   \n",
       "4                        NaN                    53447.0  ...   \n",
       "5                        NaN                     7945.0  ...   \n",
       "6                        NaN                    53447.0  ...   \n",
       "7                        NaN                    62773.0  ...   \n",
       "8                        NaN                     7945.0  ...   \n",
       "9                        NaN                    53447.0  ...   \n",
       "\n",
       "   Census_FirmwareVersionIdentifier  Census_IsSecureBootEnabled  \\\n",
       "0                           12300.0                           1   \n",
       "1                           70187.0                           0   \n",
       "2                           13909.0                           1   \n",
       "3                           33319.0                           0   \n",
       "4                           70290.0                           1   \n",
       "5                           33165.0                           1   \n",
       "6                           70299.0                           1   \n",
       "7                           11778.0                           1   \n",
       "8                           69919.0                           1   \n",
       "9                           19970.0                           1   \n",
       "\n",
       "   Census_IsWIMBootEnabled  Census_IsVirtualDevice  Census_IsTouchEnabled  \\\n",
       "0                      NaN                     0.0                      0   \n",
       "1                      NaN                     0.0                      0   \n",
       "2                      0.0                     0.0                      0   \n",
       "3                      NaN                     0.0                      0   \n",
       "4                      NaN                     0.0                      0   \n",
       "5                      NaN                     0.0                      0   \n",
       "6                      0.0                     0.0                      0   \n",
       "7                      0.0                     0.0                      0   \n",
       "8                      NaN                     0.0                      0   \n",
       "9                      0.0                     0.0                      0   \n",
       "\n",
       "   Census_IsPenCapable  Census_IsAlwaysOnAlwaysConnectedCapable  Wdft_IsGamer  \\\n",
       "0                    0                                      0.0           0.0   \n",
       "1                    0                                      0.0           1.0   \n",
       "2                    0                                      0.0           0.0   \n",
       "3                    0                                      0.0           1.0   \n",
       "4                    0                                      0.0           1.0   \n",
       "5                    0                                      0.0           0.0   \n",
       "6                    0                                      0.0           1.0   \n",
       "7                    0                                      0.0           0.0   \n",
       "8                    0                                      0.0           0.0   \n",
       "9                    0                                      0.0           1.0   \n",
       "\n",
       "  Wdft_RegionIdentifier HasDetections  \n",
       "0                   6.0             1  \n",
       "1                   4.0             0  \n",
       "2                  10.0             1  \n",
       "3                  10.0             1  \n",
       "4                  15.0             0  \n",
       "5                  10.0             0  \n",
       "6                  11.0             1  \n",
       "7                  10.0             1  \n",
       "8                  11.0             1  \n",
       "9                   3.0             1  \n",
       "\n",
       "[10 rows x 83 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 892148 entries, 0 to 892147\n",
      "Data columns (total 83 columns):\n",
      " #   Column                                             Non-Null Count   Dtype  \n",
      "---  ------                                             --------------   -----  \n",
      " 0   MachineIdentifier                                  892148 non-null  object \n",
      " 1   ProductName                                        892148 non-null  object \n",
      " 2   EngineVersion                                      892148 non-null  object \n",
      " 3   AppVersion                                         892148 non-null  object \n",
      " 4   AvSigVersion                                       892148 non-null  object \n",
      " 5   IsBeta                                             892148 non-null  int64  \n",
      " 6   RtpStateBitfield                                   888846 non-null  float64\n",
      " 7   IsSxsPassiveMode                                   892148 non-null  int64  \n",
      " 8   DefaultBrowsersIdentifier                          43369 non-null   float64\n",
      " 9   AVProductStatesIdentifier                          888564 non-null  float64\n",
      " 10  AVProductsInstalled                                888564 non-null  float64\n",
      " 11  AVProductsEnabled                                  888564 non-null  float64\n",
      " 12  HasTpm                                             892148 non-null  int64  \n",
      " 13  CountryIdentifier                                  892148 non-null  int64  \n",
      " 14  CityIdentifier                                     859124 non-null  float64\n",
      " 15  OrganizationIdentifier                             616605 non-null  float64\n",
      " 16  GeoNameIdentifier                                  892131 non-null  float64\n",
      " 17  LocaleEnglishNameIdentifier                        892148 non-null  int64  \n",
      " 18  Platform                                           892148 non-null  object \n",
      " 19  Processor                                          892148 non-null  object \n",
      " 20  OsVer                                              892148 non-null  object \n",
      " 21  OsBuild                                            892148 non-null  int64  \n",
      " 22  OsSuite                                            892148 non-null  int64  \n",
      " 23  OsPlatformSubRelease                               892148 non-null  object \n",
      " 24  OsBuildLab                                         892148 non-null  object \n",
      " 25  SkuEdition                                         892148 non-null  object \n",
      " 26  IsProtected                                        888586 non-null  float64\n",
      " 27  AutoSampleOptIn                                    892148 non-null  int64  \n",
      " 28  PuaMode                                            223 non-null     object \n",
      " 29  SMode                                              838350 non-null  float64\n",
      " 30  IeVerIdentifier                                    886309 non-null  float64\n",
      " 31  SmartScreen                                        574786 non-null  object \n",
      " 32  Firewall                                           883049 non-null  float64\n",
      " 33  UacLuaenable                                       890987 non-null  float64\n",
      " 34  Census_MDC2FormFactor                              892148 non-null  object \n",
      " 35  Census_DeviceFamily                                892148 non-null  object \n",
      " 36  Census_OEMNameIdentifier                           882496 non-null  float64\n",
      " 37  Census_OEMModelIdentifier                          881821 non-null  float64\n",
      " 38  Census_ProcessorCoreCount                          887964 non-null  float64\n",
      " 39  Census_ProcessorManufacturerIdentifier             887964 non-null  float64\n",
      " 40  Census_ProcessorModelIdentifier                    887959 non-null  float64\n",
      " 41  Census_ProcessorClass                              3731 non-null    object \n",
      " 42  Census_PrimaryDiskTotalCapacity                    886813 non-null  float64\n",
      " 43  Census_PrimaryDiskTypeName                         890851 non-null  object \n",
      " 44  Census_SystemVolumeTotalCapacity                   886816 non-null  float64\n",
      " 45  Census_HasOpticalDiskDrive                         892148 non-null  int64  \n",
      " 46  Census_TotalPhysicalRAM                            884057 non-null  float64\n",
      " 47  Census_ChassisTypeName                             892079 non-null  object \n",
      " 48  Census_InternalPrimaryDiagonalDisplaySizeInInches  887556 non-null  float64\n",
      " 49  Census_InternalPrimaryDisplayResolutionHorizontal  887568 non-null  float64\n",
      " 50  Census_InternalPrimaryDisplayResolutionVertical    887568 non-null  float64\n",
      " 51  Census_PowerPlatformRoleName                       892142 non-null  object \n",
      " 52  Census_InternalBatteryType                         258161 non-null  object \n",
      " 53  Census_InternalBatteryNumberOfCharges              865086 non-null  float64\n",
      " 54  Census_OSVersion                                   892148 non-null  object \n",
      " 55  Census_OSArchitecture                              892148 non-null  object \n",
      " 56  Census_OSBranch                                    892148 non-null  object \n",
      " 57  Census_OSBuildNumber                               892148 non-null  int64  \n",
      " 58  Census_OSBuildRevision                             892148 non-null  int64  \n",
      " 59  Census_OSEdition                                   892148 non-null  object \n",
      " 60  Census_OSSkuName                                   892148 non-null  object \n",
      " 61  Census_OSInstallTypeName                           892148 non-null  object \n",
      " 62  Census_OSInstallLanguageIdentifier                 886071 non-null  float64\n",
      " 63  Census_OSUILocaleIdentifier                        892148 non-null  int64  \n",
      " 64  Census_OSWUAutoUpdateOptionsName                   892148 non-null  object \n",
      " 65  Census_IsPortableOperatingSystem                   892148 non-null  int64  \n",
      " 66  Census_GenuineStateName                            892148 non-null  object \n",
      " 67  Census_ActivationChannel                           892148 non-null  object \n",
      " 68  Census_IsFlightingInternal                         151099 non-null  float64\n",
      " 69  Census_IsFlightsDisabled                           876198 non-null  float64\n",
      " 70  Census_FlightRing                                  892148 non-null  object \n",
      " 71  Census_ThresholdOptIn                              325283 non-null  float64\n",
      " 72  Census_FirmwareManufacturerIdentifier              873771 non-null  float64\n",
      " 73  Census_FirmwareVersionIdentifier                   876065 non-null  float64\n",
      " 74  Census_IsSecureBootEnabled                         892148 non-null  int64  \n",
      " 75  Census_IsWIMBootEnabled                            326030 non-null  float64\n",
      " 76  Census_IsVirtualDevice                             890504 non-null  float64\n",
      " 77  Census_IsTouchEnabled                              892148 non-null  int64  \n",
      " 78  Census_IsPenCapable                                892148 non-null  int64  \n",
      " 79  Census_IsAlwaysOnAlwaysConnectedCapable            884915 non-null  float64\n",
      " 80  Wdft_IsGamer                                       861721 non-null  float64\n",
      " 81  Wdft_RegionIdentifier                              861721 non-null  float64\n",
      " 82  HasDetections                                      892148 non-null  int64  \n",
      "dtypes: float64(36), int64(17), object(30)\n",
      "memory usage: 564.9+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Nulls count---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6070297"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---Nulls count---\")\n",
    "dataset_df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Uniques count--\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MachineIdentifier                                    892148\n",
       "ProductName                                               5\n",
       "EngineVersion                                            55\n",
       "AppVersion                                               95\n",
       "AvSigVersion                                           7242\n",
       "IsBeta                                                    2\n",
       "RtpStateBitfield                                          7\n",
       "IsSxsPassiveMode                                          2\n",
       "DefaultBrowsersIdentifier                               719\n",
       "AVProductStatesIdentifier                              7546\n",
       "AVProductsInstalled                                       6\n",
       "AVProductsEnabled                                         6\n",
       "HasTpm                                                    2\n",
       "CountryIdentifier                                       222\n",
       "CityIdentifier                                        48331\n",
       "OrganizationIdentifier                                   44\n",
       "GeoNameIdentifier                                       276\n",
       "LocaleEnglishNameIdentifier                             249\n",
       "Platform                                                  4\n",
       "Processor                                                 3\n",
       "OsVer                                                    20\n",
       "OsBuild                                                  51\n",
       "OsSuite                                                  10\n",
       "OsPlatformSubRelease                                      9\n",
       "OsBuildLab                                              494\n",
       "SkuEdition                                                8\n",
       "IsProtected                                               2\n",
       "AutoSampleOptIn                                           2\n",
       "PuaMode                                                   1\n",
       "SMode                                                     2\n",
       "IeVerIdentifier                                         203\n",
       "SmartScreen                                              11\n",
       "Firewall                                                  2\n",
       "UacLuaenable                                              6\n",
       "Census_MDC2FormFactor                                    12\n",
       "Census_DeviceFamily                                       3\n",
       "Census_OEMNameIdentifier                               1934\n",
       "Census_OEMModelIdentifier                             56345\n",
       "Census_ProcessorCoreCount                                29\n",
       "Census_ProcessorManufacturerIdentifier                    4\n",
       "Census_ProcessorModelIdentifier                        2455\n",
       "Census_ProcessorClass                                     3\n",
       "Census_PrimaryDiskTotalCapacity                        1499\n",
       "Census_PrimaryDiskTypeName                                4\n",
       "Census_SystemVolumeTotalCapacity                     199720\n",
       "Census_HasOpticalDiskDrive                                2\n",
       "Census_TotalPhysicalRAM                                 823\n",
       "Census_ChassisTypeName                                   39\n",
       "Census_InternalPrimaryDiagonalDisplaySizeInInches       564\n",
       "Census_InternalPrimaryDisplayResolutionHorizontal       676\n",
       "Census_InternalPrimaryDisplayResolutionVertical         676\n",
       "Census_PowerPlatformRoleName                              9\n",
       "Census_InternalBatteryType                               37\n",
       "Census_InternalBatteryNumberOfCharges                  8049\n",
       "Census_OSVersion                                        328\n",
       "Census_OSArchitecture                                     3\n",
       "Census_OSBranch                                          17\n",
       "Census_OSBuildNumber                                     77\n",
       "Census_OSBuildRevision                                  246\n",
       "Census_OSEdition                                         24\n",
       "Census_OSSkuName                                         20\n",
       "Census_OSInstallTypeName                                  9\n",
       "Census_OSInstallLanguageIdentifier                       39\n",
       "Census_OSUILocaleIdentifier                             107\n",
       "Census_OSWUAutoUpdateOptionsName                          6\n",
       "Census_IsPortableOperatingSystem                          2\n",
       "Census_GenuineStateName                                   4\n",
       "Census_ActivationChannel                                  6\n",
       "Census_IsFlightingInternal                                2\n",
       "Census_IsFlightsDisabled                                  2\n",
       "Census_FlightRing                                         8\n",
       "Census_ThresholdOptIn                                     2\n",
       "Census_FirmwareManufacturerIdentifier                   354\n",
       "Census_FirmwareVersionIdentifier                      28186\n",
       "Census_IsSecureBootEnabled                                2\n",
       "Census_IsWIMBootEnabled                                   1\n",
       "Census_IsVirtualDevice                                    2\n",
       "Census_IsTouchEnabled                                     2\n",
       "Census_IsPenCapable                                       2\n",
       "Census_IsAlwaysOnAlwaysConnectedCapable                   2\n",
       "Wdft_IsGamer                                              2\n",
       "Wdft_RegionIdentifier                                    15\n",
       "HasDetections                                             2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"--Uniques count--\")\n",
    "dataset_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificación del id y el label\n",
    "id=\"MachineIdentifier\"\n",
    "label=\"HasDetections\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HasDetections\n",
       "0    0.500207\n",
       "1    0.499793\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comprobar si se necesita imbalance learning\n",
    "dataset_df[label].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Transformaciones\n",
    "+ Eliminación del Id\n",
    "+ Selección de variables - Metodo manual\n",
    "+ Transformación manual de valores y tipos de datos\n",
    "+ Creacion de variables de máscara y contextuales\n",
    "+ División de los datos en train y test\n",
    "+ Imputación de nulos \n",
    "    + Simple Imputer (por la media para valores numéricos y valor mas frecuente para valores categoricos)\n",
    "+ Aplicación de encoders\n",
    "    + OneHot Encoder (variables con 5 o menos valores únicos)\n",
    "    + Target Encoder (variables con mas de 5 valores únicos)\n",
    "+ Tuneo de hiperparametros\n",
    "+ Selección de variables con RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminación del ID por no aportar ningun valor al entrenamiento\n",
    "dataset_df=dataset_df.drop(columns=id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selección de variables - Metodo manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Top variables por porcentaje de nulos del total---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Unique_values</th>\n",
       "      <th>Percentage of missing values</th>\n",
       "      <th>Percentage of values in the biggest category</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>PuaMode</td>\n",
       "      <td>1</td>\n",
       "      <td>99.975004</td>\n",
       "      <td>99.975004</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Census_ProcessorClass</td>\n",
       "      <td>3</td>\n",
       "      <td>99.581796</td>\n",
       "      <td>99.581796</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DefaultBrowsersIdentifier</td>\n",
       "      <td>719</td>\n",
       "      <td>95.138811</td>\n",
       "      <td>95.138811</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Census_IsFlightingInternal</td>\n",
       "      <td>2</td>\n",
       "      <td>83.063460</td>\n",
       "      <td>83.063460</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Census_InternalBatteryType</td>\n",
       "      <td>37</td>\n",
       "      <td>71.062985</td>\n",
       "      <td>71.062985</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Census_ThresholdOptIn</td>\n",
       "      <td>2</td>\n",
       "      <td>63.539345</td>\n",
       "      <td>63.539345</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Census_IsWIMBootEnabled</td>\n",
       "      <td>1</td>\n",
       "      <td>63.455615</td>\n",
       "      <td>63.455615</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>SmartScreen</td>\n",
       "      <td>11</td>\n",
       "      <td>35.572797</td>\n",
       "      <td>48.387151</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>OrganizationIdentifier</td>\n",
       "      <td>44</td>\n",
       "      <td>30.885346</td>\n",
       "      <td>46.970794</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>SMode</td>\n",
       "      <td>2</td>\n",
       "      <td>6.030165</td>\n",
       "      <td>93.923542</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CityIdentifier</td>\n",
       "      <td>48331</td>\n",
       "      <td>3.701628</td>\n",
       "      <td>3.701628</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Wdft_IsGamer</td>\n",
       "      <td>2</td>\n",
       "      <td>3.410533</td>\n",
       "      <td>69.258800</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Wdft_RegionIdentifier</td>\n",
       "      <td>15</td>\n",
       "      <td>3.410533</td>\n",
       "      <td>20.195640</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Census_InternalBatteryNumberOfCharges</td>\n",
       "      <td>8049</td>\n",
       "      <td>3.033353</td>\n",
       "      <td>56.532661</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Census_FirmwareManufacturerIdentifier</td>\n",
       "      <td>354</td>\n",
       "      <td>2.059860</td>\n",
       "      <td>30.328824</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Census_FirmwareVersionIdentifier</td>\n",
       "      <td>28186</td>\n",
       "      <td>1.802728</td>\n",
       "      <td>1.802728</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Census_IsFlightsDisabled</td>\n",
       "      <td>2</td>\n",
       "      <td>1.787820</td>\n",
       "      <td>98.211508</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Census_OEMModelIdentifier</td>\n",
       "      <td>56345</td>\n",
       "      <td>1.157543</td>\n",
       "      <td>3.430373</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Census_OEMNameIdentifier</td>\n",
       "      <td>1934</td>\n",
       "      <td>1.081883</td>\n",
       "      <td>14.443456</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Firewall</td>\n",
       "      <td>2</td>\n",
       "      <td>1.019898</td>\n",
       "      <td>96.838081</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Feature  Unique_values  \\\n",
       "27                                PuaMode              1   \n",
       "40                  Census_ProcessorClass              3   \n",
       "7               DefaultBrowsersIdentifier            719   \n",
       "67             Census_IsFlightingInternal              2   \n",
       "51             Census_InternalBatteryType             37   \n",
       "70                  Census_ThresholdOptIn              2   \n",
       "74                Census_IsWIMBootEnabled              1   \n",
       "30                            SmartScreen             11   \n",
       "14                 OrganizationIdentifier             44   \n",
       "28                                  SMode              2   \n",
       "13                         CityIdentifier          48331   \n",
       "79                           Wdft_IsGamer              2   \n",
       "80                  Wdft_RegionIdentifier             15   \n",
       "52  Census_InternalBatteryNumberOfCharges           8049   \n",
       "71  Census_FirmwareManufacturerIdentifier            354   \n",
       "72       Census_FirmwareVersionIdentifier          28186   \n",
       "68               Census_IsFlightsDisabled              2   \n",
       "36              Census_OEMModelIdentifier          56345   \n",
       "35               Census_OEMNameIdentifier           1934   \n",
       "31                               Firewall              2   \n",
       "\n",
       "    Percentage of missing values  \\\n",
       "27                     99.975004   \n",
       "40                     99.581796   \n",
       "7                      95.138811   \n",
       "67                     83.063460   \n",
       "51                     71.062985   \n",
       "70                     63.539345   \n",
       "74                     63.455615   \n",
       "30                     35.572797   \n",
       "14                     30.885346   \n",
       "28                      6.030165   \n",
       "13                      3.701628   \n",
       "79                      3.410533   \n",
       "80                      3.410533   \n",
       "52                      3.033353   \n",
       "71                      2.059860   \n",
       "72                      1.802728   \n",
       "68                      1.787820   \n",
       "36                      1.157543   \n",
       "35                      1.081883   \n",
       "31                      1.019898   \n",
       "\n",
       "    Percentage of values in the biggest category     type  \n",
       "27                                     99.975004   object  \n",
       "40                                     99.581796   object  \n",
       "7                                      95.138811  float64  \n",
       "67                                     83.063460  float64  \n",
       "51                                     71.062985   object  \n",
       "70                                     63.539345  float64  \n",
       "74                                     63.455615  float64  \n",
       "30                                     48.387151   object  \n",
       "14                                     46.970794  float64  \n",
       "28                                     93.923542  float64  \n",
       "13                                      3.701628  float64  \n",
       "79                                     69.258800  float64  \n",
       "80                                     20.195640  float64  \n",
       "52                                     56.532661  float64  \n",
       "71                                     30.328824  float64  \n",
       "72                                      1.802728  float64  \n",
       "68                                     98.211508  float64  \n",
       "36                                      3.430373  float64  \n",
       "35                                     14.443456  float64  \n",
       "31                                     96.838081  float64  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"---Top variables por porcentaje de nulos del total---\")\n",
    "stats = []\n",
    "for col in dataset_df.columns:\n",
    "    stats.append((col, dataset_df[col].nunique(), dataset_df[col].isna().sum() * 100 / dataset_df.shape[0], \n",
    "                dataset_df[col].value_counts(normalize=True, dropna=False).values[0] * 100, dataset_df[col].dtype))\n",
    "\n",
    "stats_df = pd.DataFrame(stats, columns=['Feature', \n",
    "                                        'Unique_values', \n",
    "                                        'Percentage of missing values', \n",
    "                                        'Percentage of values in the biggest category', \n",
    "                                        'type'])\n",
    "\n",
    "stats_df.sort_values('Percentage of missing values', ascending=False).head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se borrarán columnas con una cantidad de nulos mayor al 90%, que sean constantes, ids o que sean totalmente irrelevantes para el modelo.\n",
    "delete_features=[\"PuaMode\",\"Census_ProcessorClass\",\"DefaultBrowsersIdentifier\",\"Census_IsFlightingInternal\",\"Census_InternalBatteryType\",\n",
    "                                \"Census_ThresholdOptIn\",\"SmartScreen\",\"OrganizationIdentifier\"]\n",
    "dataset_df=dataset_df[[col for col in dataset_df.columns if col not in delete_features]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaciones manuales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La columna Census_IsWIMBootEnabled tiene un 36% de 0 y un 63% de valores nulos, a juzgar por el nombre, podemos suponer que el nulo equivale al valor 1\n",
    "dataset_df[\"Census_IsWIMBootEnabled\"]=dataset_df[\"Census_IsWIMBootEnabled\"].fillna(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Las siguientes columnas tienen nulos y son numeros pero deberian ser tratadas como categorias en vez de numericas, para imputarse por el valor mas frecuente.\n",
    "new_cat_cols=[\"CountryIdentifier\",\"CityIdentifier\",\"GeoNameIdentifier\",\"AVProductStatesIdentifier\",\"AVProductsInstalled\",\"AVProductsEnabled\",\"IsProtected\",\"SMode\",\n",
    "            \"IeVerIdentifier\",\"Firewall\",\"Census_OEMNameIdentifier\",\"Census_OEMModelIdentifier\",\"Census_ProcessorCoreCount\",\"Census_ProcessorManufacturerIdentifier\",\n",
    "            \"Census_ProcessorModelIdentifier\",\"Census_TotalPhysicalRAM\",\"Census_OSInstallLanguageIdentifier\",\"Census_IsFlightsDisabled\",\n",
    "            \"Census_FirmwareManufacturerIdentifier\",\"Census_FirmwareVersionIdentifier\",\"Census_IsVirtualDevice\",\"Census_IsAlwaysOnAlwaysConnectedCapable\",\"Wdft_IsGamer\",\"Wdft_RegionIdentifier\"] \n",
    "dataset_df[new_cat_cols]=dataset_df[new_cat_cols].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de variables de máscara y contextuales sintéticas \n",
    "+ Para las variables del estilo máscara de red, se creará una columna con cada division posible de esa máscara.\n",
    "+ Se creará una columna con el valor medio, el máximo y el mínimo por cada una de las combinaciones posibles entre columnas categoricas y numericas indicadas a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de variables de máscara\n",
    "dataset_df=func.add_mask_features(dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables elegidas por ser de las más relevantes en las primeras iteraciones, sin añadir ninguna variable extra. \n",
    "synth_cat_cols=[\"CityIdentifier\",\"AVProductStatesIdentifier\"]\n",
    "synth_num_cols=[\"Census_SystemVolumeTotalCapacity\",\"Census_PrimaryDiskTotalCapacity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SumeriO\\Desktop\\Master BI IA\\TFM IA\\Fase 2\\functions.py:25: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  group_by_feat = df_ext.groupby(by=[cat])\n",
      "c:\\Users\\SumeriO\\Desktop\\Master BI IA\\TFM IA\\Fase 2\\functions.py:31: FutureWarning: The provided callable <function mean at 0x0000012A38D180E0> is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  df_grouped = group_by_feat[num_feat].agg([np.mean, np.max, np.min]).reset_index()\n",
      "c:\\Users\\SumeriO\\Desktop\\Master BI IA\\TFM IA\\Fase 2\\functions.py:31: FutureWarning: The provided callable <function max at 0x0000012A38D136A0> is currently using SeriesGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  df_grouped = group_by_feat[num_feat].agg([np.mean, np.max, np.min]).reset_index()\n",
      "c:\\Users\\SumeriO\\Desktop\\Master BI IA\\TFM IA\\Fase 2\\functions.py:31: FutureWarning: The provided callable <function min at 0x0000012A38D137E0> is currently using SeriesGroupBy.min. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"min\" instead.\n",
      "  df_grouped = group_by_feat[num_feat].agg([np.mean, np.max, np.min]).reset_index()\n",
      "c:\\Users\\SumeriO\\Desktop\\Master BI IA\\TFM IA\\Fase 2\\functions.py:31: FutureWarning: The provided callable <function mean at 0x0000012A38D180E0> is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  df_grouped = group_by_feat[num_feat].agg([np.mean, np.max, np.min]).reset_index()\n",
      "c:\\Users\\SumeriO\\Desktop\\Master BI IA\\TFM IA\\Fase 2\\functions.py:31: FutureWarning: The provided callable <function max at 0x0000012A38D136A0> is currently using SeriesGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  df_grouped = group_by_feat[num_feat].agg([np.mean, np.max, np.min]).reset_index()\n",
      "c:\\Users\\SumeriO\\Desktop\\Master BI IA\\TFM IA\\Fase 2\\functions.py:31: FutureWarning: The provided callable <function min at 0x0000012A38D137E0> is currently using SeriesGroupBy.min. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"min\" instead.\n",
      "  df_grouped = group_by_feat[num_feat].agg([np.mean, np.max, np.min]).reset_index()\n",
      "c:\\Users\\SumeriO\\Desktop\\Master BI IA\\TFM IA\\Fase 2\\functions.py:25: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  group_by_feat = df_ext.groupby(by=[cat])\n",
      "c:\\Users\\SumeriO\\Desktop\\Master BI IA\\TFM IA\\Fase 2\\functions.py:31: FutureWarning: The provided callable <function mean at 0x0000012A38D180E0> is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  df_grouped = group_by_feat[num_feat].agg([np.mean, np.max, np.min]).reset_index()\n",
      "c:\\Users\\SumeriO\\Desktop\\Master BI IA\\TFM IA\\Fase 2\\functions.py:31: FutureWarning: The provided callable <function max at 0x0000012A38D136A0> is currently using SeriesGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  df_grouped = group_by_feat[num_feat].agg([np.mean, np.max, np.min]).reset_index()\n",
      "c:\\Users\\SumeriO\\Desktop\\Master BI IA\\TFM IA\\Fase 2\\functions.py:31: FutureWarning: The provided callable <function min at 0x0000012A38D137E0> is currently using SeriesGroupBy.min. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"min\" instead.\n",
      "  df_grouped = group_by_feat[num_feat].agg([np.mean, np.max, np.min]).reset_index()\n",
      "c:\\Users\\SumeriO\\Desktop\\Master BI IA\\TFM IA\\Fase 2\\functions.py:31: FutureWarning: The provided callable <function mean at 0x0000012A38D180E0> is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  df_grouped = group_by_feat[num_feat].agg([np.mean, np.max, np.min]).reset_index()\n",
      "c:\\Users\\SumeriO\\Desktop\\Master BI IA\\TFM IA\\Fase 2\\functions.py:31: FutureWarning: The provided callable <function max at 0x0000012A38D136A0> is currently using SeriesGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  df_grouped = group_by_feat[num_feat].agg([np.mean, np.max, np.min]).reset_index()\n",
      "c:\\Users\\SumeriO\\Desktop\\Master BI IA\\TFM IA\\Fase 2\\functions.py:31: FutureWarning: The provided callable <function min at 0x0000012A38D137E0> is currently using SeriesGroupBy.min. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"min\" instead.\n",
      "  df_grouped = group_by_feat[num_feat].agg([np.mean, np.max, np.min]).reset_index()\n"
     ]
    }
   ],
   "source": [
    "# Creación de variables contextuales\n",
    "dataset_df, added_cols = func.generate_grouped_stats(dataset_df, synth_num_cols, synth_cat_cols)\n",
    "dataset_df = func.generate_synthetic_features(dataset_df, added_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Train dataset--\n",
      "Rows:892148\n",
      "Columns:119\n",
      "\n",
      "Cantidad de nulos: \n",
      "Train: 822492\n"
     ]
    }
   ],
   "source": [
    "print(f\"--Train dataset--\\nRows:{dataset_df.shape[0]}\\nColumns:{dataset_df.shape[1]}\\n\")\n",
    "print(f\"Cantidad de nulos: \\nTrain: {dataset_df.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### División del dataset en train y test\n",
    "+ Se dividiran los datos en una proporcion de 80% para la parte de entrenamiento y un 20% para la parte de pruebas\n",
    "+ Se utilizá una semilla para lograr siempre la misma división de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division en train y test\n",
    "X_train, X_test, y_train, y_test=train_test_split(\n",
    "                                        dataset_df.drop(columns=label),\n",
    "                                        dataset_df[label],\n",
    "                                        test_size=0.2,\n",
    "                                        random_state=411)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputación de nulos\n",
    "+ División de las columnas por tipo de dato\n",
    "+ Imputación de nulos de variables numericas por la media de ellas\n",
    "+ Imputación de nulos de variables categoricas por el valor mas frecuente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se devuelve columna a su estado numerico original\n",
    "X_train[\"Census_PrimaryDiskTotalCapacity\"]=X_train[\"Census_PrimaryDiskTotalCapacity\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# División por tipo de dato\n",
    "numeric_cols=X_train.select_dtypes(include=[\"int64\",\"float64\"]).columns.to_list()\n",
    "category_cols=X_train.select_dtypes(include=[\"category\",\"object\"]).columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputar variables numericas por la media \n",
    "numeric_imputer=SimpleImputer(strategy=\"mean\")\n",
    "X_train[numeric_cols]=numeric_imputer.fit_transform(X_train[numeric_cols])\n",
    "X_test[numeric_cols]=numeric_imputer.transform(X_test[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputar variables categoricas por el valor mas frecuente\n",
    "category_imputer=SimpleImputer(strategy=\"most_frequent\")\n",
    "X_train[category_cols]=category_imputer.fit_transform(X_train[category_cols])\n",
    "X_test[category_cols]=category_imputer.transform(X_test[category_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category Encoders\n",
    "+ Encoding de variables por cantidad de valores unicos que presentan:\n",
    "    + OneHotEncoder para columnas con 5 o menos valores unicos\n",
    "    + TargetEncoder para columnas con más de 5 valores unicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar las columnas por la cantidad de valores unicos para usar en cada encoder\n",
    "onehot_cols=list(filter(lambda col:X_train[col].nunique()<=5,X_train.columns))\n",
    "target_cols=list(filter(lambda col:X_train[col].nunique()>5,X_train.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### -> OneHot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHot encoding para variables con pocos valores unicos (<=5)\n",
    "onehot=OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "train_onehot_encoded=onehot.fit_transform(X_train[onehot_cols])\n",
    "test_onehot_encoded=onehot.transform(X_test[onehot_cols])\n",
    "\n",
    "train_onehot_df=pd.DataFrame(data=train_onehot_encoded.toarray(), columns=onehot.get_feature_names_out(onehot_cols),index=X_train.index)\n",
    "test_onehot_df=pd.DataFrame(data=test_onehot_encoded.toarray(), columns=onehot.get_feature_names_out(onehot_cols),index=X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### -> Target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target encoding para variables con más valores unicos (>5)\n",
    "target=TargetEncoder()\n",
    "\n",
    "train_target_df=target.fit_transform(X_train[target_cols].astype(\"category\"), y_train)\n",
    "test_target_df=target.transform(X_test[target_cols].astype(\"category\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusión de los DataFrames resultantes de los encoders \n",
    "X_train=train_onehot_df.join(train_target_df)\n",
    "X_test=test_onehot_df.join(test_target_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Train dataset--\n",
      "Rows:713718\n",
      "Columns:162\n",
      "\n",
      "--Test dataset--\n",
      "Rows:178430\n",
      "Columns:162\n",
      "\n",
      "Cantidad de nulos: \n",
      "Train: 0\n",
      "Test: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"--Train dataset--\\nRows:{X_train.shape[0]}\\nColumns:{X_train.shape[1]}\\n\\n--Test dataset--\\nRows:{X_test.shape[0]}\\nColumns:{X_test.shape[1]}\\n\")\n",
    "print(f\"Cantidad de nulos: \\nTrain: {X_train.isna().sum().sum()}\\nTest: {X_test.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuneo de hiperparametros\n",
    "+ Optimización bayesiana con Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-04 23:50:00,325] A new study created in memory with name: no-name-43d4d4c4-3b05-4ed4-aef7-38877d6624e8\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-04 23:50:23,154] Trial 0 finished with value: 0.7588794176486677 and parameters: {'n_estimators': 65, 'feature_fraction': 0.6, 'bagging_fraction': 0.65, 'num_leaves': 60, 'learning_rate': 0.01, 'max_depth': 20, 'min_child_samples': 4, 'reg_alpha': 0.4, 'reg_lambda': 1.0, 'colsample_bytree': 0.4}. Best is trial 0 with value: 0.7588794176486677.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-04 23:50:38,581] Trial 1 finished with value: 0.7760898911254804 and parameters: {'n_estimators': 60, 'feature_fraction': 0.30000000000000004, 'bagging_fraction': 0.55, 'num_leaves': 30, 'learning_rate': 0.16000000000000003, 'max_depth': 60, 'min_child_samples': 3, 'reg_alpha': 0.4, 'reg_lambda': 0.6000000000000001, 'colsample_bytree': 0.6000000000000001}. Best is trial 1 with value: 0.7760898911254804.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-04 23:50:51,757] Trial 2 finished with value: 0.7777671833776374 and parameters: {'n_estimators': 45, 'feature_fraction': 0.30000000000000004, 'bagging_fraction': 0.55, 'num_leaves': 30, 'learning_rate': 0.46, 'max_depth': 45, 'min_child_samples': 2, 'reg_alpha': 0.2, 'reg_lambda': 0.4, 'colsample_bytree': 0.2}. Best is trial 2 with value: 0.7777671833776374.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-04 23:51:02,845] Trial 3 finished with value: 0.748207059076646 and parameters: {'n_estimators': 35, 'feature_fraction': 0.1, 'bagging_fraction': 0.5, 'num_leaves': 100, 'learning_rate': 0.01, 'max_depth': 75, 'min_child_samples': 4, 'reg_alpha': 0.0, 'reg_lambda': 0.8, 'colsample_bytree': 0.8}. Best is trial 2 with value: 0.7777671833776374.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-04 23:51:21,350] Trial 4 finished with value: 0.7791359426530924 and parameters: {'n_estimators': 90, 'feature_fraction': 0.9, 'bagging_fraction': 0.65, 'num_leaves': 40, 'learning_rate': 0.41000000000000003, 'max_depth': 20, 'min_child_samples': 5, 'reg_alpha': 1.0, 'reg_lambda': 0.6000000000000001, 'colsample_bytree': 0.0}. Best is trial 4 with value: 0.7791359426530924.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-04 23:51:43,228] Trial 5 finished with value: 0.781135794189367 and parameters: {'n_estimators': 100, 'feature_fraction': 0.4, 'bagging_fraction': 0.7, 'num_leaves': 70, 'learning_rate': 0.26, 'max_depth': 90, 'min_child_samples': 5, 'reg_alpha': 0.8, 'reg_lambda': 0.0, 'colsample_bytree': 0.2}. Best is trial 5 with value: 0.781135794189367.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-04 23:52:00,589] Trial 6 finished with value: 0.7786871714228913 and parameters: {'n_estimators': 75, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'num_leaves': 40, 'learning_rate': 0.26, 'max_depth': 65, 'min_child_samples': 5, 'reg_alpha': 0.4, 'reg_lambda': 0.4, 'colsample_bytree': 0.0}. Best is trial 5 with value: 0.781135794189367.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-04 23:52:12,519] Trial 7 finished with value: 0.7775259634755596 and parameters: {'n_estimators': 40, 'feature_fraction': 0.2, 'bagging_fraction': 0.75, 'num_leaves': 60, 'learning_rate': 0.36000000000000004, 'max_depth': 50, 'min_child_samples': 5, 'reg_alpha': 0.6000000000000001, 'reg_lambda': 0.0, 'colsample_bytree': 0.4}. Best is trial 5 with value: 0.781135794189367.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-04 23:52:24,004] Trial 8 finished with value: 0.7745865026326133 and parameters: {'n_estimators': 20, 'feature_fraction': 0.8, 'bagging_fraction': 0.7, 'num_leaves': 90, 'learning_rate': 0.36000000000000004, 'max_depth': 65, 'min_child_samples': 6, 'reg_alpha': 0.6000000000000001, 'reg_lambda': 0.4, 'colsample_bytree': 0.4}. Best is trial 5 with value: 0.781135794189367.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-04 23:52:41,670] Trial 9 finished with value: 0.7807066562572261 and parameters: {'n_estimators': 95, 'feature_fraction': 0.30000000000000004, 'bagging_fraction': 0.7, 'num_leaves': 40, 'learning_rate': 0.36000000000000004, 'max_depth': 85, 'min_child_samples': 7, 'reg_alpha': 0.2, 'reg_lambda': 0.0, 'colsample_bytree': 0.2}. Best is trial 5 with value: 0.781135794189367.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-04 23:53:09,627] Trial 10 finished with value: 0.7808244610805224 and parameters: {'n_estimators': 100, 'feature_fraction': 0.6, 'bagging_fraction': 0.95, 'num_leaves': 80, 'learning_rate': 0.16000000000000003, 'max_depth': 100, 'min_child_samples': 7, 'reg_alpha': 1.0, 'reg_lambda': 0.2, 'colsample_bytree': 1.0}. Best is trial 5 with value: 0.781135794189367.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-04 23:53:36,089] Trial 11 finished with value: 0.7807150689263092 and parameters: {'n_estimators': 100, 'feature_fraction': 0.6, 'bagging_fraction': 0.9, 'num_leaves': 80, 'learning_rate': 0.21000000000000002, 'max_depth': 100, 'min_child_samples': 7, 'reg_alpha': 1.0, 'reg_lambda': 0.2, 'colsample_bytree': 1.0}. Best is trial 5 with value: 0.781135794189367.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-04 23:54:01,678] Trial 12 finished with value: 0.7783449841903323 and parameters: {'n_estimators': 80, 'feature_fraction': 0.5, 'bagging_fraction': 0.95, 'num_leaves': 70, 'learning_rate': 0.11, 'max_depth': 100, 'min_child_samples': 6, 'reg_alpha': 0.8, 'reg_lambda': 0.2, 'colsample_bytree': 1.0}. Best is trial 5 with value: 0.781135794189367.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-04 23:54:22,785] Trial 13 finished with value: 0.7807403108671019 and parameters: {'n_estimators': 80, 'feature_fraction': 0.5, 'bagging_fraction': 0.8500000000000001, 'num_leaves': 80, 'learning_rate': 0.26, 'max_depth': 90, 'min_child_samples': 6, 'reg_alpha': 0.8, 'reg_lambda': 0.2, 'colsample_bytree': 0.8}. Best is trial 5 with value: 0.781135794189367.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-04 23:54:50,891] Trial 14 finished with value: 0.7793098470024138 and parameters: {'n_estimators': 100, 'feature_fraction': 0.7000000000000001, 'bagging_fraction': 0.8, 'num_leaves': 70, 'learning_rate': 0.11, 'max_depth': 85, 'min_child_samples': 7, 'reg_alpha': 0.8, 'reg_lambda': 0.2, 'colsample_bytree': 0.6000000000000001}. Best is trial 5 with value: 0.781135794189367.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-04 23:55:12,518] Trial 15 finished with value: 0.7817696903191957 and parameters: {'n_estimators': 85, 'feature_fraction': 0.4, 'bagging_fraction': 0.95, 'num_leaves': 100, 'learning_rate': 0.21000000000000002, 'max_depth': 35, 'min_child_samples': 2, 'reg_alpha': 1.0, 'reg_lambda': 0.0, 'colsample_bytree': 0.2}. Best is trial 15 with value: 0.7817696903191957.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-04 23:55:33,111] Trial 16 finished with value: 0.7800110573082586 and parameters: {'n_estimators': 85, 'feature_fraction': 0.4, 'bagging_fraction': 0.8500000000000001, 'num_leaves': 100, 'learning_rate': 0.31000000000000005, 'max_depth': 35, 'min_child_samples': 2, 'reg_alpha': 0.8, 'reg_lambda': 0.0, 'colsample_bytree': 0.2}. Best is trial 15 with value: 0.7817696903191957.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-04 23:55:51,810] Trial 17 finished with value: 0.7809703134050439 and parameters: {'n_estimators': 70, 'feature_fraction': 0.4, 'bagging_fraction': 0.6, 'num_leaves': 90, 'learning_rate': 0.21000000000000002, 'max_depth': 10, 'min_child_samples': 3, 'reg_alpha': 1.0, 'reg_lambda': 0.0, 'colsample_bytree': 0.0}. Best is trial 15 with value: 0.7817696903191957.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-04 23:56:03,429] Trial 18 finished with value: 0.7801372734238979 and parameters: {'n_estimators': 50, 'feature_fraction': 0.1, 'bagging_fraction': 0.75, 'num_leaves': 50, 'learning_rate': 0.31000000000000005, 'max_depth': 35, 'min_child_samples': 3, 'reg_alpha': 0.6000000000000001, 'reg_lambda': 0.0, 'colsample_bytree': 0.2}. Best is trial 15 with value: 0.7817696903191957.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-04 23:56:27,205] Trial 19 finished with value: 0.7806645835499773 and parameters: {'n_estimators': 90, 'feature_fraction': 0.4, 'bagging_fraction': 0.8500000000000001, 'num_leaves': 90, 'learning_rate': 0.11, 'max_depth': 40, 'min_child_samples': 4, 'reg_alpha': 0.8, 'reg_lambda': 1.0, 'colsample_bytree': 0.2}. Best is trial 15 with value: 0.7817696903191957.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-04 23:56:48,016] Trial 20 finished with value: 0.7819183493055417 and parameters: {'n_estimators': 85, 'feature_fraction': 0.2, 'bagging_fraction': 0.65, 'num_leaves': 100, 'learning_rate': 0.16000000000000003, 'max_depth': 75, 'min_child_samples': 2, 'reg_alpha': 1.0, 'reg_lambda': 0.8, 'colsample_bytree': 0.0}. Best is trial 20 with value: 0.7819183493055417.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-04 23:57:07,256] Trial 21 finished with value: 0.7819183493055417 and parameters: {'n_estimators': 85, 'feature_fraction': 0.2, 'bagging_fraction': 0.65, 'num_leaves': 100, 'learning_rate': 0.16000000000000003, 'max_depth': 75, 'min_child_samples': 2, 'reg_alpha': 1.0, 'reg_lambda': 0.8, 'colsample_bytree': 0.0}. Best is trial 20 with value: 0.7819183493055417.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-04 23:57:25,239] Trial 22 finished with value: 0.7813629906948514 and parameters: {'n_estimators': 75, 'feature_fraction': 0.2, 'bagging_fraction': 0.6, 'num_leaves': 100, 'learning_rate': 0.16000000000000003, 'max_depth': 75, 'min_child_samples': 2, 'reg_alpha': 1.0, 'reg_lambda': 0.8, 'colsample_bytree': 0.0}. Best is trial 20 with value: 0.7819183493055417.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-04 23:57:46,103] Trial 23 finished with value: 0.779489362826619 and parameters: {'n_estimators': 85, 'feature_fraction': 0.2, 'bagging_fraction': 0.65, 'num_leaves': 100, 'learning_rate': 0.060000000000000005, 'max_depth': 75, 'min_child_samples': 2, 'reg_alpha': 1.0, 'reg_lambda': 0.8, 'colsample_bytree': 0.0}. Best is trial 20 with value: 0.7819183493055417.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-04 23:57:59,582] Trial 24 finished with value: 0.7811442162596187 and parameters: {'n_estimators': 55, 'feature_fraction': 0.1, 'bagging_fraction': 0.6, 'num_leaves': 90, 'learning_rate': 0.21000000000000002, 'max_depth': 55, 'min_child_samples': 3, 'reg_alpha': 1.0, 'reg_lambda': 0.6000000000000001, 'colsample_bytree': 0.0}. Best is trial 20 with value: 0.7819183493055417.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-04 23:58:16,545] Trial 25 finished with value: 0.7787825461988319 and parameters: {'n_estimators': 70, 'feature_fraction': 0.2, 'bagging_fraction': 0.65, 'num_leaves': 100, 'learning_rate': 0.060000000000000005, 'max_depth': 70, 'min_child_samples': 2, 'reg_alpha': 1.0, 'reg_lambda': 0.8, 'colsample_bytree': 0.0}. Best is trial 20 with value: 0.7819183493055417.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-04 23:58:39,009] Trial 26 finished with value: 0.781553711492531 and parameters: {'n_estimators': 90, 'feature_fraction': 0.30000000000000004, 'bagging_fraction': 0.5, 'num_leaves': 90, 'learning_rate': 0.16000000000000003, 'max_depth': 55, 'min_child_samples': 3, 'reg_alpha': 0.8, 'reg_lambda': 1.0, 'colsample_bytree': 0.2}. Best is trial 20 with value: 0.7819183493055417.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-04 23:58:57,241] Trial 27 finished with value: 0.7780981651646965 and parameters: {'n_estimators': 80, 'feature_fraction': 0.2, 'bagging_fraction': 0.75, 'num_leaves': 80, 'learning_rate': 0.060000000000000005, 'max_depth': 25, 'min_child_samples': 2, 'reg_alpha': 0.6000000000000001, 'reg_lambda': 0.8, 'colsample_bytree': 0.4}. Best is trial 20 with value: 0.7819183493055417.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-04 23:59:10,588] Trial 28 finished with value: 0.7819323784451087 and parameters: {'n_estimators': 65, 'feature_fraction': 0.1, 'bagging_fraction': 0.55, 'num_leaves': 100, 'learning_rate': 0.21000000000000002, 'max_depth': 80, 'min_child_samples': 3, 'reg_alpha': 1.0, 'reg_lambda': 0.6000000000000001, 'colsample_bytree': 0.0}. Best is trial 28 with value: 0.7819323784451087.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-04 23:59:25,152] Trial 29 finished with value: 0.7788470710206143 and parameters: {'n_estimators': 65, 'feature_fraction': 0.1, 'bagging_fraction': 0.55, 'num_leaves': 90, 'learning_rate': 0.11, 'max_depth': 80, 'min_child_samples': 3, 'reg_alpha': 0.8, 'reg_lambda': 1.0, 'colsample_bytree': 0.0}. Best is trial 28 with value: 0.7819323784451087.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-04 23:59:51,659] Trial 30 finished with value: 0.7543075340494442 and parameters: {'n_estimators': 60, 'feature_fraction': 1.0, 'bagging_fraction': 0.6, 'num_leaves': 100, 'learning_rate': 0.01, 'max_depth': 90, 'min_child_samples': 4, 'reg_alpha': 0.6000000000000001, 'reg_lambda': 0.6000000000000001, 'colsample_bytree': 0.4}. Best is trial 28 with value: 0.7819323784451087.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-05 00:00:07,649] Trial 31 finished with value: 0.781267629764983 and parameters: {'n_estimators': 70, 'feature_fraction': 0.1, 'bagging_fraction': 0.65, 'num_leaves': 100, 'learning_rate': 0.21000000000000002, 'max_depth': 65, 'min_child_samples': 2, 'reg_alpha': 1.0, 'reg_lambda': 0.6000000000000001, 'colsample_bytree': 0.0}. Best is trial 28 with value: 0.7819323784451087.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-05 00:00:28,241] Trial 32 finished with value: 0.7817668821232893 and parameters: {'n_estimators': 85, 'feature_fraction': 0.30000000000000004, 'bagging_fraction': 0.55, 'num_leaves': 90, 'learning_rate': 0.16000000000000003, 'max_depth': 80, 'min_child_samples': 3, 'reg_alpha': 1.0, 'reg_lambda': 0.8, 'colsample_bytree': 0.2}. Best is trial 28 with value: 0.7819323784451087.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-05 00:00:46,112] Trial 33 finished with value: 0.7815396922261579 and parameters: {'n_estimators': 65, 'feature_fraction': 0.30000000000000004, 'bagging_fraction': 0.5, 'num_leaves': 100, 'learning_rate': 0.16000000000000003, 'max_depth': 60, 'min_child_samples': 2, 'reg_alpha': 1.0, 'reg_lambda': 0.6000000000000001, 'colsample_bytree': 0.0}. Best is trial 28 with value: 0.7819323784451087.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-05 00:01:03,287] Trial 34 finished with value: 0.7817837136764539 and parameters: {'n_estimators': 75, 'feature_fraction': 0.2, 'bagging_fraction': 0.55, 'num_leaves': 100, 'learning_rate': 0.21000000000000002, 'max_depth': 45, 'min_child_samples': 2, 'reg_alpha': 0.0, 'reg_lambda': 1.0, 'colsample_bytree': 0.2}. Best is trial 28 with value: 0.7819323784451087.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-05 00:01:19,818] Trial 35 finished with value: 0.7807234841915309 and parameters: {'n_estimators': 75, 'feature_fraction': 0.2, 'bagging_fraction': 0.55, 'num_leaves': 90, 'learning_rate': 0.31000000000000005, 'max_depth': 50, 'min_child_samples': 3, 'reg_alpha': 0.0, 'reg_lambda': 1.0, 'colsample_bytree': 0.0}. Best is trial 28 with value: 0.7819323784451087.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-05 00:01:33,154] Trial 36 finished with value: 0.7799129040739989 and parameters: {'n_estimators': 60, 'feature_fraction': 0.1, 'bagging_fraction': 0.5, 'num_leaves': 80, 'learning_rate': 0.26, 'max_depth': 70, 'min_child_samples': 2, 'reg_alpha': 0.2, 'reg_lambda': 1.0, 'colsample_bytree': 0.2}. Best is trial 28 with value: 0.7819323784451087.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-05 00:01:49,206] Trial 37 finished with value: 0.7803364163311647 and parameters: {'n_estimators': 55, 'feature_fraction': 0.30000000000000004, 'bagging_fraction': 0.55, 'num_leaves': 100, 'learning_rate': 0.16000000000000003, 'max_depth': 80, 'min_child_samples': 4, 'reg_alpha': 0.0, 'reg_lambda': 0.8, 'colsample_bytree': 0.6000000000000001}. Best is trial 28 with value: 0.7819323784451087.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-05 00:02:06,123] Trial 38 finished with value: 0.7802915372767749 and parameters: {'n_estimators': 95, 'feature_fraction': 0.2, 'bagging_fraction': 0.6, 'num_leaves': 50, 'learning_rate': 0.46, 'max_depth': 45, 'min_child_samples': 3, 'reg_alpha': 0.4, 'reg_lambda': 1.0, 'colsample_bytree': 0.0}. Best is trial 28 with value: 0.7819323784451087.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-05 00:02:21,313] Trial 39 finished with value: 0.7818089575446829 and parameters: {'n_estimators': 75, 'feature_fraction': 0.1, 'bagging_fraction': 0.65, 'num_leaves': 90, 'learning_rate': 0.26, 'max_depth': 70, 'min_child_samples': 2, 'reg_alpha': 0.2, 'reg_lambda': 0.8, 'colsample_bytree': 0.0}. Best is trial 28 with value: 0.7819323784451087.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-05 00:02:36,526] Trial 40 finished with value: 0.7819043256729354 and parameters: {'n_estimators': 70, 'feature_fraction': 0.1, 'bagging_fraction': 0.65, 'num_leaves': 90, 'learning_rate': 0.26, 'max_depth': 70, 'min_child_samples': 2, 'reg_alpha': 0.2, 'reg_lambda': 0.6000000000000001, 'colsample_bytree': 0.0}. Best is trial 28 with value: 0.7819323784451087.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-05 00:02:50,685] Trial 41 finished with value: 0.7812087291627647 and parameters: {'n_estimators': 65, 'feature_fraction': 0.1, 'bagging_fraction': 0.65, 'num_leaves': 90, 'learning_rate': 0.26, 'max_depth': 70, 'min_child_samples': 2, 'reg_alpha': 0.2, 'reg_lambda': 0.6000000000000001, 'colsample_bytree': 0.0}. Best is trial 28 with value: 0.7819323784451087.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-05 00:03:04,974] Trial 42 finished with value: 0.7818903018043164 and parameters: {'n_estimators': 70, 'feature_fraction': 0.1, 'bagging_fraction': 0.7, 'num_leaves': 90, 'learning_rate': 0.31000000000000005, 'max_depth': 60, 'min_child_samples': 2, 'reg_alpha': 0.2, 'reg_lambda': 0.4, 'colsample_bytree': 0.0}. Best is trial 28 with value: 0.7819323784451087.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-05 00:03:17,569] Trial 43 finished with value: 0.779924106175987 and parameters: {'n_estimators': 55, 'feature_fraction': 0.1, 'bagging_fraction': 0.7, 'num_leaves': 80, 'learning_rate': 0.41000000000000003, 'max_depth': 60, 'min_child_samples': 3, 'reg_alpha': 0.4, 'reg_lambda': 0.4, 'colsample_bytree': 0.0}. Best is trial 28 with value: 0.7819323784451087.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-05 00:03:33,270] Trial 44 finished with value: 0.7815453057858195 and parameters: {'n_estimators': 70, 'feature_fraction': 0.2, 'bagging_fraction': 0.7, 'num_leaves': 100, 'learning_rate': 0.31000000000000005, 'max_depth': 60, 'min_child_samples': 2, 'reg_alpha': 0.2, 'reg_lambda': 0.4, 'colsample_bytree': 0.0}. Best is trial 28 with value: 0.7819323784451087.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-05 00:03:42,882] Trial 45 finished with value: 0.7781374290860075 and parameters: {'n_estimators': 30, 'feature_fraction': 0.1, 'bagging_fraction': 0.7, 'num_leaves': 90, 'learning_rate': 0.31000000000000005, 'max_depth': 85, 'min_child_samples': 3, 'reg_alpha': 0.4, 'reg_lambda': 0.4, 'colsample_bytree': 0.8}. Best is trial 28 with value: 0.7819323784451087.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-05 00:03:56,642] Trial 46 finished with value: 0.7802438548647369 and parameters: {'n_estimators': 45, 'feature_fraction': 0.30000000000000004, 'bagging_fraction': 0.75, 'num_leaves': 100, 'learning_rate': 0.36000000000000004, 'max_depth': 95, 'min_child_samples': 4, 'reg_alpha': 0.0, 'reg_lambda': 0.6000000000000001, 'colsample_bytree': 0.2}. Best is trial 28 with value: 0.7819323784451087.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-05 00:04:15,561] Trial 47 finished with value: 0.7786114439090894 and parameters: {'n_estimators': 80, 'feature_fraction': 0.7000000000000001, 'bagging_fraction': 0.8, 'num_leaves': 30, 'learning_rate': 0.21000000000000002, 'max_depth': 65, 'min_child_samples': 2, 'reg_alpha': 0.2, 'reg_lambda': 0.6000000000000001, 'colsample_bytree': 0.0}. Best is trial 28 with value: 0.7819323784451087.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-05 00:04:31,875] Trial 48 finished with value: 0.7825073611493678 and parameters: {'n_estimators': 95, 'feature_fraction': 0.1, 'bagging_fraction': 0.65, 'num_leaves': 60, 'learning_rate': 0.26, 'max_depth': 75, 'min_child_samples': 2, 'reg_alpha': 0.2, 'reg_lambda': 0.4, 'colsample_bytree': 0.2}. Best is trial 48 with value: 0.7825073611493678.\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.5, 0.99] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.5, 0.95].\n",
      "  warnings.warn(\n",
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.01, 0.5] and step=0.05, but the range is not divisible by `step`. It will be replaced by [0.01, 0.46].\n",
      "  warnings.warn(\n",
      "[I 2024-12-05 00:04:50,037] Trial 49 finished with value: 0.7818790756290435 and parameters: {'n_estimators': 95, 'feature_fraction': 0.2, 'bagging_fraction': 0.6, 'num_leaves': 60, 'learning_rate': 0.26, 'max_depth': 75, 'min_child_samples': 3, 'reg_alpha': 0.4, 'reg_lambda': 0.8, 'colsample_bytree': 0.2}. Best is trial 48 with value: 0.7825073611493678.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros encontrados: {'n_estimators': 95, 'feature_fraction': 0.1, 'bagging_fraction': 0.65, 'num_leaves': 60, 'learning_rate': 0.26, 'max_depth': 75, 'min_child_samples': 2, 'reg_alpha': 0.2, 'reg_lambda': 0.4, 'colsample_bytree': 0.2}\n"
     ]
    }
   ],
   "source": [
    "if realizar_tuneo_HP_y_RFECV:\n",
    "\n",
    "    # Crear el estudio con Optuna buscando maximizar la metrica\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: func.objective(trial, X_train, y_train), n_trials=50)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # Resultado de último tuneo de hiperparametros con Optuna\n",
    "    best_params={'n_estimators': 95,\n",
    "        'feature_fraction': 0.1,\n",
    "        'bagging_fraction': 0.65,\n",
    "        'num_leaves': 60,\n",
    "        'learning_rate': 0.26,\n",
    "        'max_depth': 75,\n",
    "        'min_child_samples': 2,\n",
    "        'reg_alpha': 0.2,\n",
    "        'reg_lambda': 0.4,\n",
    "        'colsample_bytree': 0.2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selección de variables - Metodo RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033070 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10508\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 160\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032206 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10506\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 159\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034562 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10504\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 158\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029154 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10502\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 157\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030226 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10502\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 157\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032926 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10500\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 156\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030713 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10500\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 156\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030419 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10498\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 155\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030893 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10496\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 154\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10494\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 153\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029941 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10492\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 152\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026069 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10490\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 151\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029261 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10488\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 150\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030811 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10486\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 149\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026164 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10484\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 148\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030102 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10482\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 147\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027034 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10480\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 146\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028774 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10478\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 145\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028699 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10476\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 144\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029790 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10474\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 143\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029475 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10472\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 142\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026118 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10470\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 141\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027633 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10468\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 140\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026049 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10466\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 139\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026767 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10464\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 138\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028449 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10462\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 137\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025794 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10460\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 136\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027092 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10458\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 135\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025568 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10456\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 134\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029226 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10454\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 133\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027227 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10452\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023948 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10450\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 131\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025424 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10448\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 130\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026632 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10446\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 129\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024297 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10444\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 128\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024572 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10442\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 127\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024173 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10440\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 126\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023558 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10438\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 125\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022287 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10436\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 124\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020137 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10434\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 123\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023778 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10432\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 122\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022978 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10430\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023892 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10256\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 120\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022915 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10249\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023666 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10244\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 118\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024779 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10242\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 117\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020416 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9987\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022130 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9957\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 115\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9955\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 114\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019900 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9953\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021069 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9951\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021855 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9949\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020825 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9947\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020154 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9939\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 109\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019641 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9937\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 108\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020126 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9935\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 107\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021033 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9933\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 106\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018931 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9931\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 105\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018921 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9923\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 104\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017907 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9921\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 103\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019701 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9919\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 102\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019143 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9703\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 101\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020256 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9448\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018425 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9446\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 99\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018319 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9444\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 98\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017123 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9410\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 97\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016836 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9408\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 96\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016077 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9406\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 95\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016671 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9404\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 94\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015844 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9402\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 93\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017147 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9400\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 92\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017175 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9398\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015975 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9396\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 90\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014678 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9394\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 89\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013897 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9392\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 88\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015149 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9390\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 87\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014900 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9384\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 86\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016398 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9382\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 85\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014223 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9380\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015896 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9378\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 83\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016136 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9249\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 82\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013294 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9247\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016176 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9245\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014717 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9243\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 79\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013262 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9241\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012690 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9239\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012100 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9004\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013341 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8809\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011197 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8807\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 74\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011203 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8805\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 73\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009293 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8799\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009314 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8797\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006108 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8795\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049898 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8776\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006548 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8746\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 68\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006898 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8738\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 67\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001682 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8730\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 66\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001971 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8720\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 65\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001600 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8712\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001703 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8687\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 63\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001527 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8604\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001668 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8351\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001696 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8160\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 60\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001585 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8126\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 59\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001572 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7871\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035760 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7663\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 57\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001448 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7633\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 56\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001541 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7570\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 55\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001515 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7355\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 54\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001379 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7337\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 53\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001533 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7303\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 52\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001547 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7224\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001575 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7215\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001410 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7167\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 49\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001540 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7157\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001659 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7150\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 47\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001318 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7118\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001543 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6915\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001197 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6843\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001476 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6833\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 43\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001305 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6745\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 42\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001437 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6711\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001702 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6670\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001588 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6546\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001046 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6403\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 38\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001444 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6388\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 37\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001541 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6370\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001283 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6154\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001246 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6142\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 34\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001225 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6127\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000974 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5873\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000958 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5772\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000865 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5597\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000979 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5388\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001370 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5331\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001323 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5076\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001343 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5060\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 26\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001087 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4881\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000651 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4827\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 24\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001273 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4572\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001161 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4334\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000753 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4086\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001273 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4010\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000794 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3809\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001244 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3591\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000646 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3336\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000647 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3296\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000928 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3100\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000762 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2845\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000545 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2637\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000885 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2382\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000898 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2308\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000663 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2053\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029536 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10521\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 157\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034276 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10519\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 156\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030704 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10517\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 155\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032256 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10517\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 155\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027705 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10515\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 154\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030077 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10515\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 154\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029820 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10513\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 153\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029437 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10511\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 152\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029984 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10509\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 151\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031326 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10507\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 150\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029076 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10505\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 149\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028047 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10503\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 148\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031060 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10501\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 147\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028616 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10499\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 146\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029641 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10497\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 145\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028319 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10495\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 144\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022852 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10493\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 143\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027670 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10491\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 142\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028181 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10489\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 141\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027158 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10487\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 140\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024744 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10485\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 139\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024109 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10483\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 138\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026190 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10481\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 137\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027665 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10479\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 136\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025528 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10479\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 136\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025534 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10477\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 135\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025485 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10475\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 134\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025825 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10473\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 133\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025658 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10471\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026880 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10471\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024277 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10469\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 131\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026180 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10467\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 130\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028424 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10465\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 129\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024290 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10463\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 128\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022707 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10463\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 128\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024562 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10461\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 127\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10454\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 126\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024324 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10452\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 125\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023298 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10450\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 124\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020105 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10448\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 123\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023292 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10254\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 122\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019820 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10252\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024355 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10250\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 120\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021800 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10243\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021746 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10241\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 118\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021796 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10239\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 117\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025816 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10230\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022321 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10228\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 115\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022247 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10226\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 114\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019780 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10224\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021570 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10222\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019460 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10220\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022580 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10218\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021111 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10216\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 109\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019193 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10214\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 108\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020239 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10212\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 107\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.080126 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10210\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 106\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019267 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10208\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 105\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018357 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10206\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 104\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018826 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10204\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 103\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018876 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10198\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 102\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019483 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9982\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 101\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019872 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9727\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016437 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9725\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 99\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016854 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9723\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 98\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015812 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9689\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 97\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016456 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9687\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 96\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017117 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9668\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 95\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015049 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9666\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 94\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014337 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9664\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 93\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015848 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9662\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 92\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018011 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9660\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015785 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9658\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 90\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014822 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9656\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 89\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016479 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9654\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 88\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014836 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9652\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 87\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015182 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9650\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 86\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012934 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9648\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 85\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013395 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9646\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014908 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9641\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 83\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015020 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9386\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 82\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014293 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9384\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014537 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9382\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013410 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9380\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 79\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012131 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9378\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014677 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9376\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014787 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9202\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012268 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9008\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011682 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9000\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 74\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013955 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8998\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 73\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009532 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8992\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009510 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8990\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044764 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8988\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006215 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8980\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006028 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8971\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 68\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007001 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8961\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 67\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001626 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8953\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 66\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001993 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8941\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 65\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001610 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8907\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001875 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8875\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 63\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001562 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8860\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001673 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8830\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001623 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8575\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 60\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001405 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8567\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 59\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001547 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8312\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001379 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8101\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 57\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001430 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7892\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 56\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001522 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7639\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 55\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001417 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7422\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 54\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001389 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7392\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 53\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001403 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7385\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 52\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001566 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7307\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001565 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7260\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001383 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7231\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 49\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001612 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7213\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001557 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7179\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 47\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001411 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7101\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001555 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6972\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001409 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6938\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001725 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6757\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 43\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001015 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6747\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 42\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001389 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6732\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001436 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6646\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001438 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6446\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001162 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6430\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 38\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001477 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6223\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 37\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001103 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6205\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001357 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5990\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001277 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5846\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 34\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001245 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5805\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000975 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5610\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001201 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5392\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000846 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5183\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000982 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5159\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000827 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4927\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001267 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4750\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000772 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4687\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 26\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000880 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4628\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000757 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4571\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 24\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001524 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4316\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001130 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4276\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001088 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4150\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000750 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4075\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001094 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3820\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001127 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3575\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001096 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3320\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000960 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3217\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001107 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3059\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000689 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2974\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000775 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2768\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001115 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2513\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000968 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2274\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285221, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000647 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2019\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499534 -> initscore=-0.001863\n",
      "[LightGBM] [Info] Start training from score -0.001863\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031994 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10422\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 157\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027249 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10420\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 156\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030972 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10418\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 155\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034193 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10418\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 155\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032305 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10418\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 155\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030263 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10416\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 154\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029583 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10414\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 153\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031470 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10414\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 153\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030368 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10412\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 152\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028934 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10410\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 151\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029213 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10408\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 150\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028585 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10406\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 149\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030389 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10404\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 148\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030583 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10402\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 147\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030729 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10400\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 146\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026317 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10398\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 145\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026829 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10396\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 144\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030516 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10396\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 144\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026395 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10394\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 143\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026970 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10312\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 142\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026710 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10310\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 141\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026495 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10308\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 140\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027608 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10306\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 139\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027320 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10304\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 138\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026611 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10302\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 137\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027086 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10300\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 136\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024550 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10298\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 135\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022866 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10296\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 134\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024374 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10294\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 133\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024797 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10292\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026979 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10292\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024723 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10290\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 131\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022907 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10288\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 130\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023403 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10286\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 129\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027161 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10284\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 128\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024170 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10282\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 127\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023440 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10276\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 126\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024368 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10274\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 125\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024051 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10272\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 124\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024904 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10270\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 123\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022936 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10268\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 122\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024498 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10266\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023381 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10093\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 120\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020939 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10091\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023259 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10087\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 118\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024522 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10085\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 117\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021503 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9913\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021366 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9911\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 115\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019405 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9909\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 114\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020749 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9907\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020936 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9905\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019477 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9903\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.087493 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9901\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020459 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9899\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 109\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020462 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9897\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 108\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019693 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9895\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 107\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020501 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9893\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 106\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019301 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9891\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 105\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017887 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9860\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 104\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018282 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9845\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 103\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018216 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9843\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 102\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018383 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9627\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 101\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017287 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9372\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020404 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9353\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 99\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018577 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9351\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 98\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018222 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9349\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 97\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015950 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9347\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 96\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015917 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9345\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 95\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015702 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9339\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 94\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018211 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9337\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 93\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016411 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9335\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 92\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016286 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9333\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018155 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9331\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 90\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016281 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9329\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 89\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016333 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9327\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 88\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017312 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9325\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 87\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014916 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9323\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 86\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016460 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9321\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 85\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015110 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9319\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015159 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9317\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 83\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015825 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9189\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 82\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014651 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9187\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015024 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9157\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015971 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9155\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 79\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014262 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9153\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013587 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9151\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014015 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8896\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013620 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8702\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012318 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8696\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 74\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011498 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8694\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 73\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011239 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8692\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010356 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8690\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009634 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8688\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010561 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8648\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007778 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8646\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 68\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007366 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8614\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 67\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006957 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8606\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 66\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007612 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8592\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 65\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007575 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8583\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007257 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8521\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 63\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006593 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8513\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007298 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8297\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006392 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8090\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 60\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043511 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8083\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 59\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006599 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7828\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001741 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7820\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 57\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001610 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7813\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 56\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001497 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7778\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 55\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001341 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7699\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 54\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001534 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7681\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 53\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001814 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7634\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 52\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001481 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7420\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001479 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7251\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001548 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7241\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 49\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001580 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7232\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001518 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7224\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 47\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001375 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7192\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001660 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7090\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001376 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7058\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001459 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6874\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 43\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001101 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6815\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 42\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001336 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6599\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001462 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6389\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001518 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6377\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001268 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6252\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 38\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001535 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6031\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 37\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001583 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5799\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001501 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5552\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001328 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5297\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 34\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001374 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5264\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000742 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5233\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001353 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5217\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000871 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5207\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001390 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5064\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001411 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4984\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001370 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4960\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001390 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4770\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 26\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001371 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4682\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000663 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4663\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 24\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001307 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4408\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000756 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4162\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000962 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3958\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001250 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3885\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001311 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3630\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001261 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3417\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001293 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3314\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000700 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3060\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001025 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3005\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000632 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2849\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000698 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2596\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000696 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2437\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000993 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2397\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285752\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000716 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2143\n",
      "[LightGBM] [Info] Number of data points in the train set: 570974, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499536 -> initscore=-0.001856\n",
      "[LightGBM] [Info] Start training from score -0.001856\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028304 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10446\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 157\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027647 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10444\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 156\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034052 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10442\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 155\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031531 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10442\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 155\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030812 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10440\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 154\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030079 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10438\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 153\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025270 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10438\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 153\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030333 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10436\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 152\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029097 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10434\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 151\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028666 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10432\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 150\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029660 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10430\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 149\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029695 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10430\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 149\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028631 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10428\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 148\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028914 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10426\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 147\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030398 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10424\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 146\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027394 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10422\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 145\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029399 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10422\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 145\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029554 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10420\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 144\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029604 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10418\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 143\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029394 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10338\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 142\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027457 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10336\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 141\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024851 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10334\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 140\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028447 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10332\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 139\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024110 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10330\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 138\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026539 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10328\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 137\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024074 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10326\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 136\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026161 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10326\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 136\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025790 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10324\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 135\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025811 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10322\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 134\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026165 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10320\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 133\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024085 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10318\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024797 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10316\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 131\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025025 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10314\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 130\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023373 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10312\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 129\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024307 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10310\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 128\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022699 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10308\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 127\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023071 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10306\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 126\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023507 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10304\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 125\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022967 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10302\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 124\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022864 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10294\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 123\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021671 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10292\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 122\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022732 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10290\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023352 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10288\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 120\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023208 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10286\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021129 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10284\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 118\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022460 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10282\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 117\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020639 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10275\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021072 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10273\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 115\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022188 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10271\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 114\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020022 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10269\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019554 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10267\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018783 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10265\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019950 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10263\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019136 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10261\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 109\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018939 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10259\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 108\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020548 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10257\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 107\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021773 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10255\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 106\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019839 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10253\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 105\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018930 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10251\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 104\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018693 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10249\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 103\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017683 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10247\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 102\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018813 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10031\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 101\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.083035 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10029\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017866 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10027\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 99\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067407 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10025\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 98\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017336 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9993\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 97\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017767 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9991\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 96\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016565 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9989\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 95\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018842 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9987\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 94\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016610 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9985\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 93\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016404 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9983\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 92\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018109 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9978\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015214 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9976\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 90\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015604 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9974\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 89\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062316 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9967\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 88\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015145 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9965\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 87\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014271 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9963\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 86\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013503 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9961\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 85\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012857 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9959\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013217 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9957\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 83\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012796 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9923\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 82\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012543 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9891\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012972 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9882\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012579 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9880\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 79\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064448 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9878\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013156 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9876\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011688 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9843\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011115 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9652\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011222 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9643\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 74\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012964 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9635\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 73\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008672 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9629\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006395 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9627\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006099 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9625\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050866 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9610\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006053 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9569\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 68\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001738 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9561\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 67\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001826 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9512\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 66\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001821 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9505\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 65\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001571 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9493\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001754 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9240\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 63\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001441 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9150\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001698 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9116\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001637 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8906\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 60\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001563 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8891\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 59\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001559 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8636\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001661 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8423\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 57\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8394\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 56\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001453 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8361\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 55\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001351 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8144\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 54\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001263 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8125\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 53\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001264 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8118\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 52\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001583 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8037\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001521 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8019\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001369 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8011\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 49\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001653 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7992\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001584 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7805\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 47\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001452 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7741\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001639 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7487\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001386 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7477\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001096 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7262\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 43\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001400 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7204\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 42\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001397 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6994\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001570 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6784\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001340 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6659\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001172 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6516\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 38\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001451 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6440\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 37\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001437 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6185\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001491 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5971\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001602 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5769\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 34\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001369 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5738\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000968 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5535\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001242 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5525\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001062 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5453\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001113 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5429\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000873 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5272\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001289 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5096\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001355 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4841\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 26\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001368 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4766\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001274 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4710\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 24\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001281 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4489\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000777 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4473\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001012 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4301\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000773 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4071\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000986 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3816\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003272 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3572\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001257 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3317\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001253 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3214\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001275 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2965\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001164 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2925\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000546 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2670\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000745 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2415\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000593 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2160\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001182 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1922\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031578 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10425\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 157\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028492 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10423\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 156\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031827 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10421\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 155\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032121 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10421\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 155\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030535 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10419\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 154\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028464 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10417\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 153\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029777 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10417\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 153\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029328 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10415\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 152\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10413\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 151\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029503 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10411\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 150\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029316 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10411\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 150\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028805 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10409\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 149\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028392 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10407\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 148\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029487 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10405\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 147\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027653 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10403\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 146\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024809 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10401\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 145\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028120 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10399\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 144\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026802 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10397\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 143\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026987 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10395\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 142\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027018 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10395\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 142\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025658 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10393\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 141\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026784 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10391\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 140\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026168 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10389\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 139\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025340 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10387\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 138\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025629 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10385\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 137\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024074 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10383\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 136\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024671 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10381\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 135\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025579 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10379\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 134\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025899 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10377\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 133\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024070 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10375\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024783 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10375\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022483 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10373\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 131\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024107 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10371\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 130\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026533 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10369\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 129\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025264 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10367\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 128\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020390 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10365\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 127\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023337 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10363\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 126\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023795 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10361\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 125\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022566 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10359\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 124\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023699 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10357\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 123\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023656 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10355\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 122\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023170 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10353\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020820 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10351\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 120\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020047 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10349\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 119\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022542 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10347\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 118\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020995 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10345\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 117\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020760 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10335\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 116\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019287 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10333\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 115\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019654 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10331\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 114\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019993 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10329\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 113\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019383 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10327\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018930 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10325\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 111\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021754 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10323\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020820 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10314\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 109\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022692 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10312\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 108\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.077636 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10310\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 107\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018490 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10308\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 106\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018532 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10306\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 105\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017240 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10298\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 104\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017656 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10296\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 103\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017939 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10294\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 102\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017145 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10077\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 101\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017312 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9822\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015887 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9820\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 99\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020140 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9818\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 98\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017937 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9626\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 97\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017594 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9624\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 96\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016710 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9622\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 95\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017594 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9620\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 94\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017129 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9618\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 93\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015478 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9616\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 92\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018928 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9614\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014582 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9612\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 90\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014601 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9610\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 89\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012978 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9608\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 88\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013571 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9606\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 87\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068737 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9601\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 86\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014030 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9593\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 85\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011869 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9591\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 84\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012191 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9589\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 83\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013029 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9573\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 82\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011372 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9571\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012922 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9569\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012292 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9567\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 79\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013884 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9527\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011278 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9525\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010508 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9354\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012983 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9163\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011160 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9148\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 74\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009704 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9146\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 73\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009730 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9140\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010396 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9138\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050511 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9131\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009716 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9111\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049872 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9102\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 68\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009031 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9070\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 67\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006442 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9064\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 66\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006239 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9033\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 65\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006350 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9025\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006638 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8993\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 63\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005932 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 62\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006335 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8889\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 61\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006516 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8634\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 60\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001666 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8626\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 59\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001510 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8595\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001706 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8577\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 57\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001588 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8545\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 56\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001580 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8335\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 55\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001502 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8253\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 54\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001448 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8245\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 53\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001628 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8217\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 52\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001655 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8002\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 51\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001674 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7835\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001565 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7821\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 49\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001560 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7814\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001593 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7796\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 47\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001303 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7784\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001567 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7682\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001492 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7579\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 44\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001553 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7457\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 43\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7202\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 42\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001600 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7120\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001532 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7110\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001208 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7064\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028430 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7002\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 38\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001479 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6794\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 37\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001628 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6753\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001478 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6509\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001223 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6306\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 34\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001451 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6106\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001116 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6016\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001276 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5828\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001450 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5652\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001101 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5526\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001363 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5292\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001312 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5258\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 27\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001374 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5003\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 26\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001374 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4928\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001294 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4871\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 24\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000754 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4650\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000786 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4593\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001190 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4516\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001296 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4261\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000914 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4056\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000663 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4032\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000861 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3777\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001274 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3522\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001255 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3311\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000666 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3154\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000549 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2901\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001071 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2669\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000609 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2514\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 285222, number of negative: 285753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000680 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2369\n",
      "[LightGBM] [Info] Number of data points in the train set: 570975, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 356527, number of negative: 357191\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039198 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10410\n",
      "[LightGBM] [Info] Number of data points in the train set: 713718, number of used features: 157\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001861\n",
      "[LightGBM] [Info] Start training from score -0.001861\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 356527, number of negative: 357191\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035388 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10408\n",
      "[LightGBM] [Info] Number of data points in the train set: 713718, number of used features: 156\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001861\n",
      "[LightGBM] [Info] Start training from score -0.001861\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 356527, number of negative: 357191\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035985 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10406\n",
      "[LightGBM] [Info] Number of data points in the train set: 713718, number of used features: 155\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001861\n",
      "[LightGBM] [Info] Start training from score -0.001861\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 356527, number of negative: 357191\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039248 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10406\n",
      "[LightGBM] [Info] Number of data points in the train set: 713718, number of used features: 155\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001861\n",
      "[LightGBM] [Info] Start training from score -0.001861\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 356527, number of negative: 357191\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038634 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10404\n",
      "[LightGBM] [Info] Number of data points in the train set: 713718, number of used features: 154\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001861\n",
      "[LightGBM] [Info] Start training from score -0.001861\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 356527, number of negative: 357191\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038539 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10404\n",
      "[LightGBM] [Info] Number of data points in the train set: 713718, number of used features: 154\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001861\n",
      "[LightGBM] [Info] Start training from score -0.001861\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 356527, number of negative: 357191\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032407 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10402\n",
      "[LightGBM] [Info] Number of data points in the train set: 713718, number of used features: 153\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001861\n",
      "[LightGBM] [Info] Start training from score -0.001861\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 356527, number of negative: 357191\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037174 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10400\n",
      "[LightGBM] [Info] Number of data points in the train set: 713718, number of used features: 152\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001861\n",
      "[LightGBM] [Info] Start training from score -0.001861\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 356527, number of negative: 357191\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035902 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10400\n",
      "[LightGBM] [Info] Number of data points in the train set: 713718, number of used features: 152\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001861\n",
      "[LightGBM] [Info] Start training from score -0.001861\n"
     ]
    }
   ],
   "source": [
    "if realizar_tuneo_HP_y_RFECV:\n",
    "    \n",
    "    # Selección de variables usando RFECV, buscando maximizar la metrica Recall, con los hiperparametros previamente sacados\n",
    "    rfe = RFECV(\n",
    "        estimator=lgbm.LGBMClassifier(**best_params),\n",
    "        min_features_to_select=10, \n",
    "        cv=5, \n",
    "        scoring='recall')\n",
    "    \n",
    "    rfe.fit(X_train,y_train)\n",
    "    \n",
    "    rfe_features=X_train.columns[rfe.support_]\n",
    "\n",
    "else:\n",
    "    # Resultado de ultimo RFECV\n",
    "    rfe_features=['IsSxsPassiveMode_0.0',\n",
    "        'IsSxsPassiveMode_1.0',\n",
    "        'HasTpm_0.0',\n",
    "        'HasTpm_1.0',\n",
    "        'Platform_windows10',\n",
    "        'Platform_windows2016',\n",
    "        'Platform_windows7',\n",
    "        'Platform_windows8',\n",
    "        'Processor_arm64',\n",
    "        'Processor_x64',\n",
    "        'Processor_x86',\n",
    "        'IsProtected_0.0',\n",
    "        'IsProtected_1.0',\n",
    "        'AutoSampleOptIn_0.0',\n",
    "        'AutoSampleOptIn_1.0',\n",
    "        'SMode_0.0',\n",
    "        'SMode_1.0',\n",
    "        'Firewall_0.0',\n",
    "        'Firewall_1.0',\n",
    "        'Census_DeviceFamily_Windows',\n",
    "        'Census_DeviceFamily_Windows.Desktop',\n",
    "        'Census_DeviceFamily_Windows.Server',\n",
    "        'Census_ProcessorManufacturerIdentifier_1.0',\n",
    "        'Census_ProcessorManufacturerIdentifier_3.0',\n",
    "        'Census_ProcessorManufacturerIdentifier_5.0',\n",
    "        'Census_ProcessorManufacturerIdentifier_10.0',\n",
    "        'Census_PrimaryDiskTypeName_HDD',\n",
    "        'Census_PrimaryDiskTypeName_SSD',\n",
    "        'Census_PrimaryDiskTypeName_UNKNOWN',\n",
    "        'Census_PrimaryDiskTypeName_Unspecified',\n",
    "        'Census_HasOpticalDiskDrive_0.0',\n",
    "        'Census_HasOpticalDiskDrive_1.0',\n",
    "        'Census_OSArchitecture_amd64',\n",
    "        'Census_OSArchitecture_arm64',\n",
    "        'Census_OSArchitecture_x86',\n",
    "        'Census_IsPortableOperatingSystem_0.0',\n",
    "        'Census_IsPortableOperatingSystem_1.0',\n",
    "        'Census_GenuineStateName_INVALID_LICENSE',\n",
    "        'Census_GenuineStateName_IS_GENUINE',\n",
    "        'Census_GenuineStateName_OFFLINE',\n",
    "        'Census_GenuineStateName_UNKNOWN',\n",
    "        'Census_IsFlightsDisabled_0.0',\n",
    "        'Census_IsFlightsDisabled_1.0',\n",
    "        'Census_IsSecureBootEnabled_0.0',\n",
    "        'Census_IsSecureBootEnabled_1.0',\n",
    "        'Census_IsWIMBootEnabled_0.0',\n",
    "        'Census_IsWIMBootEnabled_1.0',\n",
    "        'Census_IsVirtualDevice_0.0',\n",
    "        'Census_IsVirtualDevice_1.0',\n",
    "        'Census_IsTouchEnabled_0.0',\n",
    "        'Census_IsTouchEnabled_1.0',\n",
    "        'Census_IsPenCapable_0.0',\n",
    "        'Census_IsPenCapable_1.0',\n",
    "        'Census_IsAlwaysOnAlwaysConnectedCapable_0.0',\n",
    "        'Census_IsAlwaysOnAlwaysConnectedCapable_1.0',\n",
    "        'Wdft_IsGamer_0.0',\n",
    "        'Wdft_IsGamer_1.0',\n",
    "        'EngineVersion_part_1_1',\n",
    "        'EngineVersion_part_2_1',\n",
    "        'AppVersion_part_1_4',\n",
    "        'AvSigVersion_part_1_0',\n",
    "        'AvSigVersion_part_1_1',\n",
    "        'AvSigVersion_part_4_0',\n",
    "        'OsBuildLab_part_3_amd64fre',\n",
    "        'OsBuildLab_part_3_arm64fre',\n",
    "        'OsBuildLab_part_3_x86fre',\n",
    "        'Census_OSVersion_part_1_10',\n",
    "        'Census_OSVersion_part_1_6',\n",
    "        'Census_OSVersion_part_2_0',\n",
    "        'Census_OSVersion_part_2_3',\n",
    "        'EngineVersion',\n",
    "        'AppVersion',\n",
    "        'AvSigVersion',\n",
    "        'RtpStateBitfield',\n",
    "        'AVProductStatesIdentifier',\n",
    "        'AVProductsInstalled',\n",
    "        'AVProductsEnabled',\n",
    "        'CountryIdentifier',\n",
    "        'CityIdentifier',\n",
    "        'GeoNameIdentifier',\n",
    "        'LocaleEnglishNameIdentifier',\n",
    "        'OsVer',\n",
    "        'OsBuild',\n",
    "        'OsSuite',\n",
    "        'OsPlatformSubRelease',\n",
    "        'OsBuildLab',\n",
    "        'SkuEdition',\n",
    "        'IeVerIdentifier',\n",
    "        'UacLuaenable',\n",
    "        'Census_MDC2FormFactor',\n",
    "        'Census_OEMNameIdentifier',\n",
    "        'Census_OEMModelIdentifier',\n",
    "        'Census_ProcessorCoreCount',\n",
    "        'Census_ProcessorModelIdentifier',\n",
    "        'Census_PrimaryDiskTotalCapacity',\n",
    "        'Census_SystemVolumeTotalCapacity',\n",
    "        'Census_TotalPhysicalRAM',\n",
    "        'Census_ChassisTypeName',\n",
    "        'Census_InternalPrimaryDiagonalDisplaySizeInInches',\n",
    "        'Census_InternalPrimaryDisplayResolutionHorizontal',\n",
    "        'Census_InternalPrimaryDisplayResolutionVertical',\n",
    "        'Census_PowerPlatformRoleName',\n",
    "        'Census_InternalBatteryNumberOfCharges',\n",
    "        'Census_OSVersion',\n",
    "        'Census_OSBranch',\n",
    "        'Census_OSBuildNumber',\n",
    "        'Census_OSBuildRevision',\n",
    "        'Census_OSEdition',\n",
    "        'Census_OSSkuName',\n",
    "        'Census_OSInstallTypeName',\n",
    "        'Census_OSInstallLanguageIdentifier',\n",
    "        'Census_OSUILocaleIdentifier',\n",
    "        'Census_OSWUAutoUpdateOptionsName',\n",
    "        'Census_ActivationChannel',\n",
    "        'Census_FlightRing',\n",
    "        'Census_FirmwareManufacturerIdentifier',\n",
    "        'Census_FirmwareVersionIdentifier',\n",
    "        'Wdft_RegionIdentifier',\n",
    "        'EngineVersion_part_3',\n",
    "        'EngineVersion_part_4',\n",
    "        'AppVersion_part_2',\n",
    "        'AppVersion_part_3',\n",
    "        'AppVersion_part_4',\n",
    "        'AvSigVersion_part_2',\n",
    "        'AvSigVersion_part_3',\n",
    "        'OsBuildLab_part_1',\n",
    "        'OsBuildLab_part_2',\n",
    "        'OsBuildLab_part_4',\n",
    "        'OsBuildLab_part_5',\n",
    "        'Census_OSVersion_part_3',\n",
    "        'Census_OSVersion_part_4',\n",
    "        'mean_Census_SystemVolumeTotalCapacity_by_CityIdentifier',\n",
    "        'max_Census_SystemVolumeTotalCapacity_by_CityIdentifier',\n",
    "        'min_Census_SystemVolumeTotalCapacity_by_CityIdentifier',\n",
    "        'mean_Census_PrimaryDiskTotalCapacity_by_CityIdentifier',\n",
    "        'max_Census_PrimaryDiskTotalCapacity_by_CityIdentifier',\n",
    "        'min_Census_PrimaryDiskTotalCapacity_by_CityIdentifier',\n",
    "        'mean_Census_SystemVolumeTotalCapacity_by_AVProductStatesIdentifier',\n",
    "        'max_Census_SystemVolumeTotalCapacity_by_AVProductStatesIdentifier',\n",
    "        'min_Census_SystemVolumeTotalCapacity_by_AVProductStatesIdentifier',\n",
    "        'mean_Census_PrimaryDiskTotalCapacity_by_AVProductStatesIdentifier',\n",
    "        'max_Census_PrimaryDiskTotalCapacity_by_AVProductStatesIdentifier',\n",
    "        'min_Census_PrimaryDiskTotalCapacity_by_AVProductStatesIdentifier',\n",
    "        'Census_SystemVolumeTotalCapacity_ratio_mean_on_CityIdentifier',\n",
    "        'Census_SystemVolumeTotalCapacity_amplitude_on_CityIdentifier',\n",
    "        'Census_SystemVolumeTotalCapacity_ratio_max_on_CityIdentifier',\n",
    "        'Census_PrimaryDiskTotalCapacity_ratio_mean_on_CityIdentifier',\n",
    "        'Census_PrimaryDiskTotalCapacity_amplitude_on_CityIdentifier',\n",
    "        'Census_PrimaryDiskTotalCapacity_ratio_max_on_CityIdentifier',\n",
    "        'Census_SystemVolumeTotalCapacity_ratio_mean_on_AVProductStatesIdentifier',\n",
    "        'Census_SystemVolumeTotalCapacity_amplitude_on_AVProductStatesIdentifier',\n",
    "        'Census_SystemVolumeTotalCapacity_ratio_max_on_AVProductStatesIdentifier',\n",
    "        'Census_PrimaryDiskTotalCapacity_ratio_mean_on_AVProductStatesIdentifier',\n",
    "        'Census_PrimaryDiskTotalCapacity_amplitude_on_AVProductStatesIdentifier',\n",
    "        'Census_PrimaryDiskTotalCapacity_ratio_max_on_AVProductStatesIdentifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Me quedo con las features relevantes resultantes del RFECV\n",
    "X_train=X_train[rfe_features]\n",
    "X_test=X_test[rfe_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Entrenamiento del modelo y predicción\n",
    "+ Entrenamiento de un modelo de clasificación LightGBM con los datos de entrenamiento\n",
    "+ Predicción con dataset de validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Info] Number of positive: 356527, number of negative: 357191\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037463 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10400\n",
      "[LightGBM] [Info] Number of data points in the train set: 713718, number of used features: 152\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001861\n",
      "[LightGBM] [Info] Start training from score -0.001861\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier(bagging_fraction=0.65, colsample_bytree=0.2,\n",
       "               feature_fraction=0.1, learning_rate=0.26, max_depth=75,\n",
       "               min_child_samples=2, n_estimators=95, num_leaves=60,\n",
       "               reg_alpha=0.2, reg_lambda=0.4)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;LGBMClassifier<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LGBMClassifier(bagging_fraction=0.65, colsample_bytree=0.2,\n",
       "               feature_fraction=0.1, learning_rate=0.26, max_depth=75,\n",
       "               min_child_samples=2, n_estimators=95, num_leaves=60,\n",
       "               reg_alpha=0.2, reg_lambda=0.4)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LGBMClassifier(bagging_fraction=0.65, colsample_bytree=0.2,\n",
       "               feature_fraction=0.1, learning_rate=0.26, max_depth=75,\n",
       "               min_child_samples=2, n_estimators=95, num_leaves=60,\n",
       "               reg_alpha=0.2, reg_lambda=0.4)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo de clasificación LightGBM\n",
    "model=lgbm.LGBMClassifier(**best_params)\n",
    "\n",
    "# Entrenar el clasificador con el dataset de train\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=0.2 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.65, subsample=1.0 will be ignored. Current value: bagging_fraction=0.65\n"
     ]
    }
   ],
   "source": [
    "# Predecir valores para test\n",
    "y_predict=model.predict(X_test)\n",
    "\n",
    "predict_prob=model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluación y visualización de resultados\n",
    "En esta sección se encontrarán las evaluaciones del modelo:\n",
    "+ Classification Report\n",
    "+ Matriz de confusión\n",
    "+ Curva ROC y AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Classification Report---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.59      0.59     89068\n",
      "           1       0.60      0.61      0.61     89362\n",
      "\n",
      "    accuracy                           0.60    178430\n",
      "   macro avg       0.60      0.60      0.60    178430\n",
      "weighted avg       0.60      0.60      0.60    178430\n",
      "\n",
      "Distribución de predicciones:\n",
      "Counter({1: 91841, 0: 86589})\n",
      "\n",
      "Distribución real de las clases en y_test:\n",
      "Counter({1: 89362, 0: 89068})\n"
     ]
    }
   ],
   "source": [
    "# Classification Report\n",
    "print(\"---Classification Report---\")\n",
    "classif_report=classification_report(y_test, y_predict)\n",
    "print(classif_report) \n",
    "\n",
    "print(\"Distribución de predicciones:\")\n",
    "print(Counter(y_predict))\n",
    "\n",
    "print(\"\\nDistribución real de las clases en y_test:\")\n",
    "print(Counter(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Matriz de confusión---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Matriz de confusión\n",
    "data=confusion_matrix(y_test, y_predict)\n",
    "\n",
    "print(\"---Matriz de confusión---\")\n",
    "plt.figure(figsize = (6,5))\n",
    "sns.set_theme(font_scale=1.4)\n",
    "sns.heatmap(data, cmap=\"Blues\", annot=True, annot_kws={\"size\": 12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Curva ROC y AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, predict_prob[:,1])\n",
    "roc_auc=auc(fpr,tpr)\n",
    "\n",
    "plt.figure(figsize = (8,6))\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "        lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Interpretabilidad\n",
    "+ Importancia de variables globales\n",
    "+ Importancia de variables individuales con SHAP\n",
    "+ SHAP Summary Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Importancia global de las variables en el modelo---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='predictor'>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"---Importancia global de las variables en el modelo---\")\n",
    "\n",
    "importancia_predictores = pd.DataFrame(\n",
    "                            {'predictor': X_train.columns.tolist(),\n",
    "                            'importancia': model.feature_importances_}\n",
    "                            )\n",
    "top_15_importantes = importancia_predictores.sort_values('importancia', ascending=False).head(15)\n",
    "\n",
    "top_15_importantes.set_index('predictor').sort_values('importancia', ascending=True).plot(kind='barh', figsize=(10, 6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SumeriO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\shap\\explainers\\_tree.py:448: UserWarning: LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "  warnings.warn('LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray')\n"
     ]
    }
   ],
   "source": [
    "explainer=shap.TreeExplainer(model)\n",
    "shap_values=explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Importancia individual de variables con SHAP---\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x950 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"---Importancia individual de variables con SHAP---\")\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\", color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Summary Plot---\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x950 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"---Summary Plot---\")\n",
    "shap.summary_plot(shap_values, X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
